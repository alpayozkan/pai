{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b6fbba-5b4f-4a34-bdc9-136aaba4c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf `USE_PRETRAINED_INIT` is `True`, then MAP inference uses provided pretrained weights.\\nYou should not modify MAP training or the CNN architecture before passing the hard baseline.\\nIf you set the constant to `False` (to further experiment),\\nthis solution always performs MAP inference before running your SWAG implementation.\\nNote that MAP inference can take a long time.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import abc\n",
    "import collections\n",
    "import enum\n",
    "import math\n",
    "import pathlib\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from util import draw_reliability_diagram, cost_function, setup_seeds, calc_calibration_curve\n",
    "\n",
    "EXTENDED_EVALUATION = False\n",
    "\"\"\"\n",
    "Set `EXTENDED_EVALUATION` to `True` in order to generate additional plots on validation data.\n",
    "\"\"\"\n",
    "\n",
    "USE_PRETRAINED_INIT = True\n",
    "\"\"\"\n",
    "If `USE_PRETRAINED_INIT` is `True`, then MAP inference uses provided pretrained weights.\n",
    "You should not modify MAP training or the CNN architecture before passing the hard baseline.\n",
    "If you set the constant to `False` (to further experiment),\n",
    "this solution always performs MAP inference before running your SWAG implementation.\n",
    "Note that MAP inference can take a long time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2f1c7-0241-4c30-a10c-b76cbf39feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393f3113-7760-4d37-a4a0-f46082e7b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604e6e83-f604-4fb4-ae3a-356c75e8a865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(4).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f5e8d5-7e1b-45fe-a85f-b498259694c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [torch.rand(4).unsqueeze(0) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1dd1da-b1f9-4162-b8f2-2ae3b2c226e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(L,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1701a9c6-1bcf-4bb8-8997-3a9b62d77320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4051, 0.4541, 0.7117, 0.2309])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.cat(L,0),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275f3fb-9596-48d5-bb98-67734d02f3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e99430-337e-45ae-bd3a-edf02f5ac2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     raise RuntimeError(\n",
    "#         \"This main() method is for illustrative purposes only\"\n",
    "#         \" and will NEVER be called when running your solution to generate your submission file!\\n\"\n",
    "#         \"The checker always directly interacts with your SWAGInference class and evaluate method.\\n\"\n",
    "#         \"You can remove this exception for local testing, but be aware that any changes to the main() method\"\n",
    "#         \" are ignored when generating your submission file.\"\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b69f45-371a-40ee-9776-3a0ab0a04340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pathlib' from '/Users/alpayozkan/opt/anaconda3/envs/pai/lib/python3.8/pathlib.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c15b26-ffbb-483a-b7ac-53a0c7cb34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path.cwd()\n",
    "model_dir = pathlib.Path.cwd()\n",
    "output_dir = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158b9ef-049a-4f54-b45f-fd9f3d60bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e7c782-a8d8-41c4-9d2e-c34ecea022e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_xs = torch.from_numpy(np.load(data_dir / \"train_xs.npz\")[\"train_xs\"])\n",
    "raw_train_meta = np.load(data_dir / \"train_ys.npz\")\n",
    "train_ys = torch.from_numpy(raw_train_meta[\"train_ys\"])\n",
    "train_is_snow = torch.from_numpy(raw_train_meta[\"train_is_snow\"])\n",
    "train_is_cloud = torch.from_numpy(raw_train_meta[\"train_is_cloud\"])\n",
    "dataset_train = torch.utils.data.TensorDataset(train_xs, train_is_snow, train_is_cloud, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12540b-a016-4218-8c7f-7856039c505c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4b578-97aa-4ed2-9743-d7f86752aa26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce8ef8c-c923-4b01-97ad-4624105d8825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 3, 60, 60])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3065bc-3e0d-43b8-970d-7780e9b94da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9fb79ae190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_xs[100].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50dfe4f9-6c9c-4470-ab68-f914eb5173ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0., 300., 300., 300., 300., 600.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(train_ys, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "937826a0-8617-4638-ae10-a0f3db3bb9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0., 1640.,  160.,    0.,    0.,    0.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(train_is_snow, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d485a497-c10d-496b-abac-60f51ae39613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0., 1740.,   60.,    0.,    0.,    0.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(train_is_cloud, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52a6bf-fb4a-475d-a51f-2fcebc232cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba8581-a72c-493b-94bc-65777b5b5e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd70eb-b2a4-4df9-bd8d-72f6d102908e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c42cb55-0378-46f6-92aa-97cdb1af9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "val_xs = torch.from_numpy(np.load(data_dir / \"val_xs.npz\")[\"val_xs\"])\n",
    "raw_val_meta = np.load(data_dir / \"val_ys.npz\")\n",
    "val_ys = torch.from_numpy(raw_val_meta[\"val_ys\"])\n",
    "val_is_snow = torch.from_numpy(raw_val_meta[\"val_is_snow\"])\n",
    "val_is_cloud = torch.from_numpy(raw_val_meta[\"val_is_cloud\"])\n",
    "dataset_val = torch.utils.data.TensorDataset(val_xs, val_is_snow, val_is_cloud, val_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e71185-e20e-48b3-930f-83812fdeae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 3, 60, 60])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dd09c89-79ed-4529-94b5-559d0236ecea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  1,  3, -1,  1,  0,  5,  4, -1, -1,  0,  2,  5,  2,  5,  1,  3,  3,\n",
       "         0, -1,  4,  2, -1,  4,  3, -1,  0,  1,  5,  2, -1,  4,  0,  5,  1,  3,\n",
       "         3,  2,  5,  1, -1,  4,  3,  0,  2,  4,  0,  3, -1,  5,  1, -1,  2,  4,\n",
       "         1,  1,  5,  2,  3,  0, -1,  2, -1,  1,  3, -1,  1, -1,  2,  1,  0,  3,\n",
       "         4,  1,  3,  5,  5,  3,  0,  5,  4,  1,  5,  2,  4,  3,  0,  4,  0, -1,\n",
       "         4,  4,  2,  1,  5,  4,  3,  5, -1, -1,  4,  0, -1,  1,  0,  2, -1,  0,\n",
       "         1,  3,  0,  5,  5,  4,  5,  0,  1,  4,  2,  3,  0,  2,  4, -1,  3,  2,\n",
       "         5,  2,  1,  5,  3,  4,  2,  0,  3,  1,  2,  5,  4,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f826e7-e0bf-47dc-9d12-5b778ed5fb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_is_snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baff3d9a-c6c0-4477-bab9-bb8cf3878e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20., 20., 20., 20., 20., 40.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(val_ys, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e84c8792-0fef-47eb-a0cd-4cb13d352178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20., 20., 20., 20., 20., 40.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(val_ys, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1512d4fe-d18b-4d20-95c6-4257bc6a1e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0., 134.,   6.,   0.,   0.,   0.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(val_is_snow, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a862b2-f559-4310-a63d-e5cdfb878439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0., 136.,   4.,   0.,   0.,   0.]),\n",
       " array([-1.,  0.,  1.,  2.,  3.,  4.,  5.]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(val_is_cloud, bins=[-1,0,1,2,3,4,5])\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96031173-5299-4ba7-9120-2102e55cf10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6daf9-4095-494a-8820-f6a1f852eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f6b3b3a-d7f3-45d5-b262-dd010e7d1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all randomness\n",
    "setup_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3522cff-61a5-419e-bbcc-c53cb9c5f60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a7b18a-a560-4a76-8150-4aa368c82136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run the actual solution\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e10f9f-2abc-4b36-857d-976af6f58716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd1743d-fb57-449c-a333-a6006e060b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SWAGInference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m swag \u001b[38;5;241m=\u001b[39m \u001b[43mSWAGInference\u001b[49m(\n\u001b[1;32m      2\u001b[0m     train_xs\u001b[38;5;241m=\u001b[39mdataset_train\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mmodel_dir,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m swag\u001b[38;5;241m.\u001b[39mfit(train_loader)\n\u001b[1;32m      6\u001b[0m swag\u001b[38;5;241m.\u001b[39mcalibrate(dataset_val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SWAGInference' is not defined"
     ]
    }
   ],
   "source": [
    "swag = SWAGInference(\n",
    "    train_xs=dataset_train.tensors[0],\n",
    "    model_dir=model_dir,\n",
    ")\n",
    "swag.fit(train_loader)\n",
    "swag.calibrate(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441a9ab-c2ca-4f30-8f6b-65437ad63cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2446ed4-08b9-40fd-a093-129380bbb142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577ae5b-a936-4ea9-8d49-912471d7b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fork_rng ensures that the evaluation does not change the rng state.\n",
    "# That way, you should get exactly the same results even if you remove evaluation\n",
    "# to save computational time when developing the task\n",
    "# (as long as you ONLY use torch randomness, and not e.g. random or numpy.random).\n",
    "with torch.random.fork_rng():\n",
    "    evaluate(swag, dataset_val, EXTENDED_EVALUATION, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1388a1e-badc-4dde-bf6a-32176b201f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf25443-ca49-4494-b656-a898f54f717e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbf91d-6af4-4afe-8996-4c4f643de487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846569b1-6529-4438-91c7-66e07fdc5298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949218b-ea94-42b0-8efa-10caa19918c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ad94abd-b35e-402b-aeb5-bee436154d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Small convolutional neural network used in this task.\n",
    "    You should not modify this class before passing the hard baseline.\n",
    "\n",
    "    Note that if you change the architecture of this network,\n",
    "    you need to re-run MAP inference and cannot use the provided pretrained weights anymore.\n",
    "    Hence, you need to set `USE_PRETRAINED_INIT = False` at the top of this file.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer0 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool1 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool2 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "        )\n",
    "\n",
    "        self.global_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.linear = torch.nn.Linear(64, out_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # Average features over both spatial dimensions, and remove the now superfluous dimensions\n",
    "        x = self.global_pool(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Note: this network does NOT output the per-class probabilities y =[y_1, ..., y_C],\n",
    "        # but a feature vector z such that y = softmax(z).\n",
    "        # This avoids numerical instabilities during optimization.\n",
    "        # The PyTorch loss automatically handles this.\n",
    "        log_softmax = self.linear(x)\n",
    "\n",
    "        return log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b82485bc-d60b-431b-8a9f-10c46d0880ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CNN(in_channels=3, out_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1c7826-17c3-475a-8a15-0fa0cbe088e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f9fb92f1cf0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d5648aa-6d8c-4e37-bac6-891a7b2944a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_params = {name: param.detach() for name, param in network.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f54242-bdac-4433-a3d3-4470224905fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c481b86-b42b-43fe-80d8-c2c102897289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0.0.weight': tensor([[[[ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
       "           [ 0.0692, -0.0238,  0.0587,  0.0161, -0.0141],\n",
       "           [ 0.0320,  0.0057,  0.0422, -0.0450, -0.0084],\n",
       "           [-0.0104,  0.0167, -0.0005,  0.1009,  0.0359],\n",
       "           [-0.0430, -0.0697, -0.0194, -0.0498, -0.0370]],\n",
       " \n",
       "          [[ 0.0055,  0.0688,  0.0628, -0.1129,  0.0716],\n",
       "           [ 0.0323,  0.1095,  0.0762, -0.1052, -0.1098],\n",
       "           [-0.0557,  0.1014, -0.0192,  0.0494, -0.0537],\n",
       "           [ 0.1133, -0.0489,  0.0866,  0.0014, -0.0608],\n",
       "           [ 0.0594, -0.0613,  0.0340, -0.0333, -0.0127]],\n",
       " \n",
       "          [[-0.1110, -0.0551,  0.0627, -0.0281,  0.1150],\n",
       "           [ 0.0926, -0.0054, -0.0771,  0.0703,  0.0358],\n",
       "           [-0.0746,  0.0750,  0.0701,  0.1024, -0.0647],\n",
       "           [-0.0190, -0.0022,  0.0169, -0.0876, -0.0819],\n",
       "           [ 0.0628, -0.0271,  0.0564,  0.0066,  0.0379]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0254,  0.0420,  0.0572, -0.1069,  0.0581],\n",
       "           [-0.0812, -0.0871,  0.0070, -0.0197,  0.0678],\n",
       "           [-0.0669, -0.1027,  0.0840, -0.0171,  0.0649],\n",
       "           [ 0.0371, -0.0866,  0.0232,  0.0277, -0.0773],\n",
       "           [-0.0548,  0.0394,  0.0207, -0.0491, -0.0350]],\n",
       " \n",
       "          [[ 0.1058, -0.0214,  0.0651,  0.0500, -0.0746],\n",
       "           [-0.0982,  0.1108,  0.0060,  0.0791,  0.0239],\n",
       "           [ 0.0371,  0.0863,  0.1095, -0.0766,  0.0144],\n",
       "           [ 0.0862,  0.0837,  0.0717, -0.0836, -0.0832],\n",
       "           [-0.0698,  0.0145,  0.1151, -0.0729,  0.0615]],\n",
       " \n",
       "          [[-0.0639, -0.1086, -0.0245,  0.0665,  0.1072],\n",
       "           [-0.0717,  0.0251,  0.0996,  0.0765,  0.0720],\n",
       "           [ 0.0821,  0.0730,  0.0298, -0.0790, -0.0970],\n",
       "           [-0.0529, -0.0134, -0.0708,  0.0422,  0.0357],\n",
       "           [-0.0261,  0.0444,  0.0373,  0.0705,  0.0778]]],\n",
       " \n",
       " \n",
       "         [[[-0.0391,  0.1128, -0.0133, -0.0040, -0.1090],\n",
       "           [-0.0743, -0.0675, -0.0494,  0.0821, -0.0377],\n",
       "           [-0.0863,  0.0444,  0.0370,  0.0748, -0.0598],\n",
       "           [ 0.0250, -0.0420, -0.0259, -0.0920, -0.0526],\n",
       "           [-0.0354,  0.0494,  0.0211,  0.0285,  0.1153]],\n",
       " \n",
       "          [[ 0.1125,  0.0788,  0.0037, -0.0799,  0.0902],\n",
       "           [-0.0289, -0.0093, -0.0995, -0.0228, -0.0745],\n",
       "           [ 0.1061, -0.0998, -0.0900, -0.0039, -0.0624],\n",
       "           [ 0.0413, -0.0444, -0.0542,  0.0065,  0.0836],\n",
       "           [-0.0812,  0.0542,  0.0742,  0.1130, -0.0808]],\n",
       " \n",
       "          [[ 0.0280, -0.0854,  0.0986, -0.0448,  0.0696],\n",
       "           [ 0.0034, -0.0090, -0.0037,  0.0196,  0.0544],\n",
       "           [ 0.0185,  0.0352, -0.1039,  0.0841,  0.1007],\n",
       "           [ 0.0954,  0.0854, -0.0833, -0.0428,  0.1018],\n",
       "           [-0.0879,  0.1048, -0.0908, -0.0813,  0.0565]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0058, -0.0155,  0.0586, -0.0491,  0.0908],\n",
       "           [-0.0787, -0.0601,  0.0400,  0.0710, -0.0014],\n",
       "           [-0.0906, -0.0046, -0.0390, -0.0369, -0.0213],\n",
       "           [ 0.0278, -0.0269, -0.0810, -0.0577, -0.0967],\n",
       "           [-0.1153,  0.0558,  0.0645,  0.0873,  0.0091]],\n",
       " \n",
       "          [[-0.0475, -0.0694,  0.0575, -0.0698,  0.0863],\n",
       "           [-0.0626, -0.1077, -0.0832,  0.0940,  0.0872],\n",
       "           [ 0.0282,  0.0027,  0.0990,  0.0697,  0.0684],\n",
       "           [-0.0786,  0.0185,  0.1046, -0.0813,  0.1150],\n",
       "           [-0.0756, -0.0339, -0.0054,  0.0300,  0.0355]],\n",
       " \n",
       "          [[-0.0660,  0.0529,  0.0306, -0.0129, -0.1149],\n",
       "           [ 0.1143, -0.0427, -0.0615,  0.1143, -0.0585],\n",
       "           [ 0.0771,  0.0705, -0.0752, -0.0931, -0.0628],\n",
       "           [ 0.0361,  0.1089, -0.0064, -0.0345, -0.0332],\n",
       "           [-0.0804,  0.1154, -0.0013, -0.0914, -0.0789]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0718,  0.0559,  0.0745, -0.0122, -0.0228],\n",
       "           [ 0.0378, -0.0011, -0.0372,  0.0275, -0.0658],\n",
       "           [-0.0800, -0.0255, -0.0823,  0.0029,  0.0182],\n",
       "           [ 0.1131,  0.0333, -0.0007, -0.0839, -0.0464],\n",
       "           [-0.0188, -0.0831,  0.0225,  0.0831,  0.0661]],\n",
       " \n",
       "          [[-0.0479, -0.0282,  0.1151, -0.0185, -0.1037],\n",
       "           [-0.0010,  0.0059, -0.0429, -0.0125, -0.0942],\n",
       "           [ 0.0356, -0.0801,  0.0397,  0.0870, -0.0745],\n",
       "           [-0.0452,  0.0111,  0.0003,  0.0677, -0.0069],\n",
       "           [ 0.0187, -0.0319,  0.0381,  0.0349, -0.0930]],\n",
       " \n",
       "          [[ 0.0613, -0.0237,  0.1034, -0.1146, -0.0115],\n",
       "           [ 0.0522,  0.0796,  0.0015, -0.0624, -0.0893],\n",
       "           [-0.0874,  0.0930, -0.1006, -0.0457,  0.0605],\n",
       "           [ 0.0631, -0.1010,  0.0229,  0.0140,  0.1060],\n",
       "           [-0.0041,  0.0490, -0.0100,  0.1031, -0.0118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0679,  0.0213, -0.0611,  0.0289,  0.0803],\n",
       "           [-0.0898, -0.0840,  0.0016, -0.0068, -0.0359],\n",
       "           [ 0.0967,  0.0518, -0.0383,  0.0167, -0.0174],\n",
       "           [-0.0739,  0.0472, -0.0762,  0.0140,  0.0784],\n",
       "           [-0.1061,  0.0051,  0.1066,  0.0521, -0.0455]],\n",
       " \n",
       "          [[ 0.0182, -0.1084,  0.0185,  0.1004,  0.1003],\n",
       "           [-0.0018,  0.0419, -0.0192, -0.0324,  0.0022],\n",
       "           [-0.0673,  0.0465,  0.0566, -0.0313, -0.0263],\n",
       "           [-0.0805,  0.0148,  0.0459,  0.1096, -0.1088],\n",
       "           [-0.0974,  0.0033,  0.1067,  0.0074,  0.0321]],\n",
       " \n",
       "          [[ 0.0164, -0.0138,  0.0824,  0.0848, -0.0666],\n",
       "           [-0.0692, -0.0898, -0.0281, -0.0026,  0.0966],\n",
       "           [-0.1118,  0.0820, -0.0645,  0.0094, -0.0054],\n",
       "           [ 0.0652, -0.0966, -0.0806,  0.0026,  0.0582],\n",
       "           [-0.0556,  0.0358, -0.0305, -0.1077,  0.0132]]]]),\n",
       " 'layer0.0.bias': tensor([-0.0246,  0.0683,  0.0761,  0.0755,  0.0723, -0.0893, -0.1108,  0.0129,\n",
       "          0.0335, -0.0888,  0.0944, -0.0122, -0.0677,  0.0792,  0.1011,  0.0177,\n",
       "         -0.0368,  0.0125, -0.0851,  0.0712,  0.0725, -0.0216, -0.0915,  0.0857,\n",
       "         -0.0945, -0.0606,  0.0689, -0.0574,  0.0220,  0.0949, -0.0146, -0.0261]),\n",
       " 'layer0.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer0.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.0.weight': tensor([[[[ 0.0546,  0.0101,  0.0206],\n",
       "           [-0.0194, -0.0508,  0.0092],\n",
       "           [-0.0348, -0.0305, -0.0483]],\n",
       " \n",
       "          [[ 0.0505, -0.0396,  0.0170],\n",
       "           [ 0.0121,  0.0407,  0.0322],\n",
       "           [-0.0413,  0.0261, -0.0299]],\n",
       " \n",
       "          [[ 0.0022, -0.0213, -0.0504],\n",
       "           [ 0.0176, -0.0393,  0.0577],\n",
       "           [ 0.0237, -0.0204, -0.0345]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0583,  0.0324,  0.0416],\n",
       "           [ 0.0469,  0.0359, -0.0188],\n",
       "           [ 0.0121,  0.0149, -0.0563]],\n",
       " \n",
       "          [[ 0.0318, -0.0475,  0.0212],\n",
       "           [ 0.0569, -0.0164,  0.0081],\n",
       "           [ 0.0546,  0.0486,  0.0492]],\n",
       " \n",
       "          [[-0.0408,  0.0116, -0.0493],\n",
       "           [ 0.0108, -0.0558,  0.0068],\n",
       "           [ 0.0298, -0.0525, -0.0090]]],\n",
       " \n",
       " \n",
       "         [[[-0.0244,  0.0484, -0.0058],\n",
       "           [ 0.0367, -0.0435,  0.0082],\n",
       "           [ 0.0427,  0.0033, -0.0206]],\n",
       " \n",
       "          [[-0.0241,  0.0402,  0.0232],\n",
       "           [ 0.0040,  0.0348, -0.0307],\n",
       "           [-0.0447, -0.0133,  0.0432]],\n",
       " \n",
       "          [[-0.0570, -0.0293,  0.0143],\n",
       "           [ 0.0222,  0.0295,  0.0178],\n",
       "           [-0.0520, -0.0192,  0.0362]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0269, -0.0078, -0.0461],\n",
       "           [-0.0380, -0.0191, -0.0069],\n",
       "           [-0.0127,  0.0033, -0.0540]],\n",
       " \n",
       "          [[-0.0095, -0.0517,  0.0310],\n",
       "           [ 0.0537, -0.0465, -0.0424],\n",
       "           [ 0.0571,  0.0458,  0.0271]],\n",
       " \n",
       "          [[-0.0372,  0.0185, -0.0019],\n",
       "           [ 0.0444,  0.0367, -0.0067],\n",
       "           [ 0.0230, -0.0515,  0.0101]]],\n",
       " \n",
       " \n",
       "         [[[-0.0359,  0.0468, -0.0428],\n",
       "           [-0.0512, -0.0502, -0.0168],\n",
       "           [ 0.0032, -0.0068,  0.0242]],\n",
       " \n",
       "          [[-0.0523, -0.0346, -0.0082],\n",
       "           [-0.0019, -0.0128,  0.0270],\n",
       "           [ 0.0305, -0.0106, -0.0029]],\n",
       " \n",
       "          [[-0.0092,  0.0058, -0.0247],\n",
       "           [-0.0092, -0.0432,  0.0493],\n",
       "           [ 0.0403, -0.0400,  0.0044]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0401, -0.0285, -0.0259],\n",
       "           [-0.0050, -0.0551,  0.0106],\n",
       "           [ 0.0445,  0.0369, -0.0460]],\n",
       " \n",
       "          [[-0.0178,  0.0308,  0.0211],\n",
       "           [ 0.0263,  0.0247, -0.0142],\n",
       "           [ 0.0390,  0.0552,  0.0218]],\n",
       " \n",
       "          [[-0.0097,  0.0256,  0.0409],\n",
       "           [-0.0466,  0.0163, -0.0281],\n",
       "           [ 0.0030, -0.0391, -0.0109]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0194, -0.0357,  0.0117],\n",
       "           [ 0.0072, -0.0325, -0.0014],\n",
       "           [ 0.0456, -0.0435,  0.0098]],\n",
       " \n",
       "          [[ 0.0320,  0.0543, -0.0537],\n",
       "           [ 0.0267,  0.0560,  0.0403],\n",
       "           [-0.0508, -0.0085, -0.0072]],\n",
       " \n",
       "          [[-0.0085,  0.0042,  0.0038],\n",
       "           [ 0.0312, -0.0462, -0.0416],\n",
       "           [-0.0018, -0.0555,  0.0578]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0169,  0.0042,  0.0447],\n",
       "           [-0.0125,  0.0027,  0.0512],\n",
       "           [-0.0242, -0.0469, -0.0531]],\n",
       " \n",
       "          [[-0.0371, -0.0043, -0.0548],\n",
       "           [-0.0245,  0.0057,  0.0537],\n",
       "           [-0.0011,  0.0426,  0.0344]],\n",
       " \n",
       "          [[-0.0172,  0.0337, -0.0225],\n",
       "           [ 0.0088, -0.0114,  0.0121],\n",
       "           [-0.0192,  0.0237,  0.0269]]],\n",
       " \n",
       " \n",
       "         [[[-0.0037, -0.0553, -0.0541],\n",
       "           [-0.0205, -0.0239,  0.0142],\n",
       "           [ 0.0084, -0.0058,  0.0161]],\n",
       " \n",
       "          [[ 0.0074,  0.0498,  0.0194],\n",
       "           [-0.0311, -0.0490, -0.0356],\n",
       "           [-0.0336,  0.0319, -0.0069]],\n",
       " \n",
       "          [[-0.0362, -0.0574, -0.0031],\n",
       "           [-0.0213, -0.0345,  0.0364],\n",
       "           [ 0.0138, -0.0144,  0.0195]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0320, -0.0554, -0.0148],\n",
       "           [-0.0214,  0.0331,  0.0398],\n",
       "           [ 0.0071,  0.0106, -0.0159]],\n",
       " \n",
       "          [[-0.0255,  0.0531,  0.0584],\n",
       "           [ 0.0261,  0.0347, -0.0573],\n",
       "           [ 0.0132,  0.0088, -0.0572]],\n",
       " \n",
       "          [[ 0.0217, -0.0435,  0.0497],\n",
       "           [ 0.0121,  0.0476, -0.0020],\n",
       "           [-0.0400,  0.0024, -0.0448]]],\n",
       " \n",
       " \n",
       "         [[[-0.0536, -0.0045,  0.0082],\n",
       "           [ 0.0016,  0.0405, -0.0227],\n",
       "           [-0.0341,  0.0054,  0.0062]],\n",
       " \n",
       "          [[-0.0014, -0.0461, -0.0520],\n",
       "           [-0.0377,  0.0143,  0.0103],\n",
       "           [ 0.0545, -0.0154, -0.0308]],\n",
       " \n",
       "          [[-0.0100,  0.0565, -0.0555],\n",
       "           [ 0.0302, -0.0189,  0.0516],\n",
       "           [ 0.0415,  0.0222,  0.0087]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0453, -0.0079,  0.0073],\n",
       "           [-0.0204,  0.0051, -0.0157],\n",
       "           [ 0.0565,  0.0389, -0.0149]],\n",
       " \n",
       "          [[ 0.0438, -0.0230,  0.0282],\n",
       "           [ 0.0552, -0.0253,  0.0328],\n",
       "           [-0.0390, -0.0553,  0.0518]],\n",
       " \n",
       "          [[ 0.0100, -0.0365,  0.0531],\n",
       "           [ 0.0285,  0.0130, -0.0063],\n",
       "           [ 0.0179, -0.0586, -0.0554]]]]),\n",
       " 'layer1.0.bias': tensor([-0.0543, -0.0043, -0.0017,  0.0498, -0.0517,  0.0165,  0.0087,  0.0565,\n",
       "          0.0444, -0.0543, -0.0388,  0.0033,  0.0551,  0.0235,  0.0446, -0.0100,\n",
       "         -0.0346,  0.0379,  0.0387,  0.0266, -0.0253,  0.0476,  0.0511, -0.0268,\n",
       "         -0.0131,  0.0463,  0.0268,  0.0487,  0.0120, -0.0506,  0.0130, -0.0039]),\n",
       " 'layer1.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer1.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.0.weight': tensor([[[[ 5.7134e-02,  2.2763e-03,  7.6729e-03],\n",
       "           [ 2.2283e-02, -5.0950e-02, -1.4664e-02],\n",
       "           [-4.8651e-02, -4.9914e-02,  2.2173e-02]],\n",
       " \n",
       "          [[-2.2566e-02, -3.4802e-02, -5.1511e-02],\n",
       "           [ 5.8050e-02, -4.7091e-02, -4.5683e-02],\n",
       "           [-4.2887e-02, -2.2843e-02,  9.9157e-03]],\n",
       " \n",
       "          [[-1.1733e-02, -4.2466e-02,  3.9983e-02],\n",
       "           [-3.8778e-02, -3.6147e-02,  6.0745e-03],\n",
       "           [-1.9005e-02, -3.8071e-02, -4.7358e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8251e-02, -2.0059e-02, -1.7664e-03],\n",
       "           [ 3.6452e-02, -5.8274e-02, -2.5591e-02],\n",
       "           [ 3.0273e-02,  2.4939e-02, -1.4607e-02]],\n",
       " \n",
       "          [[-5.6558e-02,  1.9704e-02, -1.7753e-02],\n",
       "           [ 2.6979e-02,  5.5089e-02,  1.3478e-02],\n",
       "           [ 2.6325e-02,  5.3409e-02, -1.9308e-02]],\n",
       " \n",
       "          [[ 1.3918e-02, -4.2279e-03,  3.3063e-02],\n",
       "           [ 1.9445e-02, -3.4774e-03,  1.6238e-02],\n",
       "           [ 6.9609e-04, -3.7400e-02,  2.5798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.9733e-02,  5.1662e-02, -3.5295e-02],\n",
       "           [-4.7838e-03, -2.9584e-02,  1.9393e-02],\n",
       "           [-4.9178e-02,  1.5128e-02,  3.1844e-02]],\n",
       " \n",
       "          [[-4.8651e-02, -2.3051e-02,  4.6810e-02],\n",
       "           [ 2.2733e-02,  3.7091e-02,  7.8823e-03],\n",
       "           [ 4.6600e-05, -3.1117e-02, -2.0206e-02]],\n",
       " \n",
       "          [[-5.0931e-02,  5.2320e-02,  1.5128e-02],\n",
       "           [-1.2424e-02, -3.7507e-02,  2.9012e-04],\n",
       "           [ 1.3300e-02,  4.9583e-03,  2.4148e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8632e-02, -4.4404e-02,  1.1814e-02],\n",
       "           [-1.0807e-02,  4.8829e-02,  4.4790e-02],\n",
       "           [-2.0285e-02,  3.6822e-02,  2.8248e-02]],\n",
       " \n",
       "          [[ 1.7641e-02,  1.0618e-02, -3.1017e-02],\n",
       "           [-3.5140e-02, -1.0367e-02, -4.9357e-03],\n",
       "           [-2.6300e-03,  1.1192e-02, -4.5672e-02]],\n",
       " \n",
       "          [[-3.9225e-03, -3.7740e-02, -5.4893e-02],\n",
       "           [ 2.5574e-02,  5.4951e-02, -2.9725e-02],\n",
       "           [ 1.5240e-02, -3.6052e-02,  3.4700e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 5.5491e-02, -5.2750e-03, -1.8764e-02],\n",
       "           [-1.7387e-02, -5.2585e-02,  7.1799e-03],\n",
       "           [-2.7637e-02, -4.1692e-02,  3.6744e-02]],\n",
       " \n",
       "          [[ 2.0283e-02, -4.8516e-02, -2.9705e-02],\n",
       "           [ 3.5179e-02,  2.7032e-02,  5.6624e-02],\n",
       "           [-3.8420e-03,  1.8063e-02, -2.4304e-02]],\n",
       " \n",
       "          [[-3.5589e-02, -2.8323e-02, -3.3601e-03],\n",
       "           [-6.5405e-04, -5.8679e-02,  5.0192e-02],\n",
       "           [-3.0432e-02, -3.8527e-02, -4.3469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2697e-02, -1.4331e-02,  5.0963e-02],\n",
       "           [ 5.5813e-02,  1.8976e-02,  1.3342e-02],\n",
       "           [ 2.3138e-02,  3.1858e-02,  3.6571e-02]],\n",
       " \n",
       "          [[ 3.4722e-02, -2.1318e-02,  5.0169e-02],\n",
       "           [ 1.3877e-02,  1.4181e-02,  2.9957e-02],\n",
       "           [ 4.5517e-02,  2.8146e-02,  4.4315e-02]],\n",
       " \n",
       "          [[-2.2685e-02,  3.5278e-02, -7.6343e-03],\n",
       "           [ 3.1103e-02, -1.9444e-02, -2.7423e-02],\n",
       "           [ 5.1713e-02, -2.1571e-02, -4.5952e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-5.5247e-02,  3.1720e-02,  2.1539e-02],\n",
       "           [-3.8513e-02, -4.7985e-02, -3.3430e-02],\n",
       "           [-3.0489e-02, -1.7572e-02,  4.2375e-02]],\n",
       " \n",
       "          [[ 4.4120e-02,  5.4889e-02,  2.4734e-02],\n",
       "           [ 3.6550e-03, -4.3214e-02, -2.4624e-03],\n",
       "           [-4.2030e-02,  4.0658e-02, -5.2674e-02]],\n",
       " \n",
       "          [[-7.3320e-03, -7.3222e-03,  3.7609e-02],\n",
       "           [-3.0710e-02, -4.8105e-02, -1.4401e-02],\n",
       "           [-3.6586e-03,  1.9842e-02,  1.5428e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.9497e-02, -3.2710e-02, -5.1007e-02],\n",
       "           [-1.2096e-02,  4.4725e-02,  4.9262e-02],\n",
       "           [-4.5488e-02,  5.7023e-02, -1.3938e-02]],\n",
       " \n",
       "          [[ 2.6453e-02, -5.3338e-02,  4.9874e-03],\n",
       "           [ 4.9443e-02,  4.6337e-02,  4.6237e-02],\n",
       "           [ 1.9181e-02, -2.0657e-02, -4.5685e-02]],\n",
       " \n",
       "          [[-2.5192e-03,  2.8761e-02, -4.2599e-02],\n",
       "           [-4.8064e-03,  2.7911e-02, -3.7541e-02],\n",
       "           [-5.1003e-02,  1.0393e-02,  5.6107e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0096e-02,  8.2636e-03,  3.3643e-02],\n",
       "           [-3.5483e-02,  3.9823e-02, -5.1088e-02],\n",
       "           [-5.2329e-02, -7.8036e-03, -2.6994e-02]],\n",
       " \n",
       "          [[ 5.0138e-02, -1.2705e-03,  5.5152e-02],\n",
       "           [ 4.8575e-02, -2.8620e-02,  4.0724e-02],\n",
       "           [ 4.4372e-02, -5.5048e-02,  8.2719e-03]],\n",
       " \n",
       "          [[-3.6917e-03, -4.8237e-02,  1.4326e-02],\n",
       "           [ 5.8474e-02,  9.2294e-04,  5.3283e-02],\n",
       "           [ 3.6018e-02,  1.5803e-02, -4.4104e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.2120e-02, -2.1614e-02,  1.7018e-02],\n",
       "           [-4.2496e-02, -5.5788e-02,  2.9173e-02],\n",
       "           [-4.1194e-02,  3.2194e-03,  2.2686e-02]],\n",
       " \n",
       "          [[ 5.4017e-02,  1.5278e-02, -3.9786e-02],\n",
       "           [ 3.6710e-03, -4.5804e-02, -3.8354e-02],\n",
       "           [-2.4702e-02,  3.4668e-02, -4.7339e-02]],\n",
       " \n",
       "          [[ 5.3430e-02,  1.8570e-02, -3.7024e-02],\n",
       "           [-5.7286e-02,  2.4916e-02, -2.3010e-02],\n",
       "           [ 3.4885e-02, -2.9613e-02,  1.6202e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.0582e-02, -2.6968e-02, -2.8093e-02],\n",
       "           [-8.5723e-04, -1.6351e-02, -1.8424e-02],\n",
       "           [ 3.6358e-02, -2.9207e-02,  4.2901e-02]],\n",
       " \n",
       "          [[ 5.2379e-02,  2.7298e-02, -3.8665e-02],\n",
       "           [ 4.7064e-02, -1.9328e-02,  5.7603e-02],\n",
       "           [ 3.4986e-02,  3.6665e-02, -9.3917e-03]],\n",
       " \n",
       "          [[ 3.7847e-02,  5.5604e-02,  1.2676e-02],\n",
       "           [ 5.7904e-02,  4.6458e-02, -1.0816e-02],\n",
       "           [ 1.4825e-02,  3.0120e-02, -2.4013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.5582e-02,  7.9506e-03, -5.3923e-02],\n",
       "           [ 3.4684e-02,  3.2537e-03, -8.8748e-03],\n",
       "           [ 5.1414e-02,  2.1381e-02, -4.7578e-02]],\n",
       " \n",
       "          [[-1.9648e-02, -5.0517e-02, -1.2699e-02],\n",
       "           [ 2.0551e-02,  3.4886e-02, -1.7081e-02],\n",
       "           [ 4.5170e-02,  4.6150e-02,  2.6860e-02]],\n",
       " \n",
       "          [[-1.7783e-02,  3.7673e-02,  3.8889e-02],\n",
       "           [-4.2494e-02, -3.3087e-02,  4.2578e-02],\n",
       "           [-1.9450e-02, -4.6741e-02, -2.9636e-02]]]]),\n",
       " 'layer2.0.bias': tensor([-0.0029, -0.0226, -0.0220, -0.0451, -0.0162,  0.0195,  0.0535, -0.0280,\n",
       "         -0.0340,  0.0170,  0.0581,  0.0215, -0.0449,  0.0488,  0.0284,  0.0110,\n",
       "         -0.0304, -0.0164, -0.0357, -0.0168,  0.0295,  0.0206, -0.0200,  0.0095,\n",
       "         -0.0089,  0.0315,  0.0465,  0.0486, -0.0454,  0.0323,  0.0181, -0.0093]),\n",
       " 'layer2.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer2.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.0.weight': tensor([[[[ 0.0349,  0.0080,  0.0391],\n",
       "           [ 0.0168,  0.0509,  0.0433],\n",
       "           [ 0.0178,  0.0479, -0.0166]],\n",
       " \n",
       "          [[-0.0273, -0.0540, -0.0191],\n",
       "           [-0.0348, -0.0212, -0.0429],\n",
       "           [-0.0271, -0.0277,  0.0539]],\n",
       " \n",
       "          [[ 0.0317, -0.0411, -0.0299],\n",
       "           [ 0.0163, -0.0236,  0.0418],\n",
       "           [ 0.0094, -0.0482, -0.0238]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0558,  0.0193, -0.0203],\n",
       "           [-0.0455, -0.0138,  0.0564],\n",
       "           [ 0.0082, -0.0449, -0.0450]],\n",
       " \n",
       "          [[ 0.0185, -0.0095, -0.0317],\n",
       "           [ 0.0377,  0.0436, -0.0243],\n",
       "           [ 0.0119, -0.0278, -0.0025]],\n",
       " \n",
       "          [[-0.0028,  0.0118,  0.0340],\n",
       "           [-0.0475, -0.0382,  0.0570],\n",
       "           [ 0.0181,  0.0342, -0.0580]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0275, -0.0282,  0.0214],\n",
       "           [ 0.0168, -0.0525,  0.0339],\n",
       "           [ 0.0271, -0.0350,  0.0188]],\n",
       " \n",
       "          [[-0.0379,  0.0073,  0.0297],\n",
       "           [-0.0505,  0.0464, -0.0274],\n",
       "           [-0.0421,  0.0336,  0.0277]],\n",
       " \n",
       "          [[-0.0463,  0.0101, -0.0216],\n",
       "           [ 0.0380, -0.0448,  0.0118],\n",
       "           [-0.0039,  0.0029,  0.0205]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0137, -0.0485,  0.0479],\n",
       "           [ 0.0140,  0.0290,  0.0492],\n",
       "           [-0.0348,  0.0098, -0.0530]],\n",
       " \n",
       "          [[ 0.0022,  0.0261,  0.0222],\n",
       "           [-0.0144, -0.0012,  0.0539],\n",
       "           [-0.0323, -0.0358,  0.0455]],\n",
       " \n",
       "          [[-0.0019, -0.0225,  0.0147],\n",
       "           [-0.0320,  0.0233,  0.0163],\n",
       "           [ 0.0575,  0.0426, -0.0043]]],\n",
       " \n",
       " \n",
       "         [[[-0.0363, -0.0312,  0.0324],\n",
       "           [-0.0358,  0.0405, -0.0376],\n",
       "           [-0.0349,  0.0389, -0.0300]],\n",
       " \n",
       "          [[ 0.0290,  0.0511,  0.0255],\n",
       "           [ 0.0415,  0.0384, -0.0417],\n",
       "           [ 0.0170,  0.0173,  0.0497]],\n",
       " \n",
       "          [[-0.0512,  0.0508,  0.0123],\n",
       "           [ 0.0372, -0.0387, -0.0135],\n",
       "           [ 0.0373,  0.0515,  0.0059]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0423, -0.0471, -0.0543],\n",
       "           [-0.0494,  0.0340,  0.0193],\n",
       "           [ 0.0132, -0.0222, -0.0134]],\n",
       " \n",
       "          [[-0.0555,  0.0462, -0.0468],\n",
       "           [-0.0407,  0.0341, -0.0243],\n",
       "           [ 0.0366, -0.0470, -0.0287]],\n",
       " \n",
       "          [[ 0.0117,  0.0141, -0.0320],\n",
       "           [ 0.0347,  0.0127, -0.0439],\n",
       "           [-0.0045,  0.0420,  0.0280]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0276, -0.0193, -0.0327],\n",
       "           [ 0.0056, -0.0327, -0.0540],\n",
       "           [ 0.0408, -0.0589,  0.0439]],\n",
       " \n",
       "          [[ 0.0170,  0.0479,  0.0354],\n",
       "           [-0.0253,  0.0351,  0.0472],\n",
       "           [-0.0431,  0.0366,  0.0009]],\n",
       " \n",
       "          [[-0.0452,  0.0521, -0.0156],\n",
       "           [ 0.0237,  0.0153,  0.0115],\n",
       "           [ 0.0114,  0.0531, -0.0215]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0174, -0.0583, -0.0320],\n",
       "           [ 0.0349,  0.0220,  0.0481],\n",
       "           [-0.0178,  0.0080,  0.0153]],\n",
       " \n",
       "          [[ 0.0321,  0.0128,  0.0074],\n",
       "           [ 0.0360,  0.0581, -0.0427],\n",
       "           [ 0.0326,  0.0437, -0.0364]],\n",
       " \n",
       "          [[ 0.0378, -0.0583, -0.0260],\n",
       "           [-0.0287,  0.0527, -0.0389],\n",
       "           [ 0.0347, -0.0302, -0.0554]]],\n",
       " \n",
       " \n",
       "         [[[-0.0092, -0.0467, -0.0299],\n",
       "           [ 0.0316, -0.0589, -0.0055],\n",
       "           [ 0.0511, -0.0071, -0.0279]],\n",
       " \n",
       "          [[-0.0223,  0.0226, -0.0080],\n",
       "           [ 0.0587, -0.0140, -0.0529],\n",
       "           [-0.0343,  0.0180,  0.0249]],\n",
       " \n",
       "          [[-0.0064,  0.0322,  0.0150],\n",
       "           [-0.0121,  0.0542, -0.0135],\n",
       "           [-0.0501, -0.0066,  0.0361]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0131, -0.0132,  0.0006],\n",
       "           [-0.0170,  0.0108,  0.0513],\n",
       "           [-0.0477, -0.0503, -0.0247]],\n",
       " \n",
       "          [[ 0.0012,  0.0428,  0.0357],\n",
       "           [ 0.0526,  0.0207, -0.0433],\n",
       "           [ 0.0550,  0.0501,  0.0588]],\n",
       " \n",
       "          [[ 0.0333, -0.0109,  0.0188],\n",
       "           [-0.0043, -0.0551,  0.0034],\n",
       "           [ 0.0480,  0.0101, -0.0003]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0053,  0.0389,  0.0388],\n",
       "           [ 0.0026, -0.0537, -0.0568],\n",
       "           [-0.0342,  0.0285, -0.0239]],\n",
       " \n",
       "          [[ 0.0492,  0.0006,  0.0106],\n",
       "           [-0.0431, -0.0318,  0.0212],\n",
       "           [ 0.0052, -0.0454, -0.0238]],\n",
       " \n",
       "          [[ 0.0561,  0.0425, -0.0006],\n",
       "           [-0.0442,  0.0060, -0.0096],\n",
       "           [-0.0088, -0.0156,  0.0323]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0413, -0.0057, -0.0351],\n",
       "           [ 0.0050,  0.0508, -0.0194],\n",
       "           [-0.0276,  0.0149, -0.0189]],\n",
       " \n",
       "          [[-0.0123, -0.0535,  0.0464],\n",
       "           [-0.0081,  0.0378,  0.0531],\n",
       "           [-0.0053, -0.0570, -0.0249]],\n",
       " \n",
       "          [[ 0.0173, -0.0353,  0.0536],\n",
       "           [ 0.0449, -0.0072, -0.0557],\n",
       "           [-0.0291, -0.0258, -0.0001]]]]),\n",
       " 'layer3.0.bias': tensor([-0.0553,  0.0040,  0.0108, -0.0160,  0.0396,  0.0470,  0.0212,  0.0019,\n",
       "          0.0513, -0.0497,  0.0139, -0.0532,  0.0565, -0.0235,  0.0157,  0.0309,\n",
       "          0.0052,  0.0123, -0.0224, -0.0405,  0.0552,  0.0039,  0.0070, -0.0347,\n",
       "          0.0450,  0.0507, -0.0266,  0.0296,  0.0268,  0.0458,  0.0275, -0.0472,\n",
       "          0.0478, -0.0509,  0.0154, -0.0372,  0.0560, -0.0417, -0.0460,  0.0035,\n",
       "         -0.0077,  0.0466, -0.0062, -0.0025, -0.0086,  0.0558,  0.0206,  0.0366,\n",
       "          0.0067,  0.0200, -0.0214, -0.0477, -0.0559, -0.0190,  0.0371,  0.0582,\n",
       "          0.0312, -0.0585, -0.0531, -0.0584,  0.0278, -0.0551,  0.0183,  0.0443]),\n",
       " 'layer3.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer3.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.0.weight': tensor([[[[ 2.5620e-02,  1.1027e-02,  3.8344e-02],\n",
       "           [-5.6369e-04, -1.3636e-02, -1.1476e-02],\n",
       "           [ 2.4599e-02, -1.7833e-02,  1.0314e-02]],\n",
       " \n",
       "          [[-3.6600e-02,  1.6459e-02,  8.5596e-03],\n",
       "           [-2.3940e-02,  3.1190e-02,  1.1611e-03],\n",
       "           [-2.8263e-02, -2.6966e-02, -1.9842e-02]],\n",
       " \n",
       "          [[-2.2325e-02,  1.7213e-02,  1.4972e-02],\n",
       "           [ 1.2437e-03, -1.6358e-02,  2.6994e-02],\n",
       "           [ 6.8585e-03,  4.8376e-04,  2.7526e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.0617e-02, -2.6364e-02,  3.6131e-02],\n",
       "           [ 2.8148e-02,  1.4054e-02, -2.4607e-02],\n",
       "           [-6.4064e-03,  3.6277e-02,  6.3002e-03]],\n",
       " \n",
       "          [[-2.6183e-02, -1.6960e-02, -3.2769e-02],\n",
       "           [ 3.3250e-02, -1.0777e-02,  3.2396e-02],\n",
       "           [-8.2837e-03,  3.9580e-02,  2.3819e-02]],\n",
       " \n",
       "          [[ 1.0965e-02, -3.6820e-02, -1.8887e-02],\n",
       "           [ 5.6978e-03, -3.5669e-02, -1.5734e-02],\n",
       "           [-1.7920e-02,  1.7379e-02, -3.0323e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.7890e-02,  1.4734e-02,  1.2198e-02],\n",
       "           [-2.2439e-02,  4.1251e-02, -2.5036e-02],\n",
       "           [ 5.7460e-03, -1.1964e-02, -2.5474e-02]],\n",
       " \n",
       "          [[-2.0505e-02,  1.5931e-02,  1.7208e-02],\n",
       "           [-2.7738e-02, -3.5588e-02, -8.7796e-03],\n",
       "           [ 2.6947e-03,  2.5921e-03, -1.3987e-02]],\n",
       " \n",
       "          [[-7.3796e-03, -1.7497e-02,  3.8538e-03],\n",
       "           [ 2.2572e-02, -1.0772e-02,  5.4893e-03],\n",
       "           [ 4.1398e-02, -2.8769e-02, -1.4876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5627e-02, -2.2806e-03,  1.0660e-02],\n",
       "           [ 3.1863e-03, -3.0281e-02,  3.9491e-02],\n",
       "           [-2.6706e-02, -4.1496e-02, -3.4536e-02]],\n",
       " \n",
       "          [[ 1.2076e-02, -2.2478e-03,  2.5527e-02],\n",
       "           [-5.1937e-03,  7.0800e-04,  2.4908e-02],\n",
       "           [ 3.5183e-02,  2.8290e-03, -1.1281e-02]],\n",
       " \n",
       "          [[ 3.3812e-02,  2.9348e-02,  2.4271e-02],\n",
       "           [ 4.3343e-03,  1.9623e-02,  2.9991e-02],\n",
       "           [ 3.7585e-02,  6.1731e-03, -1.6640e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5880e-02, -6.0504e-03,  2.0034e-03],\n",
       "           [-3.7352e-02, -7.3350e-03,  2.5647e-02],\n",
       "           [ 2.4312e-02, -8.3213e-03,  3.5337e-03]],\n",
       " \n",
       "          [[-3.0441e-02,  6.6977e-03, -1.9300e-02],\n",
       "           [ 3.0446e-02, -7.7699e-03, -5.5637e-03],\n",
       "           [ 1.3507e-02,  2.9579e-02,  3.1732e-02]],\n",
       " \n",
       "          [[-1.2611e-02,  2.5706e-02,  6.1131e-03],\n",
       "           [ 6.8956e-03, -1.2875e-02,  1.7288e-02],\n",
       "           [ 6.3988e-03,  1.7031e-02, -1.8602e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.2298e-03,  3.1480e-02, -2.4839e-02],\n",
       "           [-9.5341e-05, -3.4622e-02,  3.4004e-02],\n",
       "           [-1.9048e-02,  1.2274e-02, -3.6575e-02]],\n",
       " \n",
       "          [[ 2.3614e-02, -1.8650e-02, -2.2995e-02],\n",
       "           [ 1.8063e-02,  3.1919e-02, -3.8414e-02],\n",
       "           [ 3.2621e-02, -2.6842e-02,  2.1469e-02]],\n",
       " \n",
       "          [[ 2.2439e-02,  2.9607e-02, -3.4393e-03],\n",
       "           [ 1.2774e-02, -1.5567e-02,  2.1473e-02],\n",
       "           [-3.5595e-02, -1.9984e-02,  1.7679e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-4.8253e-03, -3.3776e-02,  4.0900e-02],\n",
       "           [ 2.1382e-02,  2.7088e-02,  4.6202e-04],\n",
       "           [-3.8172e-04,  1.4354e-02, -2.0458e-02]],\n",
       " \n",
       "          [[ 2.8361e-02, -3.9402e-02,  2.3592e-02],\n",
       "           [-1.6914e-02,  3.1797e-02,  1.0471e-02],\n",
       "           [-3.2137e-02,  2.4701e-03, -2.4310e-02]],\n",
       " \n",
       "          [[ 3.4360e-02,  9.9863e-03,  3.2582e-02],\n",
       "           [-3.5713e-02,  2.1561e-02,  2.0948e-02],\n",
       "           [ 2.8734e-02, -2.4535e-02, -1.9657e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.8352e-02, -3.0637e-02, -4.1497e-02],\n",
       "           [-2.1351e-02,  1.9204e-02,  6.9097e-03],\n",
       "           [ 9.2071e-03, -2.6410e-02, -2.4193e-03]],\n",
       " \n",
       "          [[ 3.6037e-02,  3.7034e-02,  2.1712e-02],\n",
       "           [-1.5547e-02,  4.0763e-02, -2.4413e-02],\n",
       "           [-3.5063e-02, -6.3464e-03,  1.9633e-02]],\n",
       " \n",
       "          [[-1.1322e-02, -1.5831e-02, -2.7898e-02],\n",
       "           [-1.1176e-02, -4.0802e-02, -1.6075e-03],\n",
       "           [ 7.4114e-03, -1.5146e-02, -2.5475e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.6118e-02,  2.1552e-02, -3.4662e-02],\n",
       "           [-2.2085e-03,  1.5897e-02,  3.3573e-02],\n",
       "           [ 3.4382e-02,  2.0942e-02, -6.2279e-03]],\n",
       " \n",
       "          [[ 2.4146e-02, -3.4124e-02, -3.2239e-02],\n",
       "           [ 2.6915e-02,  1.0536e-02, -3.2622e-02],\n",
       "           [ 2.0656e-02, -1.9801e-02,  4.4186e-03]],\n",
       " \n",
       "          [[-1.3196e-02, -6.5854e-04,  3.5568e-02],\n",
       "           [-4.1158e-02,  1.5582e-02, -3.1423e-02],\n",
       "           [-9.3038e-04,  4.6161e-03, -1.9687e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6987e-03,  7.5396e-03, -7.5504e-03],\n",
       "           [ 9.4232e-03,  2.9016e-02,  2.9479e-02],\n",
       "           [-3.7800e-02,  7.1434e-03, -2.1037e-02]],\n",
       " \n",
       "          [[-1.7767e-02,  7.6964e-03,  6.1694e-03],\n",
       "           [ 2.9679e-02,  1.3413e-02, -7.4646e-04],\n",
       "           [-1.9178e-02, -9.6990e-03, -3.2808e-02]],\n",
       " \n",
       "          [[-3.6097e-02,  2.4256e-02,  3.8199e-02],\n",
       "           [-2.4204e-02, -3.6429e-02, -2.7111e-02],\n",
       "           [ 2.0422e-02,  3.4729e-03,  3.5056e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.2510e-02, -1.9212e-02,  2.3803e-02],\n",
       "           [ 1.3745e-02, -2.2359e-02,  2.6260e-02],\n",
       "           [-4.0578e-02,  3.3985e-02,  2.1799e-02]],\n",
       " \n",
       "          [[-6.0673e-03,  3.1644e-02,  3.4686e-03],\n",
       "           [ 3.6875e-02, -4.1006e-02, -6.1496e-03],\n",
       "           [ 7.4729e-03,  1.8152e-03, -1.0912e-02]],\n",
       " \n",
       "          [[-2.6291e-02, -1.4366e-03,  2.9217e-03],\n",
       "           [ 1.3097e-02,  3.0829e-02, -3.4937e-02],\n",
       "           [-3.4530e-02, -3.6971e-02,  1.4080e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.6622e-03,  2.5039e-02,  2.8354e-02],\n",
       "           [ 2.3704e-02,  8.1194e-03,  1.8403e-02],\n",
       "           [ 3.3893e-02, -2.9643e-02, -2.5597e-02]],\n",
       " \n",
       "          [[-5.5600e-03,  3.8782e-02,  1.3900e-03],\n",
       "           [-1.6761e-02,  9.2933e-03,  9.7108e-03],\n",
       "           [-2.1357e-03,  3.8266e-02,  3.8584e-02]],\n",
       " \n",
       "          [[ 1.8071e-02,  7.9090e-03,  1.1067e-02],\n",
       "           [ 1.0860e-02,  9.1985e-03,  9.1101e-03],\n",
       "           [ 5.3729e-03, -2.3801e-02, -3.0802e-02]]]]),\n",
       " 'layer4.0.bias': tensor([ 0.0177, -0.0163, -0.0093,  0.0162, -0.0377,  0.0078,  0.0001,  0.0027,\n",
       "         -0.0377,  0.0106, -0.0282,  0.0339, -0.0121, -0.0010, -0.0151, -0.0266,\n",
       "          0.0306,  0.0092, -0.0270,  0.0387,  0.0071,  0.0043,  0.0337, -0.0290,\n",
       "          0.0054, -0.0309, -0.0099,  0.0037,  0.0143,  0.0311, -0.0226, -0.0143,\n",
       "         -0.0285, -0.0150,  0.0323,  0.0416,  0.0100, -0.0162, -0.0373,  0.0130,\n",
       "         -0.0095,  0.0198,  0.0192, -0.0277,  0.0032,  0.0406,  0.0073, -0.0256,\n",
       "         -0.0246,  0.0203,  0.0312, -0.0264, -0.0383, -0.0415, -0.0085, -0.0052,\n",
       "         -0.0147,  0.0275,  0.0078,  0.0238,  0.0250,  0.0253, -0.0249,  0.0367]),\n",
       " 'layer4.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer4.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer5.0.weight': tensor([[[[ 4.0029e-02, -1.5555e-02, -1.9442e-02],\n",
       "           [ 2.7807e-02,  1.2606e-03,  2.2996e-02],\n",
       "           [ 1.6510e-02, -1.8767e-02,  1.4049e-02]],\n",
       " \n",
       "          [[-2.0909e-02,  3.2355e-03,  4.8950e-04],\n",
       "           [ 3.5541e-02, -2.6028e-02, -2.1366e-02],\n",
       "           [-1.8651e-02, -1.4314e-02, -1.5306e-04]],\n",
       " \n",
       "          [[-2.3171e-02,  1.9146e-02,  1.5210e-03],\n",
       "           [ 1.9624e-02, -1.5651e-02, -2.6014e-02],\n",
       "           [-3.2673e-02,  1.8227e-02,  3.6811e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4583e-02, -7.9460e-06, -1.4085e-02],\n",
       "           [ 2.8421e-02, -1.1669e-02,  1.9115e-02],\n",
       "           [ 3.0063e-02,  1.8295e-02, -2.4148e-02]],\n",
       " \n",
       "          [[-3.0112e-02,  3.7937e-02,  3.5968e-02],\n",
       "           [ 1.3086e-02, -1.0907e-02,  2.0002e-02],\n",
       "           [-5.9486e-03,  6.4267e-03, -1.3565e-02]],\n",
       " \n",
       "          [[ 3.3119e-02,  2.9293e-02,  4.7366e-03],\n",
       "           [ 1.3828e-02, -8.6379e-04, -3.8102e-02],\n",
       "           [ 2.7153e-02,  3.1091e-02,  1.0126e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.4183e-03, -1.7757e-03, -2.0380e-02],\n",
       "           [-1.5854e-02, -2.6645e-02,  7.1543e-03],\n",
       "           [ 6.6951e-03, -1.8744e-02, -1.2158e-02]],\n",
       " \n",
       "          [[ 1.2498e-02,  2.4377e-02,  2.7904e-02],\n",
       "           [ 3.8156e-02,  3.0734e-02,  8.7722e-03],\n",
       "           [ 2.6730e-02, -3.1139e-02,  3.4744e-02]],\n",
       " \n",
       "          [[ 2.6575e-02,  2.4073e-02, -3.0729e-02],\n",
       "           [ 2.7972e-02, -3.6623e-02,  2.2270e-02],\n",
       "           [-2.7218e-02,  3.9759e-02, -2.6646e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.4700e-03,  2.2825e-03,  3.6883e-02],\n",
       "           [ 2.7217e-02, -3.1232e-02,  3.1608e-02],\n",
       "           [-1.0017e-02,  3.1206e-02, -3.9591e-02]],\n",
       " \n",
       "          [[-2.9666e-02, -4.0470e-02, -6.2156e-03],\n",
       "           [ 3.0331e-02, -3.6313e-04, -1.9975e-02],\n",
       "           [-3.7550e-02,  2.9411e-02, -3.2907e-02]],\n",
       " \n",
       "          [[ 2.4071e-03,  2.2413e-02,  2.5404e-02],\n",
       "           [ 1.3434e-02, -3.0588e-02,  9.5532e-04],\n",
       "           [-3.2427e-02,  1.6529e-02, -1.6419e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.3955e-02,  7.1121e-03,  2.6891e-02],\n",
       "           [ 4.0416e-02, -3.3873e-02, -1.3531e-02],\n",
       "           [ 1.8637e-02,  5.2441e-05,  2.1841e-02]],\n",
       " \n",
       "          [[-1.7873e-02,  3.0563e-02,  1.1849e-02],\n",
       "           [ 9.3570e-03, -1.1101e-02,  8.0893e-03],\n",
       "           [-8.1875e-03, -1.7321e-02, -9.3326e-05]],\n",
       " \n",
       "          [[-2.1851e-02,  4.0022e-02,  8.1340e-03],\n",
       "           [ 1.9282e-02, -2.6939e-02,  1.7700e-02],\n",
       "           [ 7.5512e-03,  3.9676e-02, -3.6485e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.8432e-03,  1.4899e-02,  2.2023e-02],\n",
       "           [-5.1086e-03, -4.1410e-03, -1.7824e-02],\n",
       "           [ 3.6685e-02, -3.0121e-02, -2.4863e-02]],\n",
       " \n",
       "          [[ 7.5307e-03,  2.6082e-02,  2.2635e-02],\n",
       "           [-3.3264e-02, -1.2277e-02, -1.2866e-02],\n",
       "           [ 3.9813e-02, -1.0466e-02, -7.6253e-03]],\n",
       " \n",
       "          [[-3.7760e-02,  7.5000e-03, -5.7763e-03],\n",
       "           [-3.5322e-02,  3.6979e-02,  2.8847e-03],\n",
       "           [ 6.5473e-03,  3.5336e-02, -5.7323e-03]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 3.4297e-02, -3.6211e-02,  2.7521e-02],\n",
       "           [-9.7694e-03,  1.5891e-02,  1.4592e-02],\n",
       "           [-3.0556e-02,  9.3818e-03,  4.0286e-02]],\n",
       " \n",
       "          [[-2.7050e-02, -1.7057e-02,  2.0408e-02],\n",
       "           [ 2.3277e-02,  3.4720e-02, -2.5510e-02],\n",
       "           [ 3.2969e-02, -3.7801e-02, -2.1019e-02]],\n",
       " \n",
       "          [[ 3.2294e-02, -5.6080e-03,  3.2732e-02],\n",
       "           [ 8.6077e-03,  2.3426e-02, -2.1425e-02],\n",
       "           [-2.0202e-03,  2.7345e-02,  3.4072e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.1652e-02,  3.0896e-02, -2.1383e-02],\n",
       "           [ 3.9330e-02, -3.5811e-02,  1.0374e-02],\n",
       "           [ 1.3004e-02,  1.1354e-02, -3.0207e-02]],\n",
       " \n",
       "          [[-1.4926e-02, -2.4606e-02, -2.8183e-02],\n",
       "           [ 2.0890e-03,  3.1961e-02, -2.7149e-02],\n",
       "           [ 3.3376e-02, -4.2753e-03,  1.3732e-02]],\n",
       " \n",
       "          [[-3.1865e-02,  2.2441e-02, -2.7422e-02],\n",
       "           [ 3.6593e-02, -1.8769e-02, -1.5301e-02],\n",
       "           [ 1.6204e-02,  1.2891e-02, -2.4532e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.5621e-02, -7.0786e-03, -1.0761e-02],\n",
       "           [-5.0155e-03, -3.1408e-02,  3.0269e-02],\n",
       "           [ 3.1515e-02, -2.1544e-02,  2.6874e-04]],\n",
       " \n",
       "          [[-4.1376e-02, -6.8147e-04, -2.9623e-02],\n",
       "           [-2.8259e-02,  3.9103e-03, -2.5514e-02],\n",
       "           [ 4.5687e-03, -1.8919e-02, -7.3692e-03]],\n",
       " \n",
       "          [[-2.4708e-02, -2.0455e-02, -1.3034e-02],\n",
       "           [ 2.2828e-02,  4.3709e-04,  3.2861e-02],\n",
       "           [ 1.2342e-02,  3.5667e-02, -2.3626e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.7968e-02,  2.0814e-02, -1.4589e-02],\n",
       "           [ 3.5259e-02,  3.3609e-02,  1.2301e-02],\n",
       "           [ 7.3563e-03, -1.7608e-02,  2.7465e-02]],\n",
       " \n",
       "          [[ 6.1034e-03, -1.7165e-02, -6.9117e-03],\n",
       "           [ 2.9456e-02,  3.4874e-02,  1.7228e-02],\n",
       "           [ 3.4576e-02,  1.7019e-02, -2.9988e-02]],\n",
       " \n",
       "          [[-3.0170e-02, -2.8234e-02,  3.7775e-02],\n",
       "           [ 3.1383e-02,  3.2691e-02,  3.0495e-02],\n",
       "           [ 3.4685e-03, -1.7589e-02,  2.4322e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7058e-04, -2.9093e-02, -4.0108e-02],\n",
       "           [ 3.2973e-02,  1.0041e-02,  2.1036e-02],\n",
       "           [-2.9385e-02, -6.8148e-03,  3.9885e-02]],\n",
       " \n",
       "          [[ 2.3979e-03, -3.9971e-02, -1.7004e-02],\n",
       "           [-3.3634e-02,  1.1719e-02, -2.8307e-02],\n",
       "           [-3.2483e-02, -1.1167e-02,  1.5263e-02]],\n",
       " \n",
       "          [[ 6.7131e-03,  3.7708e-02,  3.4297e-02],\n",
       "           [ 2.8094e-02,  3.8445e-02,  3.6810e-05],\n",
       "           [ 8.6228e-03,  1.4023e-02,  2.4469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.1655e-02,  3.6069e-02, -1.6624e-02],\n",
       "           [ 2.2531e-02,  3.7367e-02, -2.7337e-02],\n",
       "           [-5.4987e-03, -3.2980e-03, -2.1241e-02]],\n",
       " \n",
       "          [[-2.5833e-03,  4.0529e-02, -2.8201e-02],\n",
       "           [ 7.9990e-03, -1.9770e-02, -9.0786e-03],\n",
       "           [ 1.9084e-02,  1.7834e-02,  1.1820e-02]],\n",
       " \n",
       "          [[-2.1395e-02, -1.0014e-02,  3.2037e-02],\n",
       "           [ 7.6473e-03,  2.1263e-02,  2.7528e-02],\n",
       "           [ 6.0682e-03,  1.0778e-02,  3.1273e-02]]]]),\n",
       " 'layer5.0.bias': tensor([-0.0339, -0.0223, -0.0282,  0.0156, -0.0012,  0.0039, -0.0381, -0.0168,\n",
       "         -0.0001,  0.0406, -0.0392, -0.0376,  0.0081, -0.0122,  0.0069, -0.0187,\n",
       "         -0.0228,  0.0056,  0.0213,  0.0152, -0.0076,  0.0395,  0.0266,  0.0247,\n",
       "         -0.0403, -0.0306,  0.0379, -0.0167, -0.0330, -0.0408, -0.0215,  0.0353,\n",
       "         -0.0026, -0.0162, -0.0388, -0.0065, -0.0216,  0.0155, -0.0153, -0.0108,\n",
       "         -0.0357, -0.0315, -0.0267, -0.0286, -0.0062, -0.0391,  0.0140, -0.0273,\n",
       "          0.0051,  0.0157, -0.0315, -0.0401,  0.0289, -0.0257, -0.0148,  0.0070,\n",
       "          0.0054,  0.0089, -0.0287, -0.0140, -0.0269,  0.0319, -0.0325, -0.0247]),\n",
       " 'linear.weight': tensor([[ 0.0916,  0.0274,  0.0482, -0.0863,  0.0164, -0.0115, -0.0446,  0.1144,\n",
       "           0.0822,  0.0646,  0.0713,  0.0645,  0.0248,  0.0175,  0.0264,  0.0958,\n",
       "           0.1223,  0.0784, -0.0136,  0.1037,  0.0573,  0.0770, -0.0095, -0.0206,\n",
       "           0.1065,  0.0468,  0.0714,  0.1024,  0.0972, -0.0708, -0.0112,  0.0418,\n",
       "          -0.0384, -0.1069, -0.0260, -0.1035,  0.0921, -0.0480, -0.0806, -0.0412,\n",
       "           0.1044, -0.1037,  0.0918,  0.1132, -0.0616,  0.0739,  0.0543,  0.0419,\n",
       "          -0.1071,  0.0925, -0.0029, -0.0504, -0.1013, -0.1048, -0.1139,  0.0840,\n",
       "           0.0123, -0.0573,  0.0097,  0.1013,  0.0002,  0.0757, -0.1092, -0.0977],\n",
       "         [-0.0976,  0.0218,  0.1236, -0.0156, -0.0988, -0.0866,  0.0803,  0.0230,\n",
       "          -0.0454, -0.0290,  0.1180, -0.1043, -0.1040, -0.0692, -0.0319,  0.0671,\n",
       "           0.0057,  0.0187,  0.1229,  0.0675,  0.0289, -0.0853,  0.0172,  0.0875,\n",
       "          -0.0131, -0.0520, -0.0023, -0.0837, -0.1215,  0.0070,  0.0172, -0.0861,\n",
       "           0.0510,  0.1128,  0.0698, -0.0959,  0.1047, -0.0330,  0.0005,  0.0720,\n",
       "           0.0032, -0.0150, -0.0510, -0.0521,  0.1005,  0.0592, -0.1216, -0.1142,\n",
       "           0.1248,  0.0489, -0.0867, -0.0501, -0.0520,  0.0541, -0.0905,  0.0906,\n",
       "           0.0294,  0.0876,  0.0433, -0.0863,  0.0969, -0.0604, -0.1024, -0.0643],\n",
       "         [ 0.1154, -0.1186, -0.0199, -0.0405, -0.1105,  0.0765,  0.0550, -0.0141,\n",
       "          -0.1237, -0.0296, -0.0304,  0.1103, -0.1205,  0.0008,  0.0556,  0.0408,\n",
       "           0.0326, -0.0969,  0.1037, -0.0864, -0.0293, -0.0055, -0.0150,  0.0192,\n",
       "          -0.0729, -0.0636,  0.0296, -0.0577, -0.0287, -0.0820, -0.0236,  0.0443,\n",
       "          -0.0256, -0.0003, -0.0927, -0.0993,  0.0258,  0.1160, -0.0434, -0.0916,\n",
       "          -0.0447, -0.0580,  0.0066,  0.0233,  0.1130, -0.1072, -0.0153, -0.0757,\n",
       "          -0.1002, -0.0961,  0.0014, -0.0149, -0.1029,  0.1006, -0.0517,  0.0877,\n",
       "          -0.0534, -0.0673,  0.0225,  0.0355,  0.1090, -0.1145,  0.1039, -0.0724],\n",
       "         [ 0.0105, -0.0863,  0.0174,  0.0673, -0.0783, -0.0996,  0.0594, -0.0393,\n",
       "          -0.0898,  0.0047, -0.1083,  0.0007,  0.0800, -0.1056,  0.0060, -0.1029,\n",
       "           0.0600,  0.0489, -0.0016,  0.0796,  0.0368, -0.0254, -0.1187, -0.0487,\n",
       "          -0.0344, -0.0922, -0.0645, -0.0865,  0.0391,  0.0313,  0.0412,  0.1113,\n",
       "          -0.1223, -0.0501, -0.0932, -0.1201,  0.0443,  0.0366, -0.0723,  0.0465,\n",
       "           0.0954,  0.0633,  0.0466, -0.0724, -0.0795, -0.0819, -0.1092,  0.0291,\n",
       "           0.0500,  0.0143, -0.0998, -0.0054,  0.1042,  0.0176, -0.0459,  0.0192,\n",
       "           0.0816, -0.0294, -0.0316, -0.0542,  0.0943, -0.0240,  0.0372, -0.0403],\n",
       "         [-0.0505, -0.0937, -0.1059,  0.0898,  0.1196, -0.1027,  0.0251,  0.0276,\n",
       "          -0.0344,  0.0518, -0.0177,  0.1089,  0.0241,  0.0386, -0.0508,  0.0748,\n",
       "          -0.0992,  0.0156,  0.0612, -0.1040, -0.0413,  0.0628,  0.0228, -0.1098,\n",
       "           0.1152,  0.0852, -0.0362,  0.0163,  0.0398,  0.0036,  0.1198,  0.0906,\n",
       "           0.0250,  0.1074,  0.0016, -0.1161, -0.1173, -0.0486, -0.0867,  0.1088,\n",
       "          -0.0890,  0.0498,  0.0614, -0.1214, -0.0523,  0.0367, -0.0253,  0.0643,\n",
       "          -0.0216,  0.0619,  0.0555,  0.1103, -0.0463, -0.1167, -0.0215, -0.0346,\n",
       "          -0.0006, -0.0089, -0.0418,  0.0205,  0.0769, -0.0936,  0.0747, -0.0514],\n",
       "         [-0.1041, -0.0652, -0.1099, -0.1172, -0.1183,  0.0261, -0.0061, -0.0181,\n",
       "           0.0306,  0.1034,  0.1023, -0.0415, -0.0590,  0.1013, -0.0350, -0.1135,\n",
       "           0.1211,  0.0423,  0.0534, -0.0943,  0.0192, -0.1066,  0.0101, -0.0907,\n",
       "          -0.0641, -0.0925,  0.0244,  0.0263,  0.0090, -0.0468,  0.0813,  0.0006,\n",
       "           0.0742,  0.0435,  0.0780, -0.0913,  0.0158, -0.1105, -0.0535, -0.0402,\n",
       "          -0.0937,  0.0195,  0.0634, -0.0159,  0.1228, -0.0416,  0.0739,  0.0429,\n",
       "           0.0029, -0.0882, -0.0594, -0.0101,  0.0290, -0.0891,  0.0971, -0.0061,\n",
       "          -0.0823, -0.0365,  0.0256, -0.1039,  0.1206,  0.1249,  0.1118, -0.0718]]),\n",
       " 'linear.bias': tensor([ 0.0939, -0.0072, -0.0662,  0.0920, -0.0422,  0.0693])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8efa1b90-371e-417b-a858-a69f483f1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in current_params.items():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7647f06d-cf3e-4ea2-b2e1-2d090d75063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer0.0.weight'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3772ba6-d512-4a43-8284-37e1f6cf45f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  5.9497,  -5.0966,  -2.2385,   5.4198, -10.8708],\n",
       "          [  6.9249,  -2.3755,   5.8745,   1.6052,  -1.4139],\n",
       "          [  3.2027,   0.5696,   4.2173,  -4.4999,  -0.8419],\n",
       "          [ -1.0395,   1.6737,  -0.0461,  10.0940,   3.5933],\n",
       "          [ -4.3002,  -6.9739,  -1.9355,  -4.9807,  -3.7002]],\n",
       "\n",
       "         [[  0.5529,   6.8835,   6.2762, -11.2878,   7.1582],\n",
       "          [  3.2258,  10.9530,   7.6217, -10.5209, -10.9790],\n",
       "          [ -5.5695,  10.1396,  -1.9234,   4.9416,  -5.3660],\n",
       "          [ 11.3301,  -4.8855,   8.6594,   0.1367,  -6.0832],\n",
       "          [  5.9353,  -6.1294,   3.3960,  -3.3343,  -1.2659]],\n",
       "\n",
       "         [[-11.1012,  -5.5054,   6.2658,  -2.8068,  11.5014],\n",
       "          [  9.2559,  -0.5407,  -7.7074,   7.0317,   3.5837],\n",
       "          [ -7.4642,   7.5003,   7.0102,  10.2410,  -6.4728],\n",
       "          [ -1.9007,  -0.2237,   1.6865,  -8.7632,  -8.1940],\n",
       "          [  6.2816,  -2.7077,   5.6404,   0.6583,   3.7914]]],\n",
       "\n",
       "\n",
       "        [[[  2.5391,   4.1985,   5.7240, -10.6938,   5.8122],\n",
       "          [ -8.1190,  -8.7123,   0.7022,  -1.9677,   6.7818],\n",
       "          [ -6.6873, -10.2651,   8.4036,  -1.7122,   6.4947],\n",
       "          [  3.7122,  -8.6588,   2.3198,   2.7736,  -7.7314],\n",
       "          [ -5.4787,   3.9372,   2.0683,  -4.9118,  -3.4955]],\n",
       "\n",
       "         [[ 10.5757,  -2.1364,   6.5101,   4.9999,  -7.4641],\n",
       "          [ -9.8199,  11.0838,   0.6029,   7.9150,   2.3929],\n",
       "          [  3.7138,   8.6251,  10.9490,  -7.6624,   1.4437],\n",
       "          [  8.6170,   8.3651,   7.1734,  -8.3568,  -8.3150],\n",
       "          [ -6.9833,   1.4506,  11.5085,  -7.2940,   6.1532]],\n",
       "\n",
       "         [[ -6.3911, -10.8567,  -2.4542,   6.6530,  10.7199],\n",
       "          [ -7.1711,   2.5058,   9.9635,   7.6517,   7.1964],\n",
       "          [  8.2057,   7.3037,   2.9825,  -7.8961,  -9.6967],\n",
       "          [ -5.2911,  -1.3446,  -7.0778,   4.2245,   3.5737],\n",
       "          [ -2.6146,   4.4380,   3.7318,   7.0499,   7.7763]]],\n",
       "\n",
       "\n",
       "        [[[ -3.9104,  11.2817,  -1.3349,  -0.3967, -10.8990],\n",
       "          [ -7.4318,  -6.7458,  -4.9389,   8.2096,  -3.7733],\n",
       "          [ -8.6289,   4.4427,   3.6970,   7.4789,  -5.9753],\n",
       "          [  2.5036,  -4.2037,  -2.5943,  -9.2020,  -5.2627],\n",
       "          [ -3.5364,   4.9379,   2.1095,   2.8523,  11.5251]],\n",
       "\n",
       "         [[ 11.2540,   7.8756,   0.3677,  -7.9881,   9.0241],\n",
       "          [ -2.8875,  -0.9335,  -9.9477,  -2.2809,  -7.4477],\n",
       "          [ 10.6106,  -9.9826,  -9.0007,  -0.3921,  -6.2436],\n",
       "          [  4.1317,  -4.4447,  -5.4235,   0.6540,   8.3589],\n",
       "          [ -8.1224,   5.4225,   7.4183,  11.2952,  -8.0821]],\n",
       "\n",
       "         [[  2.7966,  -8.5382,   9.8583,  -4.4792,   6.9554],\n",
       "          [  0.3438,  -0.8993,  -0.3684,   1.9629,   5.4432],\n",
       "          [  1.8519,   3.5225, -10.3868,   8.4124,  10.0674],\n",
       "          [  9.5448,   8.5358,  -8.3327,  -4.2805,  10.1813],\n",
       "          [ -8.7949,  10.4757,  -9.0817,  -8.1338,   5.6451]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0.5785,  -1.5470,   5.8621,  -4.9106,   9.0830],\n",
       "          [ -7.8747,  -6.0089,   3.9993,   7.0963,  -0.1414],\n",
       "          [ -9.0643,  -0.4613,  -3.8983,  -3.6896,  -2.1343],\n",
       "          [  2.7781,  -2.6878,  -8.0975,  -5.7654,  -9.6718],\n",
       "          [-11.5337,   5.5803,   6.4519,   8.7258,   0.9141]],\n",
       "\n",
       "         [[ -4.7495,  -6.9448,   5.7512,  -6.9783,   8.6292],\n",
       "          [ -6.2627, -10.7719,  -8.3157,   9.4045,   8.7158],\n",
       "          [  2.8183,   0.2671,   9.9007,   6.9682,   6.8369],\n",
       "          [ -7.8590,   1.8477,  10.4618,  -8.1341,  11.5038],\n",
       "          [ -7.5630,  -3.3931,  -0.5445,   3.0037,   3.5486]],\n",
       "\n",
       "         [[ -6.6042,   5.2851,   3.0560,  -1.2854, -11.4925],\n",
       "          [ 11.4281,  -4.2702,  -6.1548,  11.4341,  -5.8515],\n",
       "          [  7.7141,   7.0461,  -7.5158,  -9.3122,  -6.2774],\n",
       "          [  3.6140,  10.8924,  -0.6410,  -3.4488,  -3.3219],\n",
       "          [ -8.0417,  11.5395,  -0.1252,  -9.1377,  -7.8885]]],\n",
       "\n",
       "\n",
       "        [[[  7.1837,   5.5851,   7.4495,  -1.2173,  -2.2814],\n",
       "          [  3.7833,  -0.1131,  -3.7174,   2.7537,  -6.5808],\n",
       "          [ -7.9951,  -2.5525,  -8.2314,   0.2932,   1.8172],\n",
       "          [ 11.3064,   3.3278,  -0.0676,  -8.3921,  -4.6413],\n",
       "          [ -1.8824,  -8.3144,   2.2477,   8.3106,   6.6069]],\n",
       "\n",
       "         [[ -4.7889,  -2.8228,  11.5115,  -1.8488, -10.3705],\n",
       "          [ -0.1014,   0.5871,  -4.2897,  -1.2534,  -9.4160],\n",
       "          [  3.5559,  -8.0136,   3.9742,   8.7004,  -7.4547],\n",
       "          [ -4.5211,   1.1106,   0.0278,   6.7702,  -0.6923],\n",
       "          [  1.8667,  -3.1918,   3.8085,   3.4887,  -9.2971]],\n",
       "\n",
       "         [[  6.1298,  -2.3664,  10.3399, -11.4589,  -1.1463],\n",
       "          [  5.2175,   7.9625,   0.1501,  -6.2401,  -8.9258],\n",
       "          [ -8.7440,   9.2972, -10.0563,  -4.5691,   6.0528],\n",
       "          [  6.3145, -10.1006,   2.2893,   1.3960,  10.5979],\n",
       "          [ -0.4095,   4.9035,  -0.9980,  10.3067,  -1.1787]]],\n",
       "\n",
       "\n",
       "        [[[ -6.7853,   2.1298,  -6.1120,   2.8874,   8.0265],\n",
       "          [ -8.9812,  -8.4009,   0.1575,  -0.6754,  -3.5937],\n",
       "          [  9.6654,   5.1765,  -3.8285,   1.6735,  -1.7373],\n",
       "          [ -7.3930,   4.7244,  -7.6180,   1.3974,   7.8387],\n",
       "          [-10.6052,   0.5061,  10.6636,   5.2149,  -4.5538]],\n",
       "\n",
       "         [[  1.8172, -10.8420,   1.8541,  10.0396,  10.0343],\n",
       "          [ -0.1844,   4.1863,  -1.9164,  -3.2412,   0.2221],\n",
       "          [ -6.7261,   4.6491,   5.6610,  -3.1302,  -2.6272],\n",
       "          [ -8.0464,   1.4764,   4.5924,  10.9627, -10.8798],\n",
       "          [ -9.7351,   0.3265,  10.6712,   0.7431,   3.2052]],\n",
       "\n",
       "         [[  1.6394,  -1.3808,   8.2412,   8.4764,  -6.6573],\n",
       "          [ -6.9152,  -8.9787,  -2.8082,  -0.2601,   9.6647],\n",
       "          [-11.1754,   8.1955,  -6.4500,   0.9432,  -0.5396],\n",
       "          [  6.5220,  -9.6597,  -8.0637,   0.2648,   5.8231],\n",
       "          [ -5.5553,   3.5788,  -3.0515, -10.7678,   1.3180]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params[name]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c3c53d6-84e5-4c7a-8d0c-657a283553ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.5399e+01, 2.5975e+01, 5.0110e+00, 2.9374e+01, 1.1817e+02],\n",
       "          [4.7955e+01, 5.6431e+00, 3.4509e+01, 2.5768e+00, 1.9990e+00],\n",
       "          [1.0257e+01, 3.2448e-01, 1.7786e+01, 2.0249e+01, 7.0876e-01],\n",
       "          [1.0807e+00, 2.8012e+00, 2.1278e-03, 1.0189e+02, 1.2912e+01],\n",
       "          [1.8492e+01, 4.8636e+01, 3.7460e+00, 2.4807e+01, 1.3692e+01]],\n",
       "\n",
       "         [[3.0569e-01, 4.7382e+01, 3.9391e+01, 1.2741e+02, 5.1240e+01],\n",
       "          [1.0406e+01, 1.1997e+02, 5.8091e+01, 1.1069e+02, 1.2054e+02],\n",
       "          [3.1019e+01, 1.0281e+02, 3.6994e+00, 2.4420e+01, 2.8794e+01],\n",
       "          [1.2837e+02, 2.3868e+01, 7.4985e+01, 1.8697e-02, 3.7005e+01],\n",
       "          [3.5228e+01, 3.7569e+01, 1.1533e+01, 1.1117e+01, 1.6026e+00]],\n",
       "\n",
       "         [[1.2324e+02, 3.0309e+01, 3.9260e+01, 7.8782e+00, 1.3228e+02],\n",
       "          [8.5673e+01, 2.9233e-01, 5.9404e+01, 4.9445e+01, 1.2843e+01],\n",
       "          [5.5714e+01, 5.6254e+01, 4.9143e+01, 1.0488e+02, 4.1897e+01],\n",
       "          [3.6127e+00, 5.0036e-02, 2.8444e+00, 7.6794e+01, 6.7142e+01],\n",
       "          [3.9459e+01, 7.3315e+00, 3.1814e+01, 4.3335e-01, 1.4375e+01]]],\n",
       "\n",
       "\n",
       "        [[[6.4468e+00, 1.7627e+01, 3.2764e+01, 1.1436e+02, 3.3782e+01],\n",
       "          [6.5918e+01, 7.5905e+01, 4.9312e-01, 3.8718e+00, 4.5993e+01],\n",
       "          [4.4720e+01, 1.0537e+02, 7.0620e+01, 2.9317e+00, 4.2181e+01],\n",
       "          [1.3780e+01, 7.4975e+01, 5.3814e+00, 7.6926e+00, 5.9775e+01],\n",
       "          [3.0017e+01, 1.5502e+01, 4.2780e+00, 2.4126e+01, 1.2218e+01]],\n",
       "\n",
       "         [[1.1185e+02, 4.5644e+00, 4.2381e+01, 2.4999e+01, 5.5712e+01],\n",
       "          [9.6431e+01, 1.2285e+02, 3.6355e-01, 6.2647e+01, 5.7260e+00],\n",
       "          [1.3792e+01, 7.4393e+01, 1.1988e+02, 5.8712e+01, 2.0843e+00],\n",
       "          [7.4253e+01, 6.9974e+01, 5.1458e+01, 6.9836e+01, 6.9140e+01],\n",
       "          [4.8766e+01, 2.1043e+00, 1.3245e+02, 5.3202e+01, 3.7862e+01]],\n",
       "\n",
       "         [[4.0847e+01, 1.1787e+02, 6.0231e+00, 4.4263e+01, 1.1492e+02],\n",
       "          [5.1425e+01, 6.2792e+00, 9.9270e+01, 5.8548e+01, 5.1788e+01],\n",
       "          [6.7333e+01, 5.3344e+01, 8.8956e+00, 6.2348e+01, 9.4026e+01],\n",
       "          [2.7995e+01, 1.8080e+00, 5.0095e+01, 1.7847e+01, 1.2771e+01],\n",
       "          [6.8364e+00, 1.9696e+01, 1.3926e+01, 4.9701e+01, 6.0470e+01]]],\n",
       "\n",
       "\n",
       "        [[[1.5291e+01, 1.2728e+02, 1.7820e+00, 1.5738e-01, 1.1879e+02],\n",
       "          [5.5232e+01, 4.5506e+01, 2.4393e+01, 6.7397e+01, 1.4238e+01],\n",
       "          [7.4457e+01, 1.9738e+01, 1.3668e+01, 5.5934e+01, 3.5704e+01],\n",
       "          [6.2678e+00, 1.7671e+01, 6.7305e+00, 8.4677e+01, 2.7696e+01],\n",
       "          [1.2506e+01, 2.4383e+01, 4.4501e+00, 8.1356e+00, 1.3283e+02]],\n",
       "\n",
       "         [[1.2665e+02, 6.2025e+01, 1.3520e-01, 6.3810e+01, 8.1434e+01],\n",
       "          [8.3376e+00, 8.7146e-01, 9.8957e+01, 5.2023e+00, 5.5468e+01],\n",
       "          [1.1259e+02, 9.9653e+01, 8.1012e+01, 1.5374e-01, 3.8983e+01],\n",
       "          [1.7071e+01, 1.9756e+01, 2.9414e+01, 4.2771e-01, 6.9870e+01],\n",
       "          [6.5974e+01, 2.9404e+01, 5.5031e+01, 1.2758e+02, 6.5320e+01]],\n",
       "\n",
       "         [[7.8208e+00, 7.2900e+01, 9.7185e+01, 2.0063e+01, 4.8377e+01],\n",
       "          [1.1818e-01, 8.0876e-01, 1.3575e-01, 3.8531e+00, 2.9629e+01],\n",
       "          [3.4296e+00, 1.2408e+01, 1.0789e+02, 7.0769e+01, 1.0135e+02],\n",
       "          [9.1104e+01, 7.2860e+01, 6.9434e+01, 1.8322e+01, 1.0366e+02],\n",
       "          [7.7350e+01, 1.0974e+02, 8.2477e+01, 6.6159e+01, 3.1867e+01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[3.3463e-01, 2.3931e+00, 3.4364e+01, 2.4114e+01, 8.2501e+01],\n",
       "          [6.2011e+01, 3.6106e+01, 1.5994e+01, 5.0357e+01, 1.9989e-02],\n",
       "          [8.2162e+01, 2.1278e-01, 1.5197e+01, 1.3613e+01, 4.5553e+00],\n",
       "          [7.7180e+00, 7.2245e+00, 6.5570e+01, 3.3239e+01, 9.3544e+01],\n",
       "          [1.3303e+02, 3.1139e+01, 4.1626e+01, 7.6140e+01, 8.3556e-01]],\n",
       "\n",
       "         [[2.2558e+01, 4.8231e+01, 3.3076e+01, 4.8697e+01, 7.4464e+01],\n",
       "          [3.9221e+01, 1.1603e+02, 6.9150e+01, 8.8444e+01, 7.5964e+01],\n",
       "          [7.9429e+00, 7.1363e-02, 9.8023e+01, 4.8556e+01, 4.6743e+01],\n",
       "          [6.1765e+01, 3.4139e+00, 1.0945e+02, 6.6164e+01, 1.3234e+02],\n",
       "          [5.7199e+01, 1.1513e+01, 2.9648e-01, 9.0223e+00, 1.2592e+01]],\n",
       "\n",
       "         [[4.3616e+01, 2.7933e+01, 9.3389e+00, 1.6522e+00, 1.3208e+02],\n",
       "          [1.3060e+02, 1.8235e+01, 3.7882e+01, 1.3074e+02, 3.4241e+01],\n",
       "          [5.9508e+01, 4.9647e+01, 5.6487e+01, 8.6717e+01, 3.9406e+01],\n",
       "          [1.3061e+01, 1.1864e+02, 4.1086e-01, 1.1894e+01, 1.1035e+01],\n",
       "          [6.4669e+01, 1.3316e+02, 1.5684e-02, 8.3497e+01, 6.2229e+01]]],\n",
       "\n",
       "\n",
       "        [[[5.1605e+01, 3.1193e+01, 5.5495e+01, 1.4819e+00, 5.2050e+00],\n",
       "          [1.4314e+01, 1.2782e-02, 1.3819e+01, 7.5827e+00, 4.3307e+01],\n",
       "          [6.3921e+01, 6.5151e+00, 6.7755e+01, 8.5952e-02, 3.3023e+00],\n",
       "          [1.2783e+02, 1.1074e+01, 4.5666e-03, 7.0428e+01, 2.1542e+01],\n",
       "          [3.5435e+00, 6.9129e+01, 5.0523e+00, 6.9066e+01, 4.3651e+01]],\n",
       "\n",
       "         [[2.2933e+01, 7.9680e+00, 1.3251e+02, 3.4182e+00, 1.0755e+02],\n",
       "          [1.0280e-02, 3.4466e-01, 1.8402e+01, 1.5711e+00, 8.8660e+01],\n",
       "          [1.2644e+01, 6.4218e+01, 1.5794e+01, 7.5697e+01, 5.5572e+01],\n",
       "          [2.0440e+01, 1.2334e+00, 7.7560e-04, 4.5836e+01, 4.7924e-01],\n",
       "          [3.4847e+00, 1.0188e+01, 1.4505e+01, 1.2171e+01, 8.6437e+01]],\n",
       "\n",
       "         [[3.7575e+01, 5.5997e+00, 1.0691e+02, 1.3131e+02, 1.3140e+00],\n",
       "          [2.7223e+01, 6.3401e+01, 2.2532e-02, 3.8939e+01, 7.9670e+01],\n",
       "          [7.6457e+01, 8.6438e+01, 1.0113e+02, 2.0876e+01, 3.6636e+01],\n",
       "          [3.9873e+01, 1.0202e+02, 5.2410e+00, 1.9488e+00, 1.1231e+02],\n",
       "          [1.6772e-01, 2.4044e+01, 9.9602e-01, 1.0623e+02, 1.3892e+00]]],\n",
       "\n",
       "\n",
       "        [[[4.6040e+01, 4.5362e+00, 3.7357e+01, 8.3370e+00, 6.4425e+01],\n",
       "          [8.0661e+01, 7.0575e+01, 2.4796e-02, 4.5618e-01, 1.2915e+01],\n",
       "          [9.3420e+01, 2.6797e+01, 1.4657e+01, 2.8005e+00, 3.0183e+00],\n",
       "          [5.4656e+01, 2.2320e+01, 5.8033e+01, 1.9527e+00, 6.1445e+01],\n",
       "          [1.1247e+02, 2.5616e-01, 1.1371e+02, 2.7195e+01, 2.0737e+01]],\n",
       "\n",
       "         [[3.3023e+00, 1.1755e+02, 3.4378e+00, 1.0079e+02, 1.0069e+02],\n",
       "          [3.4019e-02, 1.7525e+01, 3.6726e+00, 1.0505e+01, 4.9324e-02],\n",
       "          [4.5241e+01, 2.1614e+01, 3.2047e+01, 9.7984e+00, 6.9024e+00],\n",
       "          [6.4745e+01, 2.1799e+00, 2.1091e+01, 1.2018e+02, 1.1837e+02],\n",
       "          [9.4772e+01, 1.0663e-01, 1.1387e+02, 5.5220e-01, 1.0273e+01]],\n",
       "\n",
       "         [[2.6877e+00, 1.9067e+00, 6.7918e+01, 7.1849e+01, 4.4319e+01],\n",
       "          [4.7820e+01, 8.0617e+01, 7.8859e+00, 6.7665e-02, 9.3406e+01],\n",
       "          [1.2489e+02, 6.7167e+01, 4.1603e+01, 8.8959e-01, 2.9121e-01],\n",
       "          [4.2536e+01, 9.3309e+01, 6.5023e+01, 7.0134e-02, 3.3908e+01],\n",
       "          [3.0861e+01, 1.2808e+01, 9.3119e+00, 1.1595e+02, 1.7371e+00]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params[name]*10000*current_params[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a57123-ce77-4ccc-8e0e-1c6f776659b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d3d12-dfc3-45f6-9164-3a3988b6b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68f93971-a416-49be-b5a4-461c1872b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
       "          [ 0.0692, -0.0238,  0.0587,  0.0161, -0.0141],\n",
       "          [ 0.0320,  0.0057,  0.0422, -0.0450, -0.0084],\n",
       "          [-0.0104,  0.0167, -0.0005,  0.1009,  0.0359],\n",
       "          [-0.0430, -0.0697, -0.0194, -0.0498, -0.0370]],\n",
       "\n",
       "         [[ 0.0055,  0.0688,  0.0628, -0.1129,  0.0716],\n",
       "          [ 0.0323,  0.1095,  0.0762, -0.1052, -0.1098],\n",
       "          [-0.0557,  0.1014, -0.0192,  0.0494, -0.0537],\n",
       "          [ 0.1133, -0.0489,  0.0866,  0.0014, -0.0608],\n",
       "          [ 0.0594, -0.0613,  0.0340, -0.0333, -0.0127]],\n",
       "\n",
       "         [[-0.1110, -0.0551,  0.0627, -0.0281,  0.1150],\n",
       "          [ 0.0926, -0.0054, -0.0771,  0.0703,  0.0358],\n",
       "          [-0.0746,  0.0750,  0.0701,  0.1024, -0.0647],\n",
       "          [-0.0190, -0.0022,  0.0169, -0.0876, -0.0819],\n",
       "          [ 0.0628, -0.0271,  0.0564,  0.0066,  0.0379]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0254,  0.0420,  0.0572, -0.1069,  0.0581],\n",
       "          [-0.0812, -0.0871,  0.0070, -0.0197,  0.0678],\n",
       "          [-0.0669, -0.1027,  0.0840, -0.0171,  0.0649],\n",
       "          [ 0.0371, -0.0866,  0.0232,  0.0277, -0.0773],\n",
       "          [-0.0548,  0.0394,  0.0207, -0.0491, -0.0350]],\n",
       "\n",
       "         [[ 0.1058, -0.0214,  0.0651,  0.0500, -0.0746],\n",
       "          [-0.0982,  0.1108,  0.0060,  0.0791,  0.0239],\n",
       "          [ 0.0371,  0.0863,  0.1095, -0.0766,  0.0144],\n",
       "          [ 0.0862,  0.0837,  0.0717, -0.0836, -0.0832],\n",
       "          [-0.0698,  0.0145,  0.1151, -0.0729,  0.0615]],\n",
       "\n",
       "         [[-0.0639, -0.1086, -0.0245,  0.0665,  0.1072],\n",
       "          [-0.0717,  0.0251,  0.0996,  0.0765,  0.0720],\n",
       "          [ 0.0821,  0.0730,  0.0298, -0.0790, -0.0970],\n",
       "          [-0.0529, -0.0134, -0.0708,  0.0422,  0.0357],\n",
       "          [-0.0261,  0.0444,  0.0373,  0.0705,  0.0778]]],\n",
       "\n",
       "\n",
       "        [[[-0.0391,  0.1128, -0.0133, -0.0040, -0.1090],\n",
       "          [-0.0743, -0.0675, -0.0494,  0.0821, -0.0377],\n",
       "          [-0.0863,  0.0444,  0.0370,  0.0748, -0.0598],\n",
       "          [ 0.0250, -0.0420, -0.0259, -0.0920, -0.0526],\n",
       "          [-0.0354,  0.0494,  0.0211,  0.0285,  0.1153]],\n",
       "\n",
       "         [[ 0.1125,  0.0788,  0.0037, -0.0799,  0.0902],\n",
       "          [-0.0289, -0.0093, -0.0995, -0.0228, -0.0745],\n",
       "          [ 0.1061, -0.0998, -0.0900, -0.0039, -0.0624],\n",
       "          [ 0.0413, -0.0444, -0.0542,  0.0065,  0.0836],\n",
       "          [-0.0812,  0.0542,  0.0742,  0.1130, -0.0808]],\n",
       "\n",
       "         [[ 0.0280, -0.0854,  0.0986, -0.0448,  0.0696],\n",
       "          [ 0.0034, -0.0090, -0.0037,  0.0196,  0.0544],\n",
       "          [ 0.0185,  0.0352, -0.1039,  0.0841,  0.1007],\n",
       "          [ 0.0954,  0.0854, -0.0833, -0.0428,  0.1018],\n",
       "          [-0.0879,  0.1048, -0.0908, -0.0813,  0.0565]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0058, -0.0155,  0.0586, -0.0491,  0.0908],\n",
       "          [-0.0787, -0.0601,  0.0400,  0.0710, -0.0014],\n",
       "          [-0.0906, -0.0046, -0.0390, -0.0369, -0.0213],\n",
       "          [ 0.0278, -0.0269, -0.0810, -0.0577, -0.0967],\n",
       "          [-0.1153,  0.0558,  0.0645,  0.0873,  0.0091]],\n",
       "\n",
       "         [[-0.0475, -0.0694,  0.0575, -0.0698,  0.0863],\n",
       "          [-0.0626, -0.1077, -0.0832,  0.0940,  0.0872],\n",
       "          [ 0.0282,  0.0027,  0.0990,  0.0697,  0.0684],\n",
       "          [-0.0786,  0.0185,  0.1046, -0.0813,  0.1150],\n",
       "          [-0.0756, -0.0339, -0.0054,  0.0300,  0.0355]],\n",
       "\n",
       "         [[-0.0660,  0.0529,  0.0306, -0.0129, -0.1149],\n",
       "          [ 0.1143, -0.0427, -0.0615,  0.1143, -0.0585],\n",
       "          [ 0.0771,  0.0705, -0.0752, -0.0931, -0.0628],\n",
       "          [ 0.0361,  0.1089, -0.0064, -0.0345, -0.0332],\n",
       "          [-0.0804,  0.1154, -0.0013, -0.0914, -0.0789]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0718,  0.0559,  0.0745, -0.0122, -0.0228],\n",
       "          [ 0.0378, -0.0011, -0.0372,  0.0275, -0.0658],\n",
       "          [-0.0800, -0.0255, -0.0823,  0.0029,  0.0182],\n",
       "          [ 0.1131,  0.0333, -0.0007, -0.0839, -0.0464],\n",
       "          [-0.0188, -0.0831,  0.0225,  0.0831,  0.0661]],\n",
       "\n",
       "         [[-0.0479, -0.0282,  0.1151, -0.0185, -0.1037],\n",
       "          [-0.0010,  0.0059, -0.0429, -0.0125, -0.0942],\n",
       "          [ 0.0356, -0.0801,  0.0397,  0.0870, -0.0745],\n",
       "          [-0.0452,  0.0111,  0.0003,  0.0677, -0.0069],\n",
       "          [ 0.0187, -0.0319,  0.0381,  0.0349, -0.0930]],\n",
       "\n",
       "         [[ 0.0613, -0.0237,  0.1034, -0.1146, -0.0115],\n",
       "          [ 0.0522,  0.0796,  0.0015, -0.0624, -0.0893],\n",
       "          [-0.0874,  0.0930, -0.1006, -0.0457,  0.0605],\n",
       "          [ 0.0631, -0.1010,  0.0229,  0.0140,  0.1060],\n",
       "          [-0.0041,  0.0490, -0.0100,  0.1031, -0.0118]]],\n",
       "\n",
       "\n",
       "        [[[-0.0679,  0.0213, -0.0611,  0.0289,  0.0803],\n",
       "          [-0.0898, -0.0840,  0.0016, -0.0068, -0.0359],\n",
       "          [ 0.0967,  0.0518, -0.0383,  0.0167, -0.0174],\n",
       "          [-0.0739,  0.0472, -0.0762,  0.0140,  0.0784],\n",
       "          [-0.1061,  0.0051,  0.1066,  0.0521, -0.0455]],\n",
       "\n",
       "         [[ 0.0182, -0.1084,  0.0185,  0.1004,  0.1003],\n",
       "          [-0.0018,  0.0419, -0.0192, -0.0324,  0.0022],\n",
       "          [-0.0673,  0.0465,  0.0566, -0.0313, -0.0263],\n",
       "          [-0.0805,  0.0148,  0.0459,  0.1096, -0.1088],\n",
       "          [-0.0974,  0.0033,  0.1067,  0.0074,  0.0321]],\n",
       "\n",
       "         [[ 0.0164, -0.0138,  0.0824,  0.0848, -0.0666],\n",
       "          [-0.0692, -0.0898, -0.0281, -0.0026,  0.0966],\n",
       "          [-0.1118,  0.0820, -0.0645,  0.0094, -0.0054],\n",
       "          [ 0.0652, -0.0966, -0.0806,  0.0026,  0.0582],\n",
       "          [-0.0556,  0.0358, -0.0305, -0.1077,  0.0132]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef1de5-afdf-4b22-b102-d79a80060b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88b291-b6bd-4857-bc7e-6bfacacada72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cfc8261-b898-44a3-8205-6cafcf04bb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer0.0.weight', 'layer0.0.bias', 'layer0.1.weight', 'layer0.1.bias', 'layer1.0.weight', 'layer1.0.bias', 'layer1.1.weight', 'layer1.1.bias', 'layer2.0.weight', 'layer2.0.bias', 'layer2.1.weight', 'layer2.1.bias', 'layer3.0.weight', 'layer3.0.bias', 'layer3.1.weight', 'layer3.1.bias', 'layer4.0.weight', 'layer4.0.bias', 'layer4.1.weight', 'layer4.1.bias', 'layer5.0.weight', 'layer5.0.bias', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc411263-a138-44bb-8cf0-0c78bfdb4c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cd15e74-58ae-4b29-bb78-700ff1ddab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_weight_copy(net):\n",
    "    \"\"\"Create an all-zero copy of the network weights as a dictionary that maps name -> weight\"\"\"\n",
    "    return {\n",
    "        name: torch.zeros_like(param, requires_grad=False)\n",
    "        for name, param in net.named_parameters()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e48f7-51a0-4d47-9c1b-65afe548f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f1ece0f-7d76-4cb7-84cd-3dbe303b3b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0.0.weight': tensor([[[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0.]]]]),\n",
       " 'layer0.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer0.1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer0.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.0.weight': tensor([[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]),\n",
       " 'layer1.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.0.weight': tensor([[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]),\n",
       " 'layer2.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.0.weight': tensor([[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]),\n",
       " 'layer3.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.0.weight': tensor([[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]),\n",
       " 'layer4.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.1.weight': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer5.0.weight': tensor([[[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]]]),\n",
       " 'layer5.0.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'linear.weight': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'linear.bias': tensor([0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_create_weight_copy(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bb174dc-9912-413d-9fff-61e47e35e156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480295c-9e3e-42da-ad34-e2d15f4a8046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c5c4d0a-1fde-44b4-8090-81320bd7641c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRETRAINED_WEIGHTS_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mPRETRAINED_WEIGHTS_FILE\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRETRAINED_WEIGHTS_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fd2b616-9e67-411a-bead-2665c23f584b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRETRAINED_WEIGHTS_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mPRETRAINED_WEIGHTS_FILE\u001b[49m)\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRETRAINED_WEIGHTS_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "torch.load(PRETRAINED_WEIGHTS_FILE).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "533a248b-bead-46c1-bf95-a2e5fff8fcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer0.0.weight', 'layer0.0.bias', 'layer0.1.weight', 'layer0.1.bias', 'layer1.0.weight', 'layer1.0.bias', 'layer1.1.weight', 'layer1.1.bias', 'layer2.0.weight', 'layer2.0.bias', 'layer2.1.weight', 'layer2.1.bias', 'layer3.0.weight', 'layer3.0.bias', 'layer3.1.weight', 'layer3.1.bias', 'layer4.0.weight', 'layer4.0.bias', 'layer4.1.weight', 'layer4.1.bias', 'layer5.0.weight', 'layer5.0.bias', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c58c9db4-18be-493e-8763-3a0c45ad01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_WEIGHTS_FILE = model_dir / \"map_weights.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a8b0bd2-a981-4453-9217-ca3e8381c7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0.0.weight': tensor([[[[ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
       "           [ 0.0692, -0.0238,  0.0587,  0.0161, -0.0141],\n",
       "           [ 0.0320,  0.0057,  0.0422, -0.0450, -0.0084],\n",
       "           [-0.0104,  0.0167, -0.0005,  0.1009,  0.0359],\n",
       "           [-0.0430, -0.0697, -0.0194, -0.0498, -0.0370]],\n",
       " \n",
       "          [[ 0.0055,  0.0688,  0.0628, -0.1129,  0.0716],\n",
       "           [ 0.0323,  0.1095,  0.0762, -0.1052, -0.1098],\n",
       "           [-0.0557,  0.1014, -0.0192,  0.0494, -0.0537],\n",
       "           [ 0.1133, -0.0489,  0.0866,  0.0014, -0.0608],\n",
       "           [ 0.0594, -0.0613,  0.0340, -0.0333, -0.0127]],\n",
       " \n",
       "          [[-0.1110, -0.0551,  0.0627, -0.0281,  0.1150],\n",
       "           [ 0.0926, -0.0054, -0.0771,  0.0703,  0.0358],\n",
       "           [-0.0746,  0.0750,  0.0701,  0.1024, -0.0647],\n",
       "           [-0.0190, -0.0022,  0.0169, -0.0876, -0.0819],\n",
       "           [ 0.0628, -0.0271,  0.0564,  0.0066,  0.0379]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0254,  0.0420,  0.0572, -0.1069,  0.0581],\n",
       "           [-0.0812, -0.0871,  0.0070, -0.0197,  0.0678],\n",
       "           [-0.0669, -0.1027,  0.0840, -0.0171,  0.0649],\n",
       "           [ 0.0371, -0.0866,  0.0232,  0.0277, -0.0773],\n",
       "           [-0.0548,  0.0394,  0.0207, -0.0491, -0.0350]],\n",
       " \n",
       "          [[ 0.1058, -0.0214,  0.0651,  0.0500, -0.0746],\n",
       "           [-0.0982,  0.1108,  0.0060,  0.0791,  0.0239],\n",
       "           [ 0.0371,  0.0863,  0.1095, -0.0766,  0.0144],\n",
       "           [ 0.0862,  0.0837,  0.0717, -0.0836, -0.0832],\n",
       "           [-0.0698,  0.0145,  0.1151, -0.0729,  0.0615]],\n",
       " \n",
       "          [[-0.0639, -0.1086, -0.0245,  0.0665,  0.1072],\n",
       "           [-0.0717,  0.0251,  0.0996,  0.0765,  0.0720],\n",
       "           [ 0.0821,  0.0730,  0.0298, -0.0790, -0.0970],\n",
       "           [-0.0529, -0.0134, -0.0708,  0.0422,  0.0357],\n",
       "           [-0.0261,  0.0444,  0.0373,  0.0705,  0.0778]]],\n",
       " \n",
       " \n",
       "         [[[-0.0391,  0.1128, -0.0133, -0.0040, -0.1090],\n",
       "           [-0.0743, -0.0675, -0.0494,  0.0821, -0.0377],\n",
       "           [-0.0863,  0.0444,  0.0370,  0.0748, -0.0598],\n",
       "           [ 0.0250, -0.0420, -0.0259, -0.0920, -0.0526],\n",
       "           [-0.0354,  0.0494,  0.0211,  0.0285,  0.1153]],\n",
       " \n",
       "          [[ 0.1125,  0.0788,  0.0037, -0.0799,  0.0902],\n",
       "           [-0.0289, -0.0093, -0.0995, -0.0228, -0.0745],\n",
       "           [ 0.1061, -0.0998, -0.0900, -0.0039, -0.0624],\n",
       "           [ 0.0413, -0.0444, -0.0542,  0.0065,  0.0836],\n",
       "           [-0.0812,  0.0542,  0.0742,  0.1130, -0.0808]],\n",
       " \n",
       "          [[ 0.0280, -0.0854,  0.0986, -0.0448,  0.0696],\n",
       "           [ 0.0034, -0.0090, -0.0037,  0.0196,  0.0544],\n",
       "           [ 0.0185,  0.0352, -0.1039,  0.0841,  0.1007],\n",
       "           [ 0.0954,  0.0854, -0.0833, -0.0428,  0.1018],\n",
       "           [-0.0879,  0.1048, -0.0908, -0.0813,  0.0565]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0058, -0.0155,  0.0586, -0.0491,  0.0908],\n",
       "           [-0.0787, -0.0601,  0.0400,  0.0710, -0.0014],\n",
       "           [-0.0906, -0.0046, -0.0390, -0.0369, -0.0213],\n",
       "           [ 0.0278, -0.0269, -0.0810, -0.0577, -0.0967],\n",
       "           [-0.1153,  0.0558,  0.0645,  0.0873,  0.0091]],\n",
       " \n",
       "          [[-0.0475, -0.0694,  0.0575, -0.0698,  0.0863],\n",
       "           [-0.0626, -0.1077, -0.0832,  0.0940,  0.0872],\n",
       "           [ 0.0282,  0.0027,  0.0990,  0.0697,  0.0684],\n",
       "           [-0.0786,  0.0185,  0.1046, -0.0813,  0.1150],\n",
       "           [-0.0756, -0.0339, -0.0054,  0.0300,  0.0355]],\n",
       " \n",
       "          [[-0.0660,  0.0529,  0.0306, -0.0129, -0.1149],\n",
       "           [ 0.1143, -0.0427, -0.0615,  0.1143, -0.0585],\n",
       "           [ 0.0771,  0.0705, -0.0752, -0.0931, -0.0628],\n",
       "           [ 0.0361,  0.1089, -0.0064, -0.0345, -0.0332],\n",
       "           [-0.0804,  0.1154, -0.0013, -0.0914, -0.0789]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0718,  0.0559,  0.0745, -0.0122, -0.0228],\n",
       "           [ 0.0378, -0.0011, -0.0372,  0.0275, -0.0658],\n",
       "           [-0.0800, -0.0255, -0.0823,  0.0029,  0.0182],\n",
       "           [ 0.1131,  0.0333, -0.0007, -0.0839, -0.0464],\n",
       "           [-0.0188, -0.0831,  0.0225,  0.0831,  0.0661]],\n",
       " \n",
       "          [[-0.0479, -0.0282,  0.1151, -0.0185, -0.1037],\n",
       "           [-0.0010,  0.0059, -0.0429, -0.0125, -0.0942],\n",
       "           [ 0.0356, -0.0801,  0.0397,  0.0870, -0.0745],\n",
       "           [-0.0452,  0.0111,  0.0003,  0.0677, -0.0069],\n",
       "           [ 0.0187, -0.0319,  0.0381,  0.0349, -0.0930]],\n",
       " \n",
       "          [[ 0.0613, -0.0237,  0.1034, -0.1146, -0.0115],\n",
       "           [ 0.0522,  0.0796,  0.0015, -0.0624, -0.0893],\n",
       "           [-0.0874,  0.0930, -0.1006, -0.0457,  0.0605],\n",
       "           [ 0.0631, -0.1010,  0.0229,  0.0140,  0.1060],\n",
       "           [-0.0041,  0.0490, -0.0100,  0.1031, -0.0118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0679,  0.0213, -0.0611,  0.0289,  0.0803],\n",
       "           [-0.0898, -0.0840,  0.0016, -0.0068, -0.0359],\n",
       "           [ 0.0967,  0.0518, -0.0383,  0.0167, -0.0174],\n",
       "           [-0.0739,  0.0472, -0.0762,  0.0140,  0.0784],\n",
       "           [-0.1061,  0.0051,  0.1066,  0.0521, -0.0455]],\n",
       " \n",
       "          [[ 0.0182, -0.1084,  0.0185,  0.1004,  0.1003],\n",
       "           [-0.0018,  0.0419, -0.0192, -0.0324,  0.0022],\n",
       "           [-0.0673,  0.0465,  0.0566, -0.0313, -0.0263],\n",
       "           [-0.0805,  0.0148,  0.0459,  0.1096, -0.1088],\n",
       "           [-0.0974,  0.0033,  0.1067,  0.0074,  0.0321]],\n",
       " \n",
       "          [[ 0.0164, -0.0138,  0.0824,  0.0848, -0.0666],\n",
       "           [-0.0692, -0.0898, -0.0281, -0.0026,  0.0966],\n",
       "           [-0.1118,  0.0820, -0.0645,  0.0094, -0.0054],\n",
       "           [ 0.0652, -0.0966, -0.0806,  0.0026,  0.0582],\n",
       "           [-0.0556,  0.0358, -0.0305, -0.1077,  0.0132]]]]),\n",
       " 'layer0.0.bias': tensor([-0.0246,  0.0683,  0.0761,  0.0755,  0.0723, -0.0893, -0.1108,  0.0129,\n",
       "          0.0335, -0.0888,  0.0944, -0.0122, -0.0677,  0.0792,  0.1011,  0.0177,\n",
       "         -0.0368,  0.0125, -0.0851,  0.0712,  0.0725, -0.0216, -0.0915,  0.0857,\n",
       "         -0.0945, -0.0606,  0.0689, -0.0574,  0.0220,  0.0949, -0.0146, -0.0261]),\n",
       " 'layer0.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer0.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.0.weight': tensor([[[[ 0.0546,  0.0101,  0.0206],\n",
       "           [-0.0194, -0.0508,  0.0092],\n",
       "           [-0.0348, -0.0305, -0.0483]],\n",
       " \n",
       "          [[ 0.0505, -0.0396,  0.0170],\n",
       "           [ 0.0121,  0.0407,  0.0322],\n",
       "           [-0.0413,  0.0261, -0.0299]],\n",
       " \n",
       "          [[ 0.0022, -0.0213, -0.0504],\n",
       "           [ 0.0176, -0.0393,  0.0577],\n",
       "           [ 0.0237, -0.0204, -0.0345]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0583,  0.0324,  0.0416],\n",
       "           [ 0.0469,  0.0359, -0.0188],\n",
       "           [ 0.0121,  0.0149, -0.0563]],\n",
       " \n",
       "          [[ 0.0318, -0.0475,  0.0212],\n",
       "           [ 0.0569, -0.0164,  0.0081],\n",
       "           [ 0.0546,  0.0486,  0.0492]],\n",
       " \n",
       "          [[-0.0408,  0.0116, -0.0493],\n",
       "           [ 0.0108, -0.0558,  0.0068],\n",
       "           [ 0.0298, -0.0525, -0.0090]]],\n",
       " \n",
       " \n",
       "         [[[-0.0244,  0.0484, -0.0058],\n",
       "           [ 0.0367, -0.0435,  0.0082],\n",
       "           [ 0.0427,  0.0033, -0.0206]],\n",
       " \n",
       "          [[-0.0241,  0.0402,  0.0232],\n",
       "           [ 0.0040,  0.0348, -0.0307],\n",
       "           [-0.0447, -0.0133,  0.0432]],\n",
       " \n",
       "          [[-0.0570, -0.0293,  0.0143],\n",
       "           [ 0.0222,  0.0295,  0.0178],\n",
       "           [-0.0520, -0.0192,  0.0362]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0269, -0.0078, -0.0461],\n",
       "           [-0.0380, -0.0191, -0.0069],\n",
       "           [-0.0127,  0.0033, -0.0540]],\n",
       " \n",
       "          [[-0.0095, -0.0517,  0.0310],\n",
       "           [ 0.0537, -0.0465, -0.0424],\n",
       "           [ 0.0571,  0.0458,  0.0271]],\n",
       " \n",
       "          [[-0.0372,  0.0185, -0.0019],\n",
       "           [ 0.0444,  0.0367, -0.0067],\n",
       "           [ 0.0230, -0.0515,  0.0101]]],\n",
       " \n",
       " \n",
       "         [[[-0.0359,  0.0468, -0.0428],\n",
       "           [-0.0512, -0.0502, -0.0168],\n",
       "           [ 0.0032, -0.0068,  0.0242]],\n",
       " \n",
       "          [[-0.0523, -0.0346, -0.0082],\n",
       "           [-0.0019, -0.0128,  0.0270],\n",
       "           [ 0.0305, -0.0106, -0.0029]],\n",
       " \n",
       "          [[-0.0092,  0.0058, -0.0247],\n",
       "           [-0.0092, -0.0432,  0.0493],\n",
       "           [ 0.0403, -0.0400,  0.0044]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0401, -0.0285, -0.0259],\n",
       "           [-0.0050, -0.0551,  0.0106],\n",
       "           [ 0.0445,  0.0369, -0.0460]],\n",
       " \n",
       "          [[-0.0178,  0.0308,  0.0211],\n",
       "           [ 0.0263,  0.0247, -0.0142],\n",
       "           [ 0.0390,  0.0552,  0.0218]],\n",
       " \n",
       "          [[-0.0097,  0.0256,  0.0409],\n",
       "           [-0.0466,  0.0163, -0.0281],\n",
       "           [ 0.0030, -0.0391, -0.0109]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0194, -0.0357,  0.0117],\n",
       "           [ 0.0072, -0.0325, -0.0014],\n",
       "           [ 0.0456, -0.0435,  0.0098]],\n",
       " \n",
       "          [[ 0.0320,  0.0543, -0.0537],\n",
       "           [ 0.0267,  0.0560,  0.0403],\n",
       "           [-0.0508, -0.0085, -0.0072]],\n",
       " \n",
       "          [[-0.0085,  0.0042,  0.0038],\n",
       "           [ 0.0312, -0.0462, -0.0416],\n",
       "           [-0.0018, -0.0555,  0.0578]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0169,  0.0042,  0.0447],\n",
       "           [-0.0125,  0.0027,  0.0512],\n",
       "           [-0.0242, -0.0469, -0.0531]],\n",
       " \n",
       "          [[-0.0371, -0.0043, -0.0548],\n",
       "           [-0.0245,  0.0057,  0.0537],\n",
       "           [-0.0011,  0.0426,  0.0344]],\n",
       " \n",
       "          [[-0.0172,  0.0337, -0.0225],\n",
       "           [ 0.0088, -0.0114,  0.0121],\n",
       "           [-0.0192,  0.0237,  0.0269]]],\n",
       " \n",
       " \n",
       "         [[[-0.0037, -0.0553, -0.0541],\n",
       "           [-0.0205, -0.0239,  0.0142],\n",
       "           [ 0.0084, -0.0058,  0.0161]],\n",
       " \n",
       "          [[ 0.0074,  0.0498,  0.0194],\n",
       "           [-0.0311, -0.0490, -0.0356],\n",
       "           [-0.0336,  0.0319, -0.0069]],\n",
       " \n",
       "          [[-0.0362, -0.0574, -0.0031],\n",
       "           [-0.0213, -0.0345,  0.0364],\n",
       "           [ 0.0138, -0.0144,  0.0195]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0320, -0.0554, -0.0148],\n",
       "           [-0.0214,  0.0331,  0.0398],\n",
       "           [ 0.0071,  0.0106, -0.0159]],\n",
       " \n",
       "          [[-0.0255,  0.0531,  0.0584],\n",
       "           [ 0.0261,  0.0347, -0.0573],\n",
       "           [ 0.0132,  0.0088, -0.0572]],\n",
       " \n",
       "          [[ 0.0217, -0.0435,  0.0497],\n",
       "           [ 0.0121,  0.0476, -0.0020],\n",
       "           [-0.0400,  0.0024, -0.0448]]],\n",
       " \n",
       " \n",
       "         [[[-0.0536, -0.0045,  0.0082],\n",
       "           [ 0.0016,  0.0405, -0.0227],\n",
       "           [-0.0341,  0.0054,  0.0062]],\n",
       " \n",
       "          [[-0.0014, -0.0461, -0.0520],\n",
       "           [-0.0377,  0.0143,  0.0103],\n",
       "           [ 0.0545, -0.0154, -0.0308]],\n",
       " \n",
       "          [[-0.0100,  0.0565, -0.0555],\n",
       "           [ 0.0302, -0.0189,  0.0516],\n",
       "           [ 0.0415,  0.0222,  0.0087]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0453, -0.0079,  0.0073],\n",
       "           [-0.0204,  0.0051, -0.0157],\n",
       "           [ 0.0565,  0.0389, -0.0149]],\n",
       " \n",
       "          [[ 0.0438, -0.0230,  0.0282],\n",
       "           [ 0.0552, -0.0253,  0.0328],\n",
       "           [-0.0390, -0.0553,  0.0518]],\n",
       " \n",
       "          [[ 0.0100, -0.0365,  0.0531],\n",
       "           [ 0.0285,  0.0130, -0.0063],\n",
       "           [ 0.0179, -0.0586, -0.0554]]]]),\n",
       " 'layer1.0.bias': tensor([-0.0543, -0.0043, -0.0017,  0.0498, -0.0517,  0.0165,  0.0087,  0.0565,\n",
       "          0.0444, -0.0543, -0.0388,  0.0033,  0.0551,  0.0235,  0.0446, -0.0100,\n",
       "         -0.0346,  0.0379,  0.0387,  0.0266, -0.0253,  0.0476,  0.0511, -0.0268,\n",
       "         -0.0131,  0.0463,  0.0268,  0.0487,  0.0120, -0.0506,  0.0130, -0.0039]),\n",
       " 'layer1.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer1.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.0.weight': tensor([[[[ 5.7134e-02,  2.2763e-03,  7.6729e-03],\n",
       "           [ 2.2283e-02, -5.0950e-02, -1.4664e-02],\n",
       "           [-4.8651e-02, -4.9914e-02,  2.2173e-02]],\n",
       " \n",
       "          [[-2.2566e-02, -3.4802e-02, -5.1511e-02],\n",
       "           [ 5.8050e-02, -4.7091e-02, -4.5683e-02],\n",
       "           [-4.2887e-02, -2.2843e-02,  9.9157e-03]],\n",
       " \n",
       "          [[-1.1733e-02, -4.2466e-02,  3.9983e-02],\n",
       "           [-3.8778e-02, -3.6147e-02,  6.0745e-03],\n",
       "           [-1.9005e-02, -3.8071e-02, -4.7358e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8251e-02, -2.0059e-02, -1.7664e-03],\n",
       "           [ 3.6452e-02, -5.8274e-02, -2.5591e-02],\n",
       "           [ 3.0273e-02,  2.4939e-02, -1.4607e-02]],\n",
       " \n",
       "          [[-5.6558e-02,  1.9704e-02, -1.7753e-02],\n",
       "           [ 2.6979e-02,  5.5089e-02,  1.3478e-02],\n",
       "           [ 2.6325e-02,  5.3409e-02, -1.9308e-02]],\n",
       " \n",
       "          [[ 1.3918e-02, -4.2279e-03,  3.3063e-02],\n",
       "           [ 1.9445e-02, -3.4774e-03,  1.6238e-02],\n",
       "           [ 6.9609e-04, -3.7400e-02,  2.5798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.9733e-02,  5.1662e-02, -3.5295e-02],\n",
       "           [-4.7838e-03, -2.9584e-02,  1.9393e-02],\n",
       "           [-4.9178e-02,  1.5128e-02,  3.1844e-02]],\n",
       " \n",
       "          [[-4.8651e-02, -2.3051e-02,  4.6810e-02],\n",
       "           [ 2.2733e-02,  3.7091e-02,  7.8823e-03],\n",
       "           [ 4.6600e-05, -3.1117e-02, -2.0206e-02]],\n",
       " \n",
       "          [[-5.0931e-02,  5.2320e-02,  1.5128e-02],\n",
       "           [-1.2424e-02, -3.7507e-02,  2.9012e-04],\n",
       "           [ 1.3300e-02,  4.9583e-03,  2.4148e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8632e-02, -4.4404e-02,  1.1814e-02],\n",
       "           [-1.0807e-02,  4.8829e-02,  4.4790e-02],\n",
       "           [-2.0285e-02,  3.6822e-02,  2.8248e-02]],\n",
       " \n",
       "          [[ 1.7641e-02,  1.0618e-02, -3.1017e-02],\n",
       "           [-3.5140e-02, -1.0367e-02, -4.9357e-03],\n",
       "           [-2.6300e-03,  1.1192e-02, -4.5672e-02]],\n",
       " \n",
       "          [[-3.9225e-03, -3.7740e-02, -5.4893e-02],\n",
       "           [ 2.5574e-02,  5.4951e-02, -2.9725e-02],\n",
       "           [ 1.5240e-02, -3.6052e-02,  3.4700e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 5.5491e-02, -5.2750e-03, -1.8764e-02],\n",
       "           [-1.7387e-02, -5.2585e-02,  7.1799e-03],\n",
       "           [-2.7637e-02, -4.1692e-02,  3.6744e-02]],\n",
       " \n",
       "          [[ 2.0283e-02, -4.8516e-02, -2.9705e-02],\n",
       "           [ 3.5179e-02,  2.7032e-02,  5.6624e-02],\n",
       "           [-3.8420e-03,  1.8063e-02, -2.4304e-02]],\n",
       " \n",
       "          [[-3.5589e-02, -2.8323e-02, -3.3601e-03],\n",
       "           [-6.5405e-04, -5.8679e-02,  5.0192e-02],\n",
       "           [-3.0432e-02, -3.8527e-02, -4.3469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2697e-02, -1.4331e-02,  5.0963e-02],\n",
       "           [ 5.5813e-02,  1.8976e-02,  1.3342e-02],\n",
       "           [ 2.3138e-02,  3.1858e-02,  3.6571e-02]],\n",
       " \n",
       "          [[ 3.4722e-02, -2.1318e-02,  5.0169e-02],\n",
       "           [ 1.3877e-02,  1.4181e-02,  2.9957e-02],\n",
       "           [ 4.5517e-02,  2.8146e-02,  4.4315e-02]],\n",
       " \n",
       "          [[-2.2685e-02,  3.5278e-02, -7.6343e-03],\n",
       "           [ 3.1103e-02, -1.9444e-02, -2.7423e-02],\n",
       "           [ 5.1713e-02, -2.1571e-02, -4.5952e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-5.5247e-02,  3.1720e-02,  2.1539e-02],\n",
       "           [-3.8513e-02, -4.7985e-02, -3.3430e-02],\n",
       "           [-3.0489e-02, -1.7572e-02,  4.2375e-02]],\n",
       " \n",
       "          [[ 4.4120e-02,  5.4889e-02,  2.4734e-02],\n",
       "           [ 3.6550e-03, -4.3214e-02, -2.4624e-03],\n",
       "           [-4.2030e-02,  4.0658e-02, -5.2674e-02]],\n",
       " \n",
       "          [[-7.3320e-03, -7.3222e-03,  3.7609e-02],\n",
       "           [-3.0710e-02, -4.8105e-02, -1.4401e-02],\n",
       "           [-3.6586e-03,  1.9842e-02,  1.5428e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.9497e-02, -3.2710e-02, -5.1007e-02],\n",
       "           [-1.2096e-02,  4.4725e-02,  4.9262e-02],\n",
       "           [-4.5488e-02,  5.7023e-02, -1.3938e-02]],\n",
       " \n",
       "          [[ 2.6453e-02, -5.3338e-02,  4.9874e-03],\n",
       "           [ 4.9443e-02,  4.6337e-02,  4.6237e-02],\n",
       "           [ 1.9181e-02, -2.0657e-02, -4.5685e-02]],\n",
       " \n",
       "          [[-2.5192e-03,  2.8761e-02, -4.2599e-02],\n",
       "           [-4.8064e-03,  2.7911e-02, -3.7541e-02],\n",
       "           [-5.1003e-02,  1.0393e-02,  5.6107e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0096e-02,  8.2636e-03,  3.3643e-02],\n",
       "           [-3.5483e-02,  3.9823e-02, -5.1088e-02],\n",
       "           [-5.2329e-02, -7.8036e-03, -2.6994e-02]],\n",
       " \n",
       "          [[ 5.0138e-02, -1.2705e-03,  5.5152e-02],\n",
       "           [ 4.8575e-02, -2.8620e-02,  4.0724e-02],\n",
       "           [ 4.4372e-02, -5.5048e-02,  8.2719e-03]],\n",
       " \n",
       "          [[-3.6917e-03, -4.8237e-02,  1.4326e-02],\n",
       "           [ 5.8474e-02,  9.2294e-04,  5.3283e-02],\n",
       "           [ 3.6018e-02,  1.5803e-02, -4.4104e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.2120e-02, -2.1614e-02,  1.7018e-02],\n",
       "           [-4.2496e-02, -5.5788e-02,  2.9173e-02],\n",
       "           [-4.1194e-02,  3.2194e-03,  2.2686e-02]],\n",
       " \n",
       "          [[ 5.4017e-02,  1.5278e-02, -3.9786e-02],\n",
       "           [ 3.6710e-03, -4.5804e-02, -3.8354e-02],\n",
       "           [-2.4702e-02,  3.4668e-02, -4.7339e-02]],\n",
       " \n",
       "          [[ 5.3430e-02,  1.8570e-02, -3.7024e-02],\n",
       "           [-5.7286e-02,  2.4916e-02, -2.3010e-02],\n",
       "           [ 3.4885e-02, -2.9613e-02,  1.6202e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.0582e-02, -2.6968e-02, -2.8093e-02],\n",
       "           [-8.5723e-04, -1.6351e-02, -1.8424e-02],\n",
       "           [ 3.6358e-02, -2.9207e-02,  4.2901e-02]],\n",
       " \n",
       "          [[ 5.2379e-02,  2.7298e-02, -3.8665e-02],\n",
       "           [ 4.7064e-02, -1.9328e-02,  5.7603e-02],\n",
       "           [ 3.4986e-02,  3.6665e-02, -9.3917e-03]],\n",
       " \n",
       "          [[ 3.7847e-02,  5.5604e-02,  1.2676e-02],\n",
       "           [ 5.7904e-02,  4.6458e-02, -1.0816e-02],\n",
       "           [ 1.4825e-02,  3.0120e-02, -2.4013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.5582e-02,  7.9506e-03, -5.3923e-02],\n",
       "           [ 3.4684e-02,  3.2537e-03, -8.8748e-03],\n",
       "           [ 5.1414e-02,  2.1381e-02, -4.7578e-02]],\n",
       " \n",
       "          [[-1.9648e-02, -5.0517e-02, -1.2699e-02],\n",
       "           [ 2.0551e-02,  3.4886e-02, -1.7081e-02],\n",
       "           [ 4.5170e-02,  4.6150e-02,  2.6860e-02]],\n",
       " \n",
       "          [[-1.7783e-02,  3.7673e-02,  3.8889e-02],\n",
       "           [-4.2494e-02, -3.3087e-02,  4.2578e-02],\n",
       "           [-1.9450e-02, -4.6741e-02, -2.9636e-02]]]]),\n",
       " 'layer2.0.bias': tensor([-0.0029, -0.0226, -0.0220, -0.0451, -0.0162,  0.0195,  0.0535, -0.0280,\n",
       "         -0.0340,  0.0170,  0.0581,  0.0215, -0.0449,  0.0488,  0.0284,  0.0110,\n",
       "         -0.0304, -0.0164, -0.0357, -0.0168,  0.0295,  0.0206, -0.0200,  0.0095,\n",
       "         -0.0089,  0.0315,  0.0465,  0.0486, -0.0454,  0.0323,  0.0181, -0.0093]),\n",
       " 'layer2.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer2.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.0.weight': tensor([[[[ 0.0349,  0.0080,  0.0391],\n",
       "           [ 0.0168,  0.0509,  0.0433],\n",
       "           [ 0.0178,  0.0479, -0.0166]],\n",
       " \n",
       "          [[-0.0273, -0.0540, -0.0191],\n",
       "           [-0.0348, -0.0212, -0.0429],\n",
       "           [-0.0271, -0.0277,  0.0539]],\n",
       " \n",
       "          [[ 0.0317, -0.0411, -0.0299],\n",
       "           [ 0.0163, -0.0236,  0.0418],\n",
       "           [ 0.0094, -0.0482, -0.0238]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0558,  0.0193, -0.0203],\n",
       "           [-0.0455, -0.0138,  0.0564],\n",
       "           [ 0.0082, -0.0449, -0.0450]],\n",
       " \n",
       "          [[ 0.0185, -0.0095, -0.0317],\n",
       "           [ 0.0377,  0.0436, -0.0243],\n",
       "           [ 0.0119, -0.0278, -0.0025]],\n",
       " \n",
       "          [[-0.0028,  0.0118,  0.0340],\n",
       "           [-0.0475, -0.0382,  0.0570],\n",
       "           [ 0.0181,  0.0342, -0.0580]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0275, -0.0282,  0.0214],\n",
       "           [ 0.0168, -0.0525,  0.0339],\n",
       "           [ 0.0271, -0.0350,  0.0188]],\n",
       " \n",
       "          [[-0.0379,  0.0073,  0.0297],\n",
       "           [-0.0505,  0.0464, -0.0274],\n",
       "           [-0.0421,  0.0336,  0.0277]],\n",
       " \n",
       "          [[-0.0463,  0.0101, -0.0216],\n",
       "           [ 0.0380, -0.0448,  0.0118],\n",
       "           [-0.0039,  0.0029,  0.0205]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0137, -0.0485,  0.0479],\n",
       "           [ 0.0140,  0.0290,  0.0492],\n",
       "           [-0.0348,  0.0098, -0.0530]],\n",
       " \n",
       "          [[ 0.0022,  0.0261,  0.0222],\n",
       "           [-0.0144, -0.0012,  0.0539],\n",
       "           [-0.0323, -0.0358,  0.0455]],\n",
       " \n",
       "          [[-0.0019, -0.0225,  0.0147],\n",
       "           [-0.0320,  0.0233,  0.0163],\n",
       "           [ 0.0575,  0.0426, -0.0043]]],\n",
       " \n",
       " \n",
       "         [[[-0.0363, -0.0312,  0.0324],\n",
       "           [-0.0358,  0.0405, -0.0376],\n",
       "           [-0.0349,  0.0389, -0.0300]],\n",
       " \n",
       "          [[ 0.0290,  0.0511,  0.0255],\n",
       "           [ 0.0415,  0.0384, -0.0417],\n",
       "           [ 0.0170,  0.0173,  0.0497]],\n",
       " \n",
       "          [[-0.0512,  0.0508,  0.0123],\n",
       "           [ 0.0372, -0.0387, -0.0135],\n",
       "           [ 0.0373,  0.0515,  0.0059]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0423, -0.0471, -0.0543],\n",
       "           [-0.0494,  0.0340,  0.0193],\n",
       "           [ 0.0132, -0.0222, -0.0134]],\n",
       " \n",
       "          [[-0.0555,  0.0462, -0.0468],\n",
       "           [-0.0407,  0.0341, -0.0243],\n",
       "           [ 0.0366, -0.0470, -0.0287]],\n",
       " \n",
       "          [[ 0.0117,  0.0141, -0.0320],\n",
       "           [ 0.0347,  0.0127, -0.0439],\n",
       "           [-0.0045,  0.0420,  0.0280]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0276, -0.0193, -0.0327],\n",
       "           [ 0.0056, -0.0327, -0.0540],\n",
       "           [ 0.0408, -0.0589,  0.0439]],\n",
       " \n",
       "          [[ 0.0170,  0.0479,  0.0354],\n",
       "           [-0.0253,  0.0351,  0.0472],\n",
       "           [-0.0431,  0.0366,  0.0009]],\n",
       " \n",
       "          [[-0.0452,  0.0521, -0.0156],\n",
       "           [ 0.0237,  0.0153,  0.0115],\n",
       "           [ 0.0114,  0.0531, -0.0215]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0174, -0.0583, -0.0320],\n",
       "           [ 0.0349,  0.0220,  0.0481],\n",
       "           [-0.0178,  0.0080,  0.0153]],\n",
       " \n",
       "          [[ 0.0321,  0.0128,  0.0074],\n",
       "           [ 0.0360,  0.0581, -0.0427],\n",
       "           [ 0.0326,  0.0437, -0.0364]],\n",
       " \n",
       "          [[ 0.0378, -0.0583, -0.0260],\n",
       "           [-0.0287,  0.0527, -0.0389],\n",
       "           [ 0.0347, -0.0302, -0.0554]]],\n",
       " \n",
       " \n",
       "         [[[-0.0092, -0.0467, -0.0299],\n",
       "           [ 0.0316, -0.0589, -0.0055],\n",
       "           [ 0.0511, -0.0071, -0.0279]],\n",
       " \n",
       "          [[-0.0223,  0.0226, -0.0080],\n",
       "           [ 0.0587, -0.0140, -0.0529],\n",
       "           [-0.0343,  0.0180,  0.0249]],\n",
       " \n",
       "          [[-0.0064,  0.0322,  0.0150],\n",
       "           [-0.0121,  0.0542, -0.0135],\n",
       "           [-0.0501, -0.0066,  0.0361]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0131, -0.0132,  0.0006],\n",
       "           [-0.0170,  0.0108,  0.0513],\n",
       "           [-0.0477, -0.0503, -0.0247]],\n",
       " \n",
       "          [[ 0.0012,  0.0428,  0.0357],\n",
       "           [ 0.0526,  0.0207, -0.0433],\n",
       "           [ 0.0550,  0.0501,  0.0588]],\n",
       " \n",
       "          [[ 0.0333, -0.0109,  0.0188],\n",
       "           [-0.0043, -0.0551,  0.0034],\n",
       "           [ 0.0480,  0.0101, -0.0003]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0053,  0.0389,  0.0388],\n",
       "           [ 0.0026, -0.0537, -0.0568],\n",
       "           [-0.0342,  0.0285, -0.0239]],\n",
       " \n",
       "          [[ 0.0492,  0.0006,  0.0106],\n",
       "           [-0.0431, -0.0318,  0.0212],\n",
       "           [ 0.0052, -0.0454, -0.0238]],\n",
       " \n",
       "          [[ 0.0561,  0.0425, -0.0006],\n",
       "           [-0.0442,  0.0060, -0.0096],\n",
       "           [-0.0088, -0.0156,  0.0323]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0413, -0.0057, -0.0351],\n",
       "           [ 0.0050,  0.0508, -0.0194],\n",
       "           [-0.0276,  0.0149, -0.0189]],\n",
       " \n",
       "          [[-0.0123, -0.0535,  0.0464],\n",
       "           [-0.0081,  0.0378,  0.0531],\n",
       "           [-0.0053, -0.0570, -0.0249]],\n",
       " \n",
       "          [[ 0.0173, -0.0353,  0.0536],\n",
       "           [ 0.0449, -0.0072, -0.0557],\n",
       "           [-0.0291, -0.0258, -0.0001]]]]),\n",
       " 'layer3.0.bias': tensor([-0.0553,  0.0040,  0.0108, -0.0160,  0.0396,  0.0470,  0.0212,  0.0019,\n",
       "          0.0513, -0.0497,  0.0139, -0.0532,  0.0565, -0.0235,  0.0157,  0.0309,\n",
       "          0.0052,  0.0123, -0.0224, -0.0405,  0.0552,  0.0039,  0.0070, -0.0347,\n",
       "          0.0450,  0.0507, -0.0266,  0.0296,  0.0268,  0.0458,  0.0275, -0.0472,\n",
       "          0.0478, -0.0509,  0.0154, -0.0372,  0.0560, -0.0417, -0.0460,  0.0035,\n",
       "         -0.0077,  0.0466, -0.0062, -0.0025, -0.0086,  0.0558,  0.0206,  0.0366,\n",
       "          0.0067,  0.0200, -0.0214, -0.0477, -0.0559, -0.0190,  0.0371,  0.0582,\n",
       "          0.0312, -0.0585, -0.0531, -0.0584,  0.0278, -0.0551,  0.0183,  0.0443]),\n",
       " 'layer3.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer3.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.0.weight': tensor([[[[ 2.5620e-02,  1.1027e-02,  3.8344e-02],\n",
       "           [-5.6369e-04, -1.3636e-02, -1.1476e-02],\n",
       "           [ 2.4599e-02, -1.7833e-02,  1.0314e-02]],\n",
       " \n",
       "          [[-3.6600e-02,  1.6459e-02,  8.5596e-03],\n",
       "           [-2.3940e-02,  3.1190e-02,  1.1611e-03],\n",
       "           [-2.8263e-02, -2.6966e-02, -1.9842e-02]],\n",
       " \n",
       "          [[-2.2325e-02,  1.7213e-02,  1.4972e-02],\n",
       "           [ 1.2437e-03, -1.6358e-02,  2.6994e-02],\n",
       "           [ 6.8585e-03,  4.8376e-04,  2.7526e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.0617e-02, -2.6364e-02,  3.6131e-02],\n",
       "           [ 2.8148e-02,  1.4054e-02, -2.4607e-02],\n",
       "           [-6.4064e-03,  3.6277e-02,  6.3002e-03]],\n",
       " \n",
       "          [[-2.6183e-02, -1.6960e-02, -3.2769e-02],\n",
       "           [ 3.3250e-02, -1.0777e-02,  3.2396e-02],\n",
       "           [-8.2837e-03,  3.9580e-02,  2.3819e-02]],\n",
       " \n",
       "          [[ 1.0965e-02, -3.6820e-02, -1.8887e-02],\n",
       "           [ 5.6978e-03, -3.5669e-02, -1.5734e-02],\n",
       "           [-1.7920e-02,  1.7379e-02, -3.0323e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.7890e-02,  1.4734e-02,  1.2198e-02],\n",
       "           [-2.2439e-02,  4.1251e-02, -2.5036e-02],\n",
       "           [ 5.7460e-03, -1.1964e-02, -2.5474e-02]],\n",
       " \n",
       "          [[-2.0505e-02,  1.5931e-02,  1.7208e-02],\n",
       "           [-2.7738e-02, -3.5588e-02, -8.7796e-03],\n",
       "           [ 2.6947e-03,  2.5921e-03, -1.3987e-02]],\n",
       " \n",
       "          [[-7.3796e-03, -1.7497e-02,  3.8538e-03],\n",
       "           [ 2.2572e-02, -1.0772e-02,  5.4893e-03],\n",
       "           [ 4.1398e-02, -2.8769e-02, -1.4876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5627e-02, -2.2806e-03,  1.0660e-02],\n",
       "           [ 3.1863e-03, -3.0281e-02,  3.9491e-02],\n",
       "           [-2.6706e-02, -4.1496e-02, -3.4536e-02]],\n",
       " \n",
       "          [[ 1.2076e-02, -2.2478e-03,  2.5527e-02],\n",
       "           [-5.1937e-03,  7.0800e-04,  2.4908e-02],\n",
       "           [ 3.5183e-02,  2.8290e-03, -1.1281e-02]],\n",
       " \n",
       "          [[ 3.3812e-02,  2.9348e-02,  2.4271e-02],\n",
       "           [ 4.3343e-03,  1.9623e-02,  2.9991e-02],\n",
       "           [ 3.7585e-02,  6.1731e-03, -1.6640e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5880e-02, -6.0504e-03,  2.0034e-03],\n",
       "           [-3.7352e-02, -7.3350e-03,  2.5647e-02],\n",
       "           [ 2.4312e-02, -8.3213e-03,  3.5337e-03]],\n",
       " \n",
       "          [[-3.0441e-02,  6.6977e-03, -1.9300e-02],\n",
       "           [ 3.0446e-02, -7.7699e-03, -5.5637e-03],\n",
       "           [ 1.3507e-02,  2.9579e-02,  3.1732e-02]],\n",
       " \n",
       "          [[-1.2611e-02,  2.5706e-02,  6.1131e-03],\n",
       "           [ 6.8956e-03, -1.2875e-02,  1.7288e-02],\n",
       "           [ 6.3988e-03,  1.7031e-02, -1.8602e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.2298e-03,  3.1480e-02, -2.4839e-02],\n",
       "           [-9.5341e-05, -3.4622e-02,  3.4004e-02],\n",
       "           [-1.9048e-02,  1.2274e-02, -3.6575e-02]],\n",
       " \n",
       "          [[ 2.3614e-02, -1.8650e-02, -2.2995e-02],\n",
       "           [ 1.8063e-02,  3.1919e-02, -3.8414e-02],\n",
       "           [ 3.2621e-02, -2.6842e-02,  2.1469e-02]],\n",
       " \n",
       "          [[ 2.2439e-02,  2.9607e-02, -3.4393e-03],\n",
       "           [ 1.2774e-02, -1.5567e-02,  2.1473e-02],\n",
       "           [-3.5595e-02, -1.9984e-02,  1.7679e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-4.8253e-03, -3.3776e-02,  4.0900e-02],\n",
       "           [ 2.1382e-02,  2.7088e-02,  4.6202e-04],\n",
       "           [-3.8172e-04,  1.4354e-02, -2.0458e-02]],\n",
       " \n",
       "          [[ 2.8361e-02, -3.9402e-02,  2.3592e-02],\n",
       "           [-1.6914e-02,  3.1797e-02,  1.0471e-02],\n",
       "           [-3.2137e-02,  2.4701e-03, -2.4310e-02]],\n",
       " \n",
       "          [[ 3.4360e-02,  9.9863e-03,  3.2582e-02],\n",
       "           [-3.5713e-02,  2.1561e-02,  2.0948e-02],\n",
       "           [ 2.8734e-02, -2.4535e-02, -1.9657e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.8352e-02, -3.0637e-02, -4.1497e-02],\n",
       "           [-2.1351e-02,  1.9204e-02,  6.9097e-03],\n",
       "           [ 9.2071e-03, -2.6410e-02, -2.4193e-03]],\n",
       " \n",
       "          [[ 3.6037e-02,  3.7034e-02,  2.1712e-02],\n",
       "           [-1.5547e-02,  4.0763e-02, -2.4413e-02],\n",
       "           [-3.5063e-02, -6.3464e-03,  1.9633e-02]],\n",
       " \n",
       "          [[-1.1322e-02, -1.5831e-02, -2.7898e-02],\n",
       "           [-1.1176e-02, -4.0802e-02, -1.6075e-03],\n",
       "           [ 7.4114e-03, -1.5146e-02, -2.5475e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.6118e-02,  2.1552e-02, -3.4662e-02],\n",
       "           [-2.2085e-03,  1.5897e-02,  3.3573e-02],\n",
       "           [ 3.4382e-02,  2.0942e-02, -6.2279e-03]],\n",
       " \n",
       "          [[ 2.4146e-02, -3.4124e-02, -3.2239e-02],\n",
       "           [ 2.6915e-02,  1.0536e-02, -3.2622e-02],\n",
       "           [ 2.0656e-02, -1.9801e-02,  4.4186e-03]],\n",
       " \n",
       "          [[-1.3196e-02, -6.5854e-04,  3.5568e-02],\n",
       "           [-4.1158e-02,  1.5582e-02, -3.1423e-02],\n",
       "           [-9.3038e-04,  4.6161e-03, -1.9687e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6987e-03,  7.5396e-03, -7.5504e-03],\n",
       "           [ 9.4232e-03,  2.9016e-02,  2.9479e-02],\n",
       "           [-3.7800e-02,  7.1434e-03, -2.1037e-02]],\n",
       " \n",
       "          [[-1.7767e-02,  7.6964e-03,  6.1694e-03],\n",
       "           [ 2.9679e-02,  1.3413e-02, -7.4646e-04],\n",
       "           [-1.9178e-02, -9.6990e-03, -3.2808e-02]],\n",
       " \n",
       "          [[-3.6097e-02,  2.4256e-02,  3.8199e-02],\n",
       "           [-2.4204e-02, -3.6429e-02, -2.7111e-02],\n",
       "           [ 2.0422e-02,  3.4729e-03,  3.5056e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.2510e-02, -1.9212e-02,  2.3803e-02],\n",
       "           [ 1.3745e-02, -2.2359e-02,  2.6260e-02],\n",
       "           [-4.0578e-02,  3.3985e-02,  2.1799e-02]],\n",
       " \n",
       "          [[-6.0673e-03,  3.1644e-02,  3.4686e-03],\n",
       "           [ 3.6875e-02, -4.1006e-02, -6.1496e-03],\n",
       "           [ 7.4729e-03,  1.8152e-03, -1.0912e-02]],\n",
       " \n",
       "          [[-2.6291e-02, -1.4366e-03,  2.9217e-03],\n",
       "           [ 1.3097e-02,  3.0829e-02, -3.4937e-02],\n",
       "           [-3.4530e-02, -3.6971e-02,  1.4080e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.6622e-03,  2.5039e-02,  2.8354e-02],\n",
       "           [ 2.3704e-02,  8.1194e-03,  1.8403e-02],\n",
       "           [ 3.3893e-02, -2.9643e-02, -2.5597e-02]],\n",
       " \n",
       "          [[-5.5600e-03,  3.8782e-02,  1.3900e-03],\n",
       "           [-1.6761e-02,  9.2933e-03,  9.7108e-03],\n",
       "           [-2.1357e-03,  3.8266e-02,  3.8584e-02]],\n",
       " \n",
       "          [[ 1.8071e-02,  7.9090e-03,  1.1067e-02],\n",
       "           [ 1.0860e-02,  9.1985e-03,  9.1101e-03],\n",
       "           [ 5.3729e-03, -2.3801e-02, -3.0802e-02]]]]),\n",
       " 'layer4.0.bias': tensor([ 0.0177, -0.0163, -0.0093,  0.0162, -0.0377,  0.0078,  0.0001,  0.0027,\n",
       "         -0.0377,  0.0106, -0.0282,  0.0339, -0.0121, -0.0010, -0.0151, -0.0266,\n",
       "          0.0306,  0.0092, -0.0270,  0.0387,  0.0071,  0.0043,  0.0337, -0.0290,\n",
       "          0.0054, -0.0309, -0.0099,  0.0037,  0.0143,  0.0311, -0.0226, -0.0143,\n",
       "         -0.0285, -0.0150,  0.0323,  0.0416,  0.0100, -0.0162, -0.0373,  0.0130,\n",
       "         -0.0095,  0.0198,  0.0192, -0.0277,  0.0032,  0.0406,  0.0073, -0.0256,\n",
       "         -0.0246,  0.0203,  0.0312, -0.0264, -0.0383, -0.0415, -0.0085, -0.0052,\n",
       "         -0.0147,  0.0275,  0.0078,  0.0238,  0.0250,  0.0253, -0.0249,  0.0367]),\n",
       " 'layer4.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer4.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer5.0.weight': tensor([[[[ 4.0029e-02, -1.5555e-02, -1.9442e-02],\n",
       "           [ 2.7807e-02,  1.2606e-03,  2.2996e-02],\n",
       "           [ 1.6510e-02, -1.8767e-02,  1.4049e-02]],\n",
       " \n",
       "          [[-2.0909e-02,  3.2355e-03,  4.8950e-04],\n",
       "           [ 3.5541e-02, -2.6028e-02, -2.1366e-02],\n",
       "           [-1.8651e-02, -1.4314e-02, -1.5306e-04]],\n",
       " \n",
       "          [[-2.3171e-02,  1.9146e-02,  1.5210e-03],\n",
       "           [ 1.9624e-02, -1.5651e-02, -2.6014e-02],\n",
       "           [-3.2673e-02,  1.8227e-02,  3.6811e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4583e-02, -7.9460e-06, -1.4085e-02],\n",
       "           [ 2.8421e-02, -1.1669e-02,  1.9115e-02],\n",
       "           [ 3.0063e-02,  1.8295e-02, -2.4148e-02]],\n",
       " \n",
       "          [[-3.0112e-02,  3.7937e-02,  3.5968e-02],\n",
       "           [ 1.3086e-02, -1.0907e-02,  2.0002e-02],\n",
       "           [-5.9486e-03,  6.4267e-03, -1.3565e-02]],\n",
       " \n",
       "          [[ 3.3119e-02,  2.9293e-02,  4.7366e-03],\n",
       "           [ 1.3828e-02, -8.6379e-04, -3.8102e-02],\n",
       "           [ 2.7153e-02,  3.1091e-02,  1.0126e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.4183e-03, -1.7757e-03, -2.0380e-02],\n",
       "           [-1.5854e-02, -2.6645e-02,  7.1543e-03],\n",
       "           [ 6.6951e-03, -1.8744e-02, -1.2158e-02]],\n",
       " \n",
       "          [[ 1.2498e-02,  2.4377e-02,  2.7904e-02],\n",
       "           [ 3.8156e-02,  3.0734e-02,  8.7722e-03],\n",
       "           [ 2.6730e-02, -3.1139e-02,  3.4744e-02]],\n",
       " \n",
       "          [[ 2.6575e-02,  2.4073e-02, -3.0729e-02],\n",
       "           [ 2.7972e-02, -3.6623e-02,  2.2270e-02],\n",
       "           [-2.7218e-02,  3.9759e-02, -2.6646e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.4700e-03,  2.2825e-03,  3.6883e-02],\n",
       "           [ 2.7217e-02, -3.1232e-02,  3.1608e-02],\n",
       "           [-1.0017e-02,  3.1206e-02, -3.9591e-02]],\n",
       " \n",
       "          [[-2.9666e-02, -4.0470e-02, -6.2156e-03],\n",
       "           [ 3.0331e-02, -3.6313e-04, -1.9975e-02],\n",
       "           [-3.7550e-02,  2.9411e-02, -3.2907e-02]],\n",
       " \n",
       "          [[ 2.4071e-03,  2.2413e-02,  2.5404e-02],\n",
       "           [ 1.3434e-02, -3.0588e-02,  9.5532e-04],\n",
       "           [-3.2427e-02,  1.6529e-02, -1.6419e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.3955e-02,  7.1121e-03,  2.6891e-02],\n",
       "           [ 4.0416e-02, -3.3873e-02, -1.3531e-02],\n",
       "           [ 1.8637e-02,  5.2441e-05,  2.1841e-02]],\n",
       " \n",
       "          [[-1.7873e-02,  3.0563e-02,  1.1849e-02],\n",
       "           [ 9.3570e-03, -1.1101e-02,  8.0893e-03],\n",
       "           [-8.1875e-03, -1.7321e-02, -9.3326e-05]],\n",
       " \n",
       "          [[-2.1851e-02,  4.0022e-02,  8.1340e-03],\n",
       "           [ 1.9282e-02, -2.6939e-02,  1.7700e-02],\n",
       "           [ 7.5512e-03,  3.9676e-02, -3.6485e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.8432e-03,  1.4899e-02,  2.2023e-02],\n",
       "           [-5.1086e-03, -4.1410e-03, -1.7824e-02],\n",
       "           [ 3.6685e-02, -3.0121e-02, -2.4863e-02]],\n",
       " \n",
       "          [[ 7.5307e-03,  2.6082e-02,  2.2635e-02],\n",
       "           [-3.3264e-02, -1.2277e-02, -1.2866e-02],\n",
       "           [ 3.9813e-02, -1.0466e-02, -7.6253e-03]],\n",
       " \n",
       "          [[-3.7760e-02,  7.5000e-03, -5.7763e-03],\n",
       "           [-3.5322e-02,  3.6979e-02,  2.8847e-03],\n",
       "           [ 6.5473e-03,  3.5336e-02, -5.7323e-03]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 3.4297e-02, -3.6211e-02,  2.7521e-02],\n",
       "           [-9.7694e-03,  1.5891e-02,  1.4592e-02],\n",
       "           [-3.0556e-02,  9.3818e-03,  4.0286e-02]],\n",
       " \n",
       "          [[-2.7050e-02, -1.7057e-02,  2.0408e-02],\n",
       "           [ 2.3277e-02,  3.4720e-02, -2.5510e-02],\n",
       "           [ 3.2969e-02, -3.7801e-02, -2.1019e-02]],\n",
       " \n",
       "          [[ 3.2294e-02, -5.6080e-03,  3.2732e-02],\n",
       "           [ 8.6077e-03,  2.3426e-02, -2.1425e-02],\n",
       "           [-2.0202e-03,  2.7345e-02,  3.4072e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.1652e-02,  3.0896e-02, -2.1383e-02],\n",
       "           [ 3.9330e-02, -3.5811e-02,  1.0374e-02],\n",
       "           [ 1.3004e-02,  1.1354e-02, -3.0207e-02]],\n",
       " \n",
       "          [[-1.4926e-02, -2.4606e-02, -2.8183e-02],\n",
       "           [ 2.0890e-03,  3.1961e-02, -2.7149e-02],\n",
       "           [ 3.3376e-02, -4.2753e-03,  1.3732e-02]],\n",
       " \n",
       "          [[-3.1865e-02,  2.2441e-02, -2.7422e-02],\n",
       "           [ 3.6593e-02, -1.8769e-02, -1.5301e-02],\n",
       "           [ 1.6204e-02,  1.2891e-02, -2.4532e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.5621e-02, -7.0786e-03, -1.0761e-02],\n",
       "           [-5.0155e-03, -3.1408e-02,  3.0269e-02],\n",
       "           [ 3.1515e-02, -2.1544e-02,  2.6874e-04]],\n",
       " \n",
       "          [[-4.1376e-02, -6.8147e-04, -2.9623e-02],\n",
       "           [-2.8259e-02,  3.9103e-03, -2.5514e-02],\n",
       "           [ 4.5687e-03, -1.8919e-02, -7.3692e-03]],\n",
       " \n",
       "          [[-2.4708e-02, -2.0455e-02, -1.3034e-02],\n",
       "           [ 2.2828e-02,  4.3709e-04,  3.2861e-02],\n",
       "           [ 1.2342e-02,  3.5667e-02, -2.3626e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.7968e-02,  2.0814e-02, -1.4589e-02],\n",
       "           [ 3.5259e-02,  3.3609e-02,  1.2301e-02],\n",
       "           [ 7.3563e-03, -1.7608e-02,  2.7465e-02]],\n",
       " \n",
       "          [[ 6.1034e-03, -1.7165e-02, -6.9117e-03],\n",
       "           [ 2.9456e-02,  3.4874e-02,  1.7228e-02],\n",
       "           [ 3.4576e-02,  1.7019e-02, -2.9988e-02]],\n",
       " \n",
       "          [[-3.0170e-02, -2.8234e-02,  3.7775e-02],\n",
       "           [ 3.1383e-02,  3.2691e-02,  3.0495e-02],\n",
       "           [ 3.4685e-03, -1.7589e-02,  2.4322e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7058e-04, -2.9093e-02, -4.0108e-02],\n",
       "           [ 3.2973e-02,  1.0041e-02,  2.1036e-02],\n",
       "           [-2.9385e-02, -6.8148e-03,  3.9885e-02]],\n",
       " \n",
       "          [[ 2.3979e-03, -3.9971e-02, -1.7004e-02],\n",
       "           [-3.3634e-02,  1.1719e-02, -2.8307e-02],\n",
       "           [-3.2483e-02, -1.1167e-02,  1.5263e-02]],\n",
       " \n",
       "          [[ 6.7131e-03,  3.7708e-02,  3.4297e-02],\n",
       "           [ 2.8094e-02,  3.8445e-02,  3.6810e-05],\n",
       "           [ 8.6228e-03,  1.4023e-02,  2.4469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.1655e-02,  3.6069e-02, -1.6624e-02],\n",
       "           [ 2.2531e-02,  3.7367e-02, -2.7337e-02],\n",
       "           [-5.4987e-03, -3.2980e-03, -2.1241e-02]],\n",
       " \n",
       "          [[-2.5833e-03,  4.0529e-02, -2.8201e-02],\n",
       "           [ 7.9990e-03, -1.9770e-02, -9.0786e-03],\n",
       "           [ 1.9084e-02,  1.7834e-02,  1.1820e-02]],\n",
       " \n",
       "          [[-2.1395e-02, -1.0014e-02,  3.2037e-02],\n",
       "           [ 7.6473e-03,  2.1263e-02,  2.7528e-02],\n",
       "           [ 6.0682e-03,  1.0778e-02,  3.1273e-02]]]]),\n",
       " 'layer5.0.bias': tensor([-0.0339, -0.0223, -0.0282,  0.0156, -0.0012,  0.0039, -0.0381, -0.0168,\n",
       "         -0.0001,  0.0406, -0.0392, -0.0376,  0.0081, -0.0122,  0.0069, -0.0187,\n",
       "         -0.0228,  0.0056,  0.0213,  0.0152, -0.0076,  0.0395,  0.0266,  0.0247,\n",
       "         -0.0403, -0.0306,  0.0379, -0.0167, -0.0330, -0.0408, -0.0215,  0.0353,\n",
       "         -0.0026, -0.0162, -0.0388, -0.0065, -0.0216,  0.0155, -0.0153, -0.0108,\n",
       "         -0.0357, -0.0315, -0.0267, -0.0286, -0.0062, -0.0391,  0.0140, -0.0273,\n",
       "          0.0051,  0.0157, -0.0315, -0.0401,  0.0289, -0.0257, -0.0148,  0.0070,\n",
       "          0.0054,  0.0089, -0.0287, -0.0140, -0.0269,  0.0319, -0.0325, -0.0247]),\n",
       " 'linear.weight': tensor([[ 0.0916,  0.0274,  0.0482, -0.0863,  0.0164, -0.0115, -0.0446,  0.1144,\n",
       "           0.0822,  0.0646,  0.0713,  0.0645,  0.0248,  0.0175,  0.0264,  0.0958,\n",
       "           0.1223,  0.0784, -0.0136,  0.1037,  0.0573,  0.0770, -0.0095, -0.0206,\n",
       "           0.1065,  0.0468,  0.0714,  0.1024,  0.0972, -0.0708, -0.0112,  0.0418,\n",
       "          -0.0384, -0.1069, -0.0260, -0.1035,  0.0921, -0.0480, -0.0806, -0.0412,\n",
       "           0.1044, -0.1037,  0.0918,  0.1132, -0.0616,  0.0739,  0.0543,  0.0419,\n",
       "          -0.1071,  0.0925, -0.0029, -0.0504, -0.1013, -0.1048, -0.1139,  0.0840,\n",
       "           0.0123, -0.0573,  0.0097,  0.1013,  0.0002,  0.0757, -0.1092, -0.0977],\n",
       "         [-0.0976,  0.0218,  0.1236, -0.0156, -0.0988, -0.0866,  0.0803,  0.0230,\n",
       "          -0.0454, -0.0290,  0.1180, -0.1043, -0.1040, -0.0692, -0.0319,  0.0671,\n",
       "           0.0057,  0.0187,  0.1229,  0.0675,  0.0289, -0.0853,  0.0172,  0.0875,\n",
       "          -0.0131, -0.0520, -0.0023, -0.0837, -0.1215,  0.0070,  0.0172, -0.0861,\n",
       "           0.0510,  0.1128,  0.0698, -0.0959,  0.1047, -0.0330,  0.0005,  0.0720,\n",
       "           0.0032, -0.0150, -0.0510, -0.0521,  0.1005,  0.0592, -0.1216, -0.1142,\n",
       "           0.1248,  0.0489, -0.0867, -0.0501, -0.0520,  0.0541, -0.0905,  0.0906,\n",
       "           0.0294,  0.0876,  0.0433, -0.0863,  0.0969, -0.0604, -0.1024, -0.0643],\n",
       "         [ 0.1154, -0.1186, -0.0199, -0.0405, -0.1105,  0.0765,  0.0550, -0.0141,\n",
       "          -0.1237, -0.0296, -0.0304,  0.1103, -0.1205,  0.0008,  0.0556,  0.0408,\n",
       "           0.0326, -0.0969,  0.1037, -0.0864, -0.0293, -0.0055, -0.0150,  0.0192,\n",
       "          -0.0729, -0.0636,  0.0296, -0.0577, -0.0287, -0.0820, -0.0236,  0.0443,\n",
       "          -0.0256, -0.0003, -0.0927, -0.0993,  0.0258,  0.1160, -0.0434, -0.0916,\n",
       "          -0.0447, -0.0580,  0.0066,  0.0233,  0.1130, -0.1072, -0.0153, -0.0757,\n",
       "          -0.1002, -0.0961,  0.0014, -0.0149, -0.1029,  0.1006, -0.0517,  0.0877,\n",
       "          -0.0534, -0.0673,  0.0225,  0.0355,  0.1090, -0.1145,  0.1039, -0.0724],\n",
       "         [ 0.0105, -0.0863,  0.0174,  0.0673, -0.0783, -0.0996,  0.0594, -0.0393,\n",
       "          -0.0898,  0.0047, -0.1083,  0.0007,  0.0800, -0.1056,  0.0060, -0.1029,\n",
       "           0.0600,  0.0489, -0.0016,  0.0796,  0.0368, -0.0254, -0.1187, -0.0487,\n",
       "          -0.0344, -0.0922, -0.0645, -0.0865,  0.0391,  0.0313,  0.0412,  0.1113,\n",
       "          -0.1223, -0.0501, -0.0932, -0.1201,  0.0443,  0.0366, -0.0723,  0.0465,\n",
       "           0.0954,  0.0633,  0.0466, -0.0724, -0.0795, -0.0819, -0.1092,  0.0291,\n",
       "           0.0500,  0.0143, -0.0998, -0.0054,  0.1042,  0.0176, -0.0459,  0.0192,\n",
       "           0.0816, -0.0294, -0.0316, -0.0542,  0.0943, -0.0240,  0.0372, -0.0403],\n",
       "         [-0.0505, -0.0937, -0.1059,  0.0898,  0.1196, -0.1027,  0.0251,  0.0276,\n",
       "          -0.0344,  0.0518, -0.0177,  0.1089,  0.0241,  0.0386, -0.0508,  0.0748,\n",
       "          -0.0992,  0.0156,  0.0612, -0.1040, -0.0413,  0.0628,  0.0228, -0.1098,\n",
       "           0.1152,  0.0852, -0.0362,  0.0163,  0.0398,  0.0036,  0.1198,  0.0906,\n",
       "           0.0250,  0.1074,  0.0016, -0.1161, -0.1173, -0.0486, -0.0867,  0.1088,\n",
       "          -0.0890,  0.0498,  0.0614, -0.1214, -0.0523,  0.0367, -0.0253,  0.0643,\n",
       "          -0.0216,  0.0619,  0.0555,  0.1103, -0.0463, -0.1167, -0.0215, -0.0346,\n",
       "          -0.0006, -0.0089, -0.0418,  0.0205,  0.0769, -0.0936,  0.0747, -0.0514],\n",
       "         [-0.1041, -0.0652, -0.1099, -0.1172, -0.1183,  0.0261, -0.0061, -0.0181,\n",
       "           0.0306,  0.1034,  0.1023, -0.0415, -0.0590,  0.1013, -0.0350, -0.1135,\n",
       "           0.1211,  0.0423,  0.0534, -0.0943,  0.0192, -0.1066,  0.0101, -0.0907,\n",
       "          -0.0641, -0.0925,  0.0244,  0.0263,  0.0090, -0.0468,  0.0813,  0.0006,\n",
       "           0.0742,  0.0435,  0.0780, -0.0913,  0.0158, -0.1105, -0.0535, -0.0402,\n",
       "          -0.0937,  0.0195,  0.0634, -0.0159,  0.1228, -0.0416,  0.0739,  0.0429,\n",
       "           0.0029, -0.0882, -0.0594, -0.0101,  0.0290, -0.0891,  0.0971, -0.0061,\n",
       "          -0.0823, -0.0365,  0.0256, -0.1039,  0.1206,  0.1249,  0.1118, -0.0718]]),\n",
       " 'linear.bias': tensor([ 0.0939, -0.0072, -0.0662,  0.0920, -0.0422,  0.0693])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5253980a-4b74-47e1-8cba-bfcd9ae23134",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN:\n\tMissing key(s) in state_dict: \"layer0.1.running_mean\", \"layer0.1.running_var\", \"layer1.1.running_mean\", \"layer1.1.running_var\", \"layer2.1.running_mean\", \"layer2.1.running_var\", \"layer3.1.running_mean\", \"layer3.1.running_var\", \"layer4.1.running_mean\", \"layer4.1.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pai/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN:\n\tMissing key(s) in state_dict: \"layer0.1.running_mean\", \"layer0.1.running_var\", \"layer1.1.running_mean\", \"layer1.1.running_var\", \"layer2.1.running_mean\", \"layer2.1.running_var\", \"layer3.1.running_mean\", \"layer3.1.running_var\", \"layer4.1.running_mean\", \"layer4.1.running_var\". "
     ]
    }
   ],
   "source": [
    "network.load_state_dict(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152e191-0a51-4a38-a284-bbdc97b83e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "183ff56e-cff2-477a-9e92-8051a938026a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer0.0.weight': tensor([[[[ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
       "           [ 0.0692, -0.0238,  0.0587,  0.0161, -0.0141],\n",
       "           [ 0.0320,  0.0057,  0.0422, -0.0450, -0.0084],\n",
       "           [-0.0104,  0.0167, -0.0005,  0.1009,  0.0359],\n",
       "           [-0.0430, -0.0697, -0.0194, -0.0498, -0.0370]],\n",
       " \n",
       "          [[ 0.0055,  0.0688,  0.0628, -0.1129,  0.0716],\n",
       "           [ 0.0323,  0.1095,  0.0762, -0.1052, -0.1098],\n",
       "           [-0.0557,  0.1014, -0.0192,  0.0494, -0.0537],\n",
       "           [ 0.1133, -0.0489,  0.0866,  0.0014, -0.0608],\n",
       "           [ 0.0594, -0.0613,  0.0340, -0.0333, -0.0127]],\n",
       " \n",
       "          [[-0.1110, -0.0551,  0.0627, -0.0281,  0.1150],\n",
       "           [ 0.0926, -0.0054, -0.0771,  0.0703,  0.0358],\n",
       "           [-0.0746,  0.0750,  0.0701,  0.1024, -0.0647],\n",
       "           [-0.0190, -0.0022,  0.0169, -0.0876, -0.0819],\n",
       "           [ 0.0628, -0.0271,  0.0564,  0.0066,  0.0379]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0254,  0.0420,  0.0572, -0.1069,  0.0581],\n",
       "           [-0.0812, -0.0871,  0.0070, -0.0197,  0.0678],\n",
       "           [-0.0669, -0.1027,  0.0840, -0.0171,  0.0649],\n",
       "           [ 0.0371, -0.0866,  0.0232,  0.0277, -0.0773],\n",
       "           [-0.0548,  0.0394,  0.0207, -0.0491, -0.0350]],\n",
       " \n",
       "          [[ 0.1058, -0.0214,  0.0651,  0.0500, -0.0746],\n",
       "           [-0.0982,  0.1108,  0.0060,  0.0791,  0.0239],\n",
       "           [ 0.0371,  0.0863,  0.1095, -0.0766,  0.0144],\n",
       "           [ 0.0862,  0.0837,  0.0717, -0.0836, -0.0832],\n",
       "           [-0.0698,  0.0145,  0.1151, -0.0729,  0.0615]],\n",
       " \n",
       "          [[-0.0639, -0.1086, -0.0245,  0.0665,  0.1072],\n",
       "           [-0.0717,  0.0251,  0.0996,  0.0765,  0.0720],\n",
       "           [ 0.0821,  0.0730,  0.0298, -0.0790, -0.0970],\n",
       "           [-0.0529, -0.0134, -0.0708,  0.0422,  0.0357],\n",
       "           [-0.0261,  0.0444,  0.0373,  0.0705,  0.0778]]],\n",
       " \n",
       " \n",
       "         [[[-0.0391,  0.1128, -0.0133, -0.0040, -0.1090],\n",
       "           [-0.0743, -0.0675, -0.0494,  0.0821, -0.0377],\n",
       "           [-0.0863,  0.0444,  0.0370,  0.0748, -0.0598],\n",
       "           [ 0.0250, -0.0420, -0.0259, -0.0920, -0.0526],\n",
       "           [-0.0354,  0.0494,  0.0211,  0.0285,  0.1153]],\n",
       " \n",
       "          [[ 0.1125,  0.0788,  0.0037, -0.0799,  0.0902],\n",
       "           [-0.0289, -0.0093, -0.0995, -0.0228, -0.0745],\n",
       "           [ 0.1061, -0.0998, -0.0900, -0.0039, -0.0624],\n",
       "           [ 0.0413, -0.0444, -0.0542,  0.0065,  0.0836],\n",
       "           [-0.0812,  0.0542,  0.0742,  0.1130, -0.0808]],\n",
       " \n",
       "          [[ 0.0280, -0.0854,  0.0986, -0.0448,  0.0696],\n",
       "           [ 0.0034, -0.0090, -0.0037,  0.0196,  0.0544],\n",
       "           [ 0.0185,  0.0352, -0.1039,  0.0841,  0.1007],\n",
       "           [ 0.0954,  0.0854, -0.0833, -0.0428,  0.1018],\n",
       "           [-0.0879,  0.1048, -0.0908, -0.0813,  0.0565]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0058, -0.0155,  0.0586, -0.0491,  0.0908],\n",
       "           [-0.0787, -0.0601,  0.0400,  0.0710, -0.0014],\n",
       "           [-0.0906, -0.0046, -0.0390, -0.0369, -0.0213],\n",
       "           [ 0.0278, -0.0269, -0.0810, -0.0577, -0.0967],\n",
       "           [-0.1153,  0.0558,  0.0645,  0.0873,  0.0091]],\n",
       " \n",
       "          [[-0.0475, -0.0694,  0.0575, -0.0698,  0.0863],\n",
       "           [-0.0626, -0.1077, -0.0832,  0.0940,  0.0872],\n",
       "           [ 0.0282,  0.0027,  0.0990,  0.0697,  0.0684],\n",
       "           [-0.0786,  0.0185,  0.1046, -0.0813,  0.1150],\n",
       "           [-0.0756, -0.0339, -0.0054,  0.0300,  0.0355]],\n",
       " \n",
       "          [[-0.0660,  0.0529,  0.0306, -0.0129, -0.1149],\n",
       "           [ 0.1143, -0.0427, -0.0615,  0.1143, -0.0585],\n",
       "           [ 0.0771,  0.0705, -0.0752, -0.0931, -0.0628],\n",
       "           [ 0.0361,  0.1089, -0.0064, -0.0345, -0.0332],\n",
       "           [-0.0804,  0.1154, -0.0013, -0.0914, -0.0789]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0718,  0.0559,  0.0745, -0.0122, -0.0228],\n",
       "           [ 0.0378, -0.0011, -0.0372,  0.0275, -0.0658],\n",
       "           [-0.0800, -0.0255, -0.0823,  0.0029,  0.0182],\n",
       "           [ 0.1131,  0.0333, -0.0007, -0.0839, -0.0464],\n",
       "           [-0.0188, -0.0831,  0.0225,  0.0831,  0.0661]],\n",
       " \n",
       "          [[-0.0479, -0.0282,  0.1151, -0.0185, -0.1037],\n",
       "           [-0.0010,  0.0059, -0.0429, -0.0125, -0.0942],\n",
       "           [ 0.0356, -0.0801,  0.0397,  0.0870, -0.0745],\n",
       "           [-0.0452,  0.0111,  0.0003,  0.0677, -0.0069],\n",
       "           [ 0.0187, -0.0319,  0.0381,  0.0349, -0.0930]],\n",
       " \n",
       "          [[ 0.0613, -0.0237,  0.1034, -0.1146, -0.0115],\n",
       "           [ 0.0522,  0.0796,  0.0015, -0.0624, -0.0893],\n",
       "           [-0.0874,  0.0930, -0.1006, -0.0457,  0.0605],\n",
       "           [ 0.0631, -0.1010,  0.0229,  0.0140,  0.1060],\n",
       "           [-0.0041,  0.0490, -0.0100,  0.1031, -0.0118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0679,  0.0213, -0.0611,  0.0289,  0.0803],\n",
       "           [-0.0898, -0.0840,  0.0016, -0.0068, -0.0359],\n",
       "           [ 0.0967,  0.0518, -0.0383,  0.0167, -0.0174],\n",
       "           [-0.0739,  0.0472, -0.0762,  0.0140,  0.0784],\n",
       "           [-0.1061,  0.0051,  0.1066,  0.0521, -0.0455]],\n",
       " \n",
       "          [[ 0.0182, -0.1084,  0.0185,  0.1004,  0.1003],\n",
       "           [-0.0018,  0.0419, -0.0192, -0.0324,  0.0022],\n",
       "           [-0.0673,  0.0465,  0.0566, -0.0313, -0.0263],\n",
       "           [-0.0805,  0.0148,  0.0459,  0.1096, -0.1088],\n",
       "           [-0.0974,  0.0033,  0.1067,  0.0074,  0.0321]],\n",
       " \n",
       "          [[ 0.0164, -0.0138,  0.0824,  0.0848, -0.0666],\n",
       "           [-0.0692, -0.0898, -0.0281, -0.0026,  0.0966],\n",
       "           [-0.1118,  0.0820, -0.0645,  0.0094, -0.0054],\n",
       "           [ 0.0652, -0.0966, -0.0806,  0.0026,  0.0582],\n",
       "           [-0.0556,  0.0358, -0.0305, -0.1077,  0.0132]]]]),\n",
       " 'layer0.0.bias': tensor([-0.0246,  0.0683,  0.0761,  0.0755,  0.0723, -0.0893, -0.1108,  0.0129,\n",
       "          0.0335, -0.0888,  0.0944, -0.0122, -0.0677,  0.0792,  0.1011,  0.0177,\n",
       "         -0.0368,  0.0125, -0.0851,  0.0712,  0.0725, -0.0216, -0.0915,  0.0857,\n",
       "         -0.0945, -0.0606,  0.0689, -0.0574,  0.0220,  0.0949, -0.0146, -0.0261]),\n",
       " 'layer0.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer0.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer1.0.weight': tensor([[[[ 0.0546,  0.0101,  0.0206],\n",
       "           [-0.0194, -0.0508,  0.0092],\n",
       "           [-0.0348, -0.0305, -0.0483]],\n",
       " \n",
       "          [[ 0.0505, -0.0396,  0.0170],\n",
       "           [ 0.0121,  0.0407,  0.0322],\n",
       "           [-0.0413,  0.0261, -0.0299]],\n",
       " \n",
       "          [[ 0.0022, -0.0213, -0.0504],\n",
       "           [ 0.0176, -0.0393,  0.0577],\n",
       "           [ 0.0237, -0.0204, -0.0345]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0583,  0.0324,  0.0416],\n",
       "           [ 0.0469,  0.0359, -0.0188],\n",
       "           [ 0.0121,  0.0149, -0.0563]],\n",
       " \n",
       "          [[ 0.0318, -0.0475,  0.0212],\n",
       "           [ 0.0569, -0.0164,  0.0081],\n",
       "           [ 0.0546,  0.0486,  0.0492]],\n",
       " \n",
       "          [[-0.0408,  0.0116, -0.0493],\n",
       "           [ 0.0108, -0.0558,  0.0068],\n",
       "           [ 0.0298, -0.0525, -0.0090]]],\n",
       " \n",
       " \n",
       "         [[[-0.0244,  0.0484, -0.0058],\n",
       "           [ 0.0367, -0.0435,  0.0082],\n",
       "           [ 0.0427,  0.0033, -0.0206]],\n",
       " \n",
       "          [[-0.0241,  0.0402,  0.0232],\n",
       "           [ 0.0040,  0.0348, -0.0307],\n",
       "           [-0.0447, -0.0133,  0.0432]],\n",
       " \n",
       "          [[-0.0570, -0.0293,  0.0143],\n",
       "           [ 0.0222,  0.0295,  0.0178],\n",
       "           [-0.0520, -0.0192,  0.0362]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0269, -0.0078, -0.0461],\n",
       "           [-0.0380, -0.0191, -0.0069],\n",
       "           [-0.0127,  0.0033, -0.0540]],\n",
       " \n",
       "          [[-0.0095, -0.0517,  0.0310],\n",
       "           [ 0.0537, -0.0465, -0.0424],\n",
       "           [ 0.0571,  0.0458,  0.0271]],\n",
       " \n",
       "          [[-0.0372,  0.0185, -0.0019],\n",
       "           [ 0.0444,  0.0367, -0.0067],\n",
       "           [ 0.0230, -0.0515,  0.0101]]],\n",
       " \n",
       " \n",
       "         [[[-0.0359,  0.0468, -0.0428],\n",
       "           [-0.0512, -0.0502, -0.0168],\n",
       "           [ 0.0032, -0.0068,  0.0242]],\n",
       " \n",
       "          [[-0.0523, -0.0346, -0.0082],\n",
       "           [-0.0019, -0.0128,  0.0270],\n",
       "           [ 0.0305, -0.0106, -0.0029]],\n",
       " \n",
       "          [[-0.0092,  0.0058, -0.0247],\n",
       "           [-0.0092, -0.0432,  0.0493],\n",
       "           [ 0.0403, -0.0400,  0.0044]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0401, -0.0285, -0.0259],\n",
       "           [-0.0050, -0.0551,  0.0106],\n",
       "           [ 0.0445,  0.0369, -0.0460]],\n",
       " \n",
       "          [[-0.0178,  0.0308,  0.0211],\n",
       "           [ 0.0263,  0.0247, -0.0142],\n",
       "           [ 0.0390,  0.0552,  0.0218]],\n",
       " \n",
       "          [[-0.0097,  0.0256,  0.0409],\n",
       "           [-0.0466,  0.0163, -0.0281],\n",
       "           [ 0.0030, -0.0391, -0.0109]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0194, -0.0357,  0.0117],\n",
       "           [ 0.0072, -0.0325, -0.0014],\n",
       "           [ 0.0456, -0.0435,  0.0098]],\n",
       " \n",
       "          [[ 0.0320,  0.0543, -0.0537],\n",
       "           [ 0.0267,  0.0560,  0.0403],\n",
       "           [-0.0508, -0.0085, -0.0072]],\n",
       " \n",
       "          [[-0.0085,  0.0042,  0.0038],\n",
       "           [ 0.0312, -0.0462, -0.0416],\n",
       "           [-0.0018, -0.0555,  0.0578]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0169,  0.0042,  0.0447],\n",
       "           [-0.0125,  0.0027,  0.0512],\n",
       "           [-0.0242, -0.0469, -0.0531]],\n",
       " \n",
       "          [[-0.0371, -0.0043, -0.0548],\n",
       "           [-0.0245,  0.0057,  0.0537],\n",
       "           [-0.0011,  0.0426,  0.0344]],\n",
       " \n",
       "          [[-0.0172,  0.0337, -0.0225],\n",
       "           [ 0.0088, -0.0114,  0.0121],\n",
       "           [-0.0192,  0.0237,  0.0269]]],\n",
       " \n",
       " \n",
       "         [[[-0.0037, -0.0553, -0.0541],\n",
       "           [-0.0205, -0.0239,  0.0142],\n",
       "           [ 0.0084, -0.0058,  0.0161]],\n",
       " \n",
       "          [[ 0.0074,  0.0498,  0.0194],\n",
       "           [-0.0311, -0.0490, -0.0356],\n",
       "           [-0.0336,  0.0319, -0.0069]],\n",
       " \n",
       "          [[-0.0362, -0.0574, -0.0031],\n",
       "           [-0.0213, -0.0345,  0.0364],\n",
       "           [ 0.0138, -0.0144,  0.0195]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0320, -0.0554, -0.0148],\n",
       "           [-0.0214,  0.0331,  0.0398],\n",
       "           [ 0.0071,  0.0106, -0.0159]],\n",
       " \n",
       "          [[-0.0255,  0.0531,  0.0584],\n",
       "           [ 0.0261,  0.0347, -0.0573],\n",
       "           [ 0.0132,  0.0088, -0.0572]],\n",
       " \n",
       "          [[ 0.0217, -0.0435,  0.0497],\n",
       "           [ 0.0121,  0.0476, -0.0020],\n",
       "           [-0.0400,  0.0024, -0.0448]]],\n",
       " \n",
       " \n",
       "         [[[-0.0536, -0.0045,  0.0082],\n",
       "           [ 0.0016,  0.0405, -0.0227],\n",
       "           [-0.0341,  0.0054,  0.0062]],\n",
       " \n",
       "          [[-0.0014, -0.0461, -0.0520],\n",
       "           [-0.0377,  0.0143,  0.0103],\n",
       "           [ 0.0545, -0.0154, -0.0308]],\n",
       " \n",
       "          [[-0.0100,  0.0565, -0.0555],\n",
       "           [ 0.0302, -0.0189,  0.0516],\n",
       "           [ 0.0415,  0.0222,  0.0087]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0453, -0.0079,  0.0073],\n",
       "           [-0.0204,  0.0051, -0.0157],\n",
       "           [ 0.0565,  0.0389, -0.0149]],\n",
       " \n",
       "          [[ 0.0438, -0.0230,  0.0282],\n",
       "           [ 0.0552, -0.0253,  0.0328],\n",
       "           [-0.0390, -0.0553,  0.0518]],\n",
       " \n",
       "          [[ 0.0100, -0.0365,  0.0531],\n",
       "           [ 0.0285,  0.0130, -0.0063],\n",
       "           [ 0.0179, -0.0586, -0.0554]]]]),\n",
       " 'layer1.0.bias': tensor([-0.0543, -0.0043, -0.0017,  0.0498, -0.0517,  0.0165,  0.0087,  0.0565,\n",
       "          0.0444, -0.0543, -0.0388,  0.0033,  0.0551,  0.0235,  0.0446, -0.0100,\n",
       "         -0.0346,  0.0379,  0.0387,  0.0266, -0.0253,  0.0476,  0.0511, -0.0268,\n",
       "         -0.0131,  0.0463,  0.0268,  0.0487,  0.0120, -0.0506,  0.0130, -0.0039]),\n",
       " 'layer1.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer1.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer2.0.weight': tensor([[[[ 5.7134e-02,  2.2763e-03,  7.6729e-03],\n",
       "           [ 2.2283e-02, -5.0950e-02, -1.4664e-02],\n",
       "           [-4.8651e-02, -4.9914e-02,  2.2173e-02]],\n",
       " \n",
       "          [[-2.2566e-02, -3.4802e-02, -5.1511e-02],\n",
       "           [ 5.8050e-02, -4.7091e-02, -4.5683e-02],\n",
       "           [-4.2887e-02, -2.2843e-02,  9.9157e-03]],\n",
       " \n",
       "          [[-1.1733e-02, -4.2466e-02,  3.9983e-02],\n",
       "           [-3.8778e-02, -3.6147e-02,  6.0745e-03],\n",
       "           [-1.9005e-02, -3.8071e-02, -4.7358e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8251e-02, -2.0059e-02, -1.7664e-03],\n",
       "           [ 3.6452e-02, -5.8274e-02, -2.5591e-02],\n",
       "           [ 3.0273e-02,  2.4939e-02, -1.4607e-02]],\n",
       " \n",
       "          [[-5.6558e-02,  1.9704e-02, -1.7753e-02],\n",
       "           [ 2.6979e-02,  5.5089e-02,  1.3478e-02],\n",
       "           [ 2.6325e-02,  5.3409e-02, -1.9308e-02]],\n",
       " \n",
       "          [[ 1.3918e-02, -4.2279e-03,  3.3063e-02],\n",
       "           [ 1.9445e-02, -3.4774e-03,  1.6238e-02],\n",
       "           [ 6.9609e-04, -3.7400e-02,  2.5798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.9733e-02,  5.1662e-02, -3.5295e-02],\n",
       "           [-4.7838e-03, -2.9584e-02,  1.9393e-02],\n",
       "           [-4.9178e-02,  1.5128e-02,  3.1844e-02]],\n",
       " \n",
       "          [[-4.8651e-02, -2.3051e-02,  4.6810e-02],\n",
       "           [ 2.2733e-02,  3.7091e-02,  7.8823e-03],\n",
       "           [ 4.6600e-05, -3.1117e-02, -2.0206e-02]],\n",
       " \n",
       "          [[-5.0931e-02,  5.2320e-02,  1.5128e-02],\n",
       "           [-1.2424e-02, -3.7507e-02,  2.9012e-04],\n",
       "           [ 1.3300e-02,  4.9583e-03,  2.4148e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.8632e-02, -4.4404e-02,  1.1814e-02],\n",
       "           [-1.0807e-02,  4.8829e-02,  4.4790e-02],\n",
       "           [-2.0285e-02,  3.6822e-02,  2.8248e-02]],\n",
       " \n",
       "          [[ 1.7641e-02,  1.0618e-02, -3.1017e-02],\n",
       "           [-3.5140e-02, -1.0367e-02, -4.9357e-03],\n",
       "           [-2.6300e-03,  1.1192e-02, -4.5672e-02]],\n",
       " \n",
       "          [[-3.9225e-03, -3.7740e-02, -5.4893e-02],\n",
       "           [ 2.5574e-02,  5.4951e-02, -2.9725e-02],\n",
       "           [ 1.5240e-02, -3.6052e-02,  3.4700e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 5.5491e-02, -5.2750e-03, -1.8764e-02],\n",
       "           [-1.7387e-02, -5.2585e-02,  7.1799e-03],\n",
       "           [-2.7637e-02, -4.1692e-02,  3.6744e-02]],\n",
       " \n",
       "          [[ 2.0283e-02, -4.8516e-02, -2.9705e-02],\n",
       "           [ 3.5179e-02,  2.7032e-02,  5.6624e-02],\n",
       "           [-3.8420e-03,  1.8063e-02, -2.4304e-02]],\n",
       " \n",
       "          [[-3.5589e-02, -2.8323e-02, -3.3601e-03],\n",
       "           [-6.5405e-04, -5.8679e-02,  5.0192e-02],\n",
       "           [-3.0432e-02, -3.8527e-02, -4.3469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2697e-02, -1.4331e-02,  5.0963e-02],\n",
       "           [ 5.5813e-02,  1.8976e-02,  1.3342e-02],\n",
       "           [ 2.3138e-02,  3.1858e-02,  3.6571e-02]],\n",
       " \n",
       "          [[ 3.4722e-02, -2.1318e-02,  5.0169e-02],\n",
       "           [ 1.3877e-02,  1.4181e-02,  2.9957e-02],\n",
       "           [ 4.5517e-02,  2.8146e-02,  4.4315e-02]],\n",
       " \n",
       "          [[-2.2685e-02,  3.5278e-02, -7.6343e-03],\n",
       "           [ 3.1103e-02, -1.9444e-02, -2.7423e-02],\n",
       "           [ 5.1713e-02, -2.1571e-02, -4.5952e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-5.5247e-02,  3.1720e-02,  2.1539e-02],\n",
       "           [-3.8513e-02, -4.7985e-02, -3.3430e-02],\n",
       "           [-3.0489e-02, -1.7572e-02,  4.2375e-02]],\n",
       " \n",
       "          [[ 4.4120e-02,  5.4889e-02,  2.4734e-02],\n",
       "           [ 3.6550e-03, -4.3214e-02, -2.4624e-03],\n",
       "           [-4.2030e-02,  4.0658e-02, -5.2674e-02]],\n",
       " \n",
       "          [[-7.3320e-03, -7.3222e-03,  3.7609e-02],\n",
       "           [-3.0710e-02, -4.8105e-02, -1.4401e-02],\n",
       "           [-3.6586e-03,  1.9842e-02,  1.5428e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.9497e-02, -3.2710e-02, -5.1007e-02],\n",
       "           [-1.2096e-02,  4.4725e-02,  4.9262e-02],\n",
       "           [-4.5488e-02,  5.7023e-02, -1.3938e-02]],\n",
       " \n",
       "          [[ 2.6453e-02, -5.3338e-02,  4.9874e-03],\n",
       "           [ 4.9443e-02,  4.6337e-02,  4.6237e-02],\n",
       "           [ 1.9181e-02, -2.0657e-02, -4.5685e-02]],\n",
       " \n",
       "          [[-2.5192e-03,  2.8761e-02, -4.2599e-02],\n",
       "           [-4.8064e-03,  2.7911e-02, -3.7541e-02],\n",
       "           [-5.1003e-02,  1.0393e-02,  5.6107e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0096e-02,  8.2636e-03,  3.3643e-02],\n",
       "           [-3.5483e-02,  3.9823e-02, -5.1088e-02],\n",
       "           [-5.2329e-02, -7.8036e-03, -2.6994e-02]],\n",
       " \n",
       "          [[ 5.0138e-02, -1.2705e-03,  5.5152e-02],\n",
       "           [ 4.8575e-02, -2.8620e-02,  4.0724e-02],\n",
       "           [ 4.4372e-02, -5.5048e-02,  8.2719e-03]],\n",
       " \n",
       "          [[-3.6917e-03, -4.8237e-02,  1.4326e-02],\n",
       "           [ 5.8474e-02,  9.2294e-04,  5.3283e-02],\n",
       "           [ 3.6018e-02,  1.5803e-02, -4.4104e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.2120e-02, -2.1614e-02,  1.7018e-02],\n",
       "           [-4.2496e-02, -5.5788e-02,  2.9173e-02],\n",
       "           [-4.1194e-02,  3.2194e-03,  2.2686e-02]],\n",
       " \n",
       "          [[ 5.4017e-02,  1.5278e-02, -3.9786e-02],\n",
       "           [ 3.6710e-03, -4.5804e-02, -3.8354e-02],\n",
       "           [-2.4702e-02,  3.4668e-02, -4.7339e-02]],\n",
       " \n",
       "          [[ 5.3430e-02,  1.8570e-02, -3.7024e-02],\n",
       "           [-5.7286e-02,  2.4916e-02, -2.3010e-02],\n",
       "           [ 3.4885e-02, -2.9613e-02,  1.6202e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.0582e-02, -2.6968e-02, -2.8093e-02],\n",
       "           [-8.5723e-04, -1.6351e-02, -1.8424e-02],\n",
       "           [ 3.6358e-02, -2.9207e-02,  4.2901e-02]],\n",
       " \n",
       "          [[ 5.2379e-02,  2.7298e-02, -3.8665e-02],\n",
       "           [ 4.7064e-02, -1.9328e-02,  5.7603e-02],\n",
       "           [ 3.4986e-02,  3.6665e-02, -9.3917e-03]],\n",
       " \n",
       "          [[ 3.7847e-02,  5.5604e-02,  1.2676e-02],\n",
       "           [ 5.7904e-02,  4.6458e-02, -1.0816e-02],\n",
       "           [ 1.4825e-02,  3.0120e-02, -2.4013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.5582e-02,  7.9506e-03, -5.3923e-02],\n",
       "           [ 3.4684e-02,  3.2537e-03, -8.8748e-03],\n",
       "           [ 5.1414e-02,  2.1381e-02, -4.7578e-02]],\n",
       " \n",
       "          [[-1.9648e-02, -5.0517e-02, -1.2699e-02],\n",
       "           [ 2.0551e-02,  3.4886e-02, -1.7081e-02],\n",
       "           [ 4.5170e-02,  4.6150e-02,  2.6860e-02]],\n",
       " \n",
       "          [[-1.7783e-02,  3.7673e-02,  3.8889e-02],\n",
       "           [-4.2494e-02, -3.3087e-02,  4.2578e-02],\n",
       "           [-1.9450e-02, -4.6741e-02, -2.9636e-02]]]]),\n",
       " 'layer2.0.bias': tensor([-0.0029, -0.0226, -0.0220, -0.0451, -0.0162,  0.0195,  0.0535, -0.0280,\n",
       "         -0.0340,  0.0170,  0.0581,  0.0215, -0.0449,  0.0488,  0.0284,  0.0110,\n",
       "         -0.0304, -0.0164, -0.0357, -0.0168,  0.0295,  0.0206, -0.0200,  0.0095,\n",
       "         -0.0089,  0.0315,  0.0465,  0.0486, -0.0454,  0.0323,  0.0181, -0.0093]),\n",
       " 'layer2.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer2.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer3.0.weight': tensor([[[[ 0.0349,  0.0080,  0.0391],\n",
       "           [ 0.0168,  0.0509,  0.0433],\n",
       "           [ 0.0178,  0.0479, -0.0166]],\n",
       " \n",
       "          [[-0.0273, -0.0540, -0.0191],\n",
       "           [-0.0348, -0.0212, -0.0429],\n",
       "           [-0.0271, -0.0277,  0.0539]],\n",
       " \n",
       "          [[ 0.0317, -0.0411, -0.0299],\n",
       "           [ 0.0163, -0.0236,  0.0418],\n",
       "           [ 0.0094, -0.0482, -0.0238]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0558,  0.0193, -0.0203],\n",
       "           [-0.0455, -0.0138,  0.0564],\n",
       "           [ 0.0082, -0.0449, -0.0450]],\n",
       " \n",
       "          [[ 0.0185, -0.0095, -0.0317],\n",
       "           [ 0.0377,  0.0436, -0.0243],\n",
       "           [ 0.0119, -0.0278, -0.0025]],\n",
       " \n",
       "          [[-0.0028,  0.0118,  0.0340],\n",
       "           [-0.0475, -0.0382,  0.0570],\n",
       "           [ 0.0181,  0.0342, -0.0580]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0275, -0.0282,  0.0214],\n",
       "           [ 0.0168, -0.0525,  0.0339],\n",
       "           [ 0.0271, -0.0350,  0.0188]],\n",
       " \n",
       "          [[-0.0379,  0.0073,  0.0297],\n",
       "           [-0.0505,  0.0464, -0.0274],\n",
       "           [-0.0421,  0.0336,  0.0277]],\n",
       " \n",
       "          [[-0.0463,  0.0101, -0.0216],\n",
       "           [ 0.0380, -0.0448,  0.0118],\n",
       "           [-0.0039,  0.0029,  0.0205]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0137, -0.0485,  0.0479],\n",
       "           [ 0.0140,  0.0290,  0.0492],\n",
       "           [-0.0348,  0.0098, -0.0530]],\n",
       " \n",
       "          [[ 0.0022,  0.0261,  0.0222],\n",
       "           [-0.0144, -0.0012,  0.0539],\n",
       "           [-0.0323, -0.0358,  0.0455]],\n",
       " \n",
       "          [[-0.0019, -0.0225,  0.0147],\n",
       "           [-0.0320,  0.0233,  0.0163],\n",
       "           [ 0.0575,  0.0426, -0.0043]]],\n",
       " \n",
       " \n",
       "         [[[-0.0363, -0.0312,  0.0324],\n",
       "           [-0.0358,  0.0405, -0.0376],\n",
       "           [-0.0349,  0.0389, -0.0300]],\n",
       " \n",
       "          [[ 0.0290,  0.0511,  0.0255],\n",
       "           [ 0.0415,  0.0384, -0.0417],\n",
       "           [ 0.0170,  0.0173,  0.0497]],\n",
       " \n",
       "          [[-0.0512,  0.0508,  0.0123],\n",
       "           [ 0.0372, -0.0387, -0.0135],\n",
       "           [ 0.0373,  0.0515,  0.0059]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0423, -0.0471, -0.0543],\n",
       "           [-0.0494,  0.0340,  0.0193],\n",
       "           [ 0.0132, -0.0222, -0.0134]],\n",
       " \n",
       "          [[-0.0555,  0.0462, -0.0468],\n",
       "           [-0.0407,  0.0341, -0.0243],\n",
       "           [ 0.0366, -0.0470, -0.0287]],\n",
       " \n",
       "          [[ 0.0117,  0.0141, -0.0320],\n",
       "           [ 0.0347,  0.0127, -0.0439],\n",
       "           [-0.0045,  0.0420,  0.0280]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0276, -0.0193, -0.0327],\n",
       "           [ 0.0056, -0.0327, -0.0540],\n",
       "           [ 0.0408, -0.0589,  0.0439]],\n",
       " \n",
       "          [[ 0.0170,  0.0479,  0.0354],\n",
       "           [-0.0253,  0.0351,  0.0472],\n",
       "           [-0.0431,  0.0366,  0.0009]],\n",
       " \n",
       "          [[-0.0452,  0.0521, -0.0156],\n",
       "           [ 0.0237,  0.0153,  0.0115],\n",
       "           [ 0.0114,  0.0531, -0.0215]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0174, -0.0583, -0.0320],\n",
       "           [ 0.0349,  0.0220,  0.0481],\n",
       "           [-0.0178,  0.0080,  0.0153]],\n",
       " \n",
       "          [[ 0.0321,  0.0128,  0.0074],\n",
       "           [ 0.0360,  0.0581, -0.0427],\n",
       "           [ 0.0326,  0.0437, -0.0364]],\n",
       " \n",
       "          [[ 0.0378, -0.0583, -0.0260],\n",
       "           [-0.0287,  0.0527, -0.0389],\n",
       "           [ 0.0347, -0.0302, -0.0554]]],\n",
       " \n",
       " \n",
       "         [[[-0.0092, -0.0467, -0.0299],\n",
       "           [ 0.0316, -0.0589, -0.0055],\n",
       "           [ 0.0511, -0.0071, -0.0279]],\n",
       " \n",
       "          [[-0.0223,  0.0226, -0.0080],\n",
       "           [ 0.0587, -0.0140, -0.0529],\n",
       "           [-0.0343,  0.0180,  0.0249]],\n",
       " \n",
       "          [[-0.0064,  0.0322,  0.0150],\n",
       "           [-0.0121,  0.0542, -0.0135],\n",
       "           [-0.0501, -0.0066,  0.0361]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0131, -0.0132,  0.0006],\n",
       "           [-0.0170,  0.0108,  0.0513],\n",
       "           [-0.0477, -0.0503, -0.0247]],\n",
       " \n",
       "          [[ 0.0012,  0.0428,  0.0357],\n",
       "           [ 0.0526,  0.0207, -0.0433],\n",
       "           [ 0.0550,  0.0501,  0.0588]],\n",
       " \n",
       "          [[ 0.0333, -0.0109,  0.0188],\n",
       "           [-0.0043, -0.0551,  0.0034],\n",
       "           [ 0.0480,  0.0101, -0.0003]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0053,  0.0389,  0.0388],\n",
       "           [ 0.0026, -0.0537, -0.0568],\n",
       "           [-0.0342,  0.0285, -0.0239]],\n",
       " \n",
       "          [[ 0.0492,  0.0006,  0.0106],\n",
       "           [-0.0431, -0.0318,  0.0212],\n",
       "           [ 0.0052, -0.0454, -0.0238]],\n",
       " \n",
       "          [[ 0.0561,  0.0425, -0.0006],\n",
       "           [-0.0442,  0.0060, -0.0096],\n",
       "           [-0.0088, -0.0156,  0.0323]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0413, -0.0057, -0.0351],\n",
       "           [ 0.0050,  0.0508, -0.0194],\n",
       "           [-0.0276,  0.0149, -0.0189]],\n",
       " \n",
       "          [[-0.0123, -0.0535,  0.0464],\n",
       "           [-0.0081,  0.0378,  0.0531],\n",
       "           [-0.0053, -0.0570, -0.0249]],\n",
       " \n",
       "          [[ 0.0173, -0.0353,  0.0536],\n",
       "           [ 0.0449, -0.0072, -0.0557],\n",
       "           [-0.0291, -0.0258, -0.0001]]]]),\n",
       " 'layer3.0.bias': tensor([-0.0553,  0.0040,  0.0108, -0.0160,  0.0396,  0.0470,  0.0212,  0.0019,\n",
       "          0.0513, -0.0497,  0.0139, -0.0532,  0.0565, -0.0235,  0.0157,  0.0309,\n",
       "          0.0052,  0.0123, -0.0224, -0.0405,  0.0552,  0.0039,  0.0070, -0.0347,\n",
       "          0.0450,  0.0507, -0.0266,  0.0296,  0.0268,  0.0458,  0.0275, -0.0472,\n",
       "          0.0478, -0.0509,  0.0154, -0.0372,  0.0560, -0.0417, -0.0460,  0.0035,\n",
       "         -0.0077,  0.0466, -0.0062, -0.0025, -0.0086,  0.0558,  0.0206,  0.0366,\n",
       "          0.0067,  0.0200, -0.0214, -0.0477, -0.0559, -0.0190,  0.0371,  0.0582,\n",
       "          0.0312, -0.0585, -0.0531, -0.0584,  0.0278, -0.0551,  0.0183,  0.0443]),\n",
       " 'layer3.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer3.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer4.0.weight': tensor([[[[ 2.5620e-02,  1.1027e-02,  3.8344e-02],\n",
       "           [-5.6369e-04, -1.3636e-02, -1.1476e-02],\n",
       "           [ 2.4599e-02, -1.7833e-02,  1.0314e-02]],\n",
       " \n",
       "          [[-3.6600e-02,  1.6459e-02,  8.5596e-03],\n",
       "           [-2.3940e-02,  3.1190e-02,  1.1611e-03],\n",
       "           [-2.8263e-02, -2.6966e-02, -1.9842e-02]],\n",
       " \n",
       "          [[-2.2325e-02,  1.7213e-02,  1.4972e-02],\n",
       "           [ 1.2437e-03, -1.6358e-02,  2.6994e-02],\n",
       "           [ 6.8585e-03,  4.8376e-04,  2.7526e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.0617e-02, -2.6364e-02,  3.6131e-02],\n",
       "           [ 2.8148e-02,  1.4054e-02, -2.4607e-02],\n",
       "           [-6.4064e-03,  3.6277e-02,  6.3002e-03]],\n",
       " \n",
       "          [[-2.6183e-02, -1.6960e-02, -3.2769e-02],\n",
       "           [ 3.3250e-02, -1.0777e-02,  3.2396e-02],\n",
       "           [-8.2837e-03,  3.9580e-02,  2.3819e-02]],\n",
       " \n",
       "          [[ 1.0965e-02, -3.6820e-02, -1.8887e-02],\n",
       "           [ 5.6978e-03, -3.5669e-02, -1.5734e-02],\n",
       "           [-1.7920e-02,  1.7379e-02, -3.0323e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.7890e-02,  1.4734e-02,  1.2198e-02],\n",
       "           [-2.2439e-02,  4.1251e-02, -2.5036e-02],\n",
       "           [ 5.7460e-03, -1.1964e-02, -2.5474e-02]],\n",
       " \n",
       "          [[-2.0505e-02,  1.5931e-02,  1.7208e-02],\n",
       "           [-2.7738e-02, -3.5588e-02, -8.7796e-03],\n",
       "           [ 2.6947e-03,  2.5921e-03, -1.3987e-02]],\n",
       " \n",
       "          [[-7.3796e-03, -1.7497e-02,  3.8538e-03],\n",
       "           [ 2.2572e-02, -1.0772e-02,  5.4893e-03],\n",
       "           [ 4.1398e-02, -2.8769e-02, -1.4876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5627e-02, -2.2806e-03,  1.0660e-02],\n",
       "           [ 3.1863e-03, -3.0281e-02,  3.9491e-02],\n",
       "           [-2.6706e-02, -4.1496e-02, -3.4536e-02]],\n",
       " \n",
       "          [[ 1.2076e-02, -2.2478e-03,  2.5527e-02],\n",
       "           [-5.1937e-03,  7.0800e-04,  2.4908e-02],\n",
       "           [ 3.5183e-02,  2.8290e-03, -1.1281e-02]],\n",
       " \n",
       "          [[ 3.3812e-02,  2.9348e-02,  2.4271e-02],\n",
       "           [ 4.3343e-03,  1.9623e-02,  2.9991e-02],\n",
       "           [ 3.7585e-02,  6.1731e-03, -1.6640e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5880e-02, -6.0504e-03,  2.0034e-03],\n",
       "           [-3.7352e-02, -7.3350e-03,  2.5647e-02],\n",
       "           [ 2.4312e-02, -8.3213e-03,  3.5337e-03]],\n",
       " \n",
       "          [[-3.0441e-02,  6.6977e-03, -1.9300e-02],\n",
       "           [ 3.0446e-02, -7.7699e-03, -5.5637e-03],\n",
       "           [ 1.3507e-02,  2.9579e-02,  3.1732e-02]],\n",
       " \n",
       "          [[-1.2611e-02,  2.5706e-02,  6.1131e-03],\n",
       "           [ 6.8956e-03, -1.2875e-02,  1.7288e-02],\n",
       "           [ 6.3988e-03,  1.7031e-02, -1.8602e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.2298e-03,  3.1480e-02, -2.4839e-02],\n",
       "           [-9.5341e-05, -3.4622e-02,  3.4004e-02],\n",
       "           [-1.9048e-02,  1.2274e-02, -3.6575e-02]],\n",
       " \n",
       "          [[ 2.3614e-02, -1.8650e-02, -2.2995e-02],\n",
       "           [ 1.8063e-02,  3.1919e-02, -3.8414e-02],\n",
       "           [ 3.2621e-02, -2.6842e-02,  2.1469e-02]],\n",
       " \n",
       "          [[ 2.2439e-02,  2.9607e-02, -3.4393e-03],\n",
       "           [ 1.2774e-02, -1.5567e-02,  2.1473e-02],\n",
       "           [-3.5595e-02, -1.9984e-02,  1.7679e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-4.8253e-03, -3.3776e-02,  4.0900e-02],\n",
       "           [ 2.1382e-02,  2.7088e-02,  4.6202e-04],\n",
       "           [-3.8172e-04,  1.4354e-02, -2.0458e-02]],\n",
       " \n",
       "          [[ 2.8361e-02, -3.9402e-02,  2.3592e-02],\n",
       "           [-1.6914e-02,  3.1797e-02,  1.0471e-02],\n",
       "           [-3.2137e-02,  2.4701e-03, -2.4310e-02]],\n",
       " \n",
       "          [[ 3.4360e-02,  9.9863e-03,  3.2582e-02],\n",
       "           [-3.5713e-02,  2.1561e-02,  2.0948e-02],\n",
       "           [ 2.8734e-02, -2.4535e-02, -1.9657e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.8352e-02, -3.0637e-02, -4.1497e-02],\n",
       "           [-2.1351e-02,  1.9204e-02,  6.9097e-03],\n",
       "           [ 9.2071e-03, -2.6410e-02, -2.4193e-03]],\n",
       " \n",
       "          [[ 3.6037e-02,  3.7034e-02,  2.1712e-02],\n",
       "           [-1.5547e-02,  4.0763e-02, -2.4413e-02],\n",
       "           [-3.5063e-02, -6.3464e-03,  1.9633e-02]],\n",
       " \n",
       "          [[-1.1322e-02, -1.5831e-02, -2.7898e-02],\n",
       "           [-1.1176e-02, -4.0802e-02, -1.6075e-03],\n",
       "           [ 7.4114e-03, -1.5146e-02, -2.5475e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.6118e-02,  2.1552e-02, -3.4662e-02],\n",
       "           [-2.2085e-03,  1.5897e-02,  3.3573e-02],\n",
       "           [ 3.4382e-02,  2.0942e-02, -6.2279e-03]],\n",
       " \n",
       "          [[ 2.4146e-02, -3.4124e-02, -3.2239e-02],\n",
       "           [ 2.6915e-02,  1.0536e-02, -3.2622e-02],\n",
       "           [ 2.0656e-02, -1.9801e-02,  4.4186e-03]],\n",
       " \n",
       "          [[-1.3196e-02, -6.5854e-04,  3.5568e-02],\n",
       "           [-4.1158e-02,  1.5582e-02, -3.1423e-02],\n",
       "           [-9.3038e-04,  4.6161e-03, -1.9687e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6987e-03,  7.5396e-03, -7.5504e-03],\n",
       "           [ 9.4232e-03,  2.9016e-02,  2.9479e-02],\n",
       "           [-3.7800e-02,  7.1434e-03, -2.1037e-02]],\n",
       " \n",
       "          [[-1.7767e-02,  7.6964e-03,  6.1694e-03],\n",
       "           [ 2.9679e-02,  1.3413e-02, -7.4646e-04],\n",
       "           [-1.9178e-02, -9.6990e-03, -3.2808e-02]],\n",
       " \n",
       "          [[-3.6097e-02,  2.4256e-02,  3.8199e-02],\n",
       "           [-2.4204e-02, -3.6429e-02, -2.7111e-02],\n",
       "           [ 2.0422e-02,  3.4729e-03,  3.5056e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.2510e-02, -1.9212e-02,  2.3803e-02],\n",
       "           [ 1.3745e-02, -2.2359e-02,  2.6260e-02],\n",
       "           [-4.0578e-02,  3.3985e-02,  2.1799e-02]],\n",
       " \n",
       "          [[-6.0673e-03,  3.1644e-02,  3.4686e-03],\n",
       "           [ 3.6875e-02, -4.1006e-02, -6.1496e-03],\n",
       "           [ 7.4729e-03,  1.8152e-03, -1.0912e-02]],\n",
       " \n",
       "          [[-2.6291e-02, -1.4366e-03,  2.9217e-03],\n",
       "           [ 1.3097e-02,  3.0829e-02, -3.4937e-02],\n",
       "           [-3.4530e-02, -3.6971e-02,  1.4080e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.6622e-03,  2.5039e-02,  2.8354e-02],\n",
       "           [ 2.3704e-02,  8.1194e-03,  1.8403e-02],\n",
       "           [ 3.3893e-02, -2.9643e-02, -2.5597e-02]],\n",
       " \n",
       "          [[-5.5600e-03,  3.8782e-02,  1.3900e-03],\n",
       "           [-1.6761e-02,  9.2933e-03,  9.7108e-03],\n",
       "           [-2.1357e-03,  3.8266e-02,  3.8584e-02]],\n",
       " \n",
       "          [[ 1.8071e-02,  7.9090e-03,  1.1067e-02],\n",
       "           [ 1.0860e-02,  9.1985e-03,  9.1101e-03],\n",
       "           [ 5.3729e-03, -2.3801e-02, -3.0802e-02]]]]),\n",
       " 'layer4.0.bias': tensor([ 0.0177, -0.0163, -0.0093,  0.0162, -0.0377,  0.0078,  0.0001,  0.0027,\n",
       "         -0.0377,  0.0106, -0.0282,  0.0339, -0.0121, -0.0010, -0.0151, -0.0266,\n",
       "          0.0306,  0.0092, -0.0270,  0.0387,  0.0071,  0.0043,  0.0337, -0.0290,\n",
       "          0.0054, -0.0309, -0.0099,  0.0037,  0.0143,  0.0311, -0.0226, -0.0143,\n",
       "         -0.0285, -0.0150,  0.0323,  0.0416,  0.0100, -0.0162, -0.0373,  0.0130,\n",
       "         -0.0095,  0.0198,  0.0192, -0.0277,  0.0032,  0.0406,  0.0073, -0.0256,\n",
       "         -0.0246,  0.0203,  0.0312, -0.0264, -0.0383, -0.0415, -0.0085, -0.0052,\n",
       "         -0.0147,  0.0275,  0.0078,  0.0238,  0.0250,  0.0253, -0.0249,  0.0367]),\n",
       " 'layer4.1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'layer4.1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'layer5.0.weight': tensor([[[[ 4.0029e-02, -1.5555e-02, -1.9442e-02],\n",
       "           [ 2.7807e-02,  1.2606e-03,  2.2996e-02],\n",
       "           [ 1.6510e-02, -1.8767e-02,  1.4049e-02]],\n",
       " \n",
       "          [[-2.0909e-02,  3.2355e-03,  4.8950e-04],\n",
       "           [ 3.5541e-02, -2.6028e-02, -2.1366e-02],\n",
       "           [-1.8651e-02, -1.4314e-02, -1.5306e-04]],\n",
       " \n",
       "          [[-2.3171e-02,  1.9146e-02,  1.5210e-03],\n",
       "           [ 1.9624e-02, -1.5651e-02, -2.6014e-02],\n",
       "           [-3.2673e-02,  1.8227e-02,  3.6811e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4583e-02, -7.9460e-06, -1.4085e-02],\n",
       "           [ 2.8421e-02, -1.1669e-02,  1.9115e-02],\n",
       "           [ 3.0063e-02,  1.8295e-02, -2.4148e-02]],\n",
       " \n",
       "          [[-3.0112e-02,  3.7937e-02,  3.5968e-02],\n",
       "           [ 1.3086e-02, -1.0907e-02,  2.0002e-02],\n",
       "           [-5.9486e-03,  6.4267e-03, -1.3565e-02]],\n",
       " \n",
       "          [[ 3.3119e-02,  2.9293e-02,  4.7366e-03],\n",
       "           [ 1.3828e-02, -8.6379e-04, -3.8102e-02],\n",
       "           [ 2.7153e-02,  3.1091e-02,  1.0126e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.4183e-03, -1.7757e-03, -2.0380e-02],\n",
       "           [-1.5854e-02, -2.6645e-02,  7.1543e-03],\n",
       "           [ 6.6951e-03, -1.8744e-02, -1.2158e-02]],\n",
       " \n",
       "          [[ 1.2498e-02,  2.4377e-02,  2.7904e-02],\n",
       "           [ 3.8156e-02,  3.0734e-02,  8.7722e-03],\n",
       "           [ 2.6730e-02, -3.1139e-02,  3.4744e-02]],\n",
       " \n",
       "          [[ 2.6575e-02,  2.4073e-02, -3.0729e-02],\n",
       "           [ 2.7972e-02, -3.6623e-02,  2.2270e-02],\n",
       "           [-2.7218e-02,  3.9759e-02, -2.6646e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.4700e-03,  2.2825e-03,  3.6883e-02],\n",
       "           [ 2.7217e-02, -3.1232e-02,  3.1608e-02],\n",
       "           [-1.0017e-02,  3.1206e-02, -3.9591e-02]],\n",
       " \n",
       "          [[-2.9666e-02, -4.0470e-02, -6.2156e-03],\n",
       "           [ 3.0331e-02, -3.6313e-04, -1.9975e-02],\n",
       "           [-3.7550e-02,  2.9411e-02, -3.2907e-02]],\n",
       " \n",
       "          [[ 2.4071e-03,  2.2413e-02,  2.5404e-02],\n",
       "           [ 1.3434e-02, -3.0588e-02,  9.5532e-04],\n",
       "           [-3.2427e-02,  1.6529e-02, -1.6419e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.3955e-02,  7.1121e-03,  2.6891e-02],\n",
       "           [ 4.0416e-02, -3.3873e-02, -1.3531e-02],\n",
       "           [ 1.8637e-02,  5.2441e-05,  2.1841e-02]],\n",
       " \n",
       "          [[-1.7873e-02,  3.0563e-02,  1.1849e-02],\n",
       "           [ 9.3570e-03, -1.1101e-02,  8.0893e-03],\n",
       "           [-8.1875e-03, -1.7321e-02, -9.3326e-05]],\n",
       " \n",
       "          [[-2.1851e-02,  4.0022e-02,  8.1340e-03],\n",
       "           [ 1.9282e-02, -2.6939e-02,  1.7700e-02],\n",
       "           [ 7.5512e-03,  3.9676e-02, -3.6485e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.8432e-03,  1.4899e-02,  2.2023e-02],\n",
       "           [-5.1086e-03, -4.1410e-03, -1.7824e-02],\n",
       "           [ 3.6685e-02, -3.0121e-02, -2.4863e-02]],\n",
       " \n",
       "          [[ 7.5307e-03,  2.6082e-02,  2.2635e-02],\n",
       "           [-3.3264e-02, -1.2277e-02, -1.2866e-02],\n",
       "           [ 3.9813e-02, -1.0466e-02, -7.6253e-03]],\n",
       " \n",
       "          [[-3.7760e-02,  7.5000e-03, -5.7763e-03],\n",
       "           [-3.5322e-02,  3.6979e-02,  2.8847e-03],\n",
       "           [ 6.5473e-03,  3.5336e-02, -5.7323e-03]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 3.4297e-02, -3.6211e-02,  2.7521e-02],\n",
       "           [-9.7694e-03,  1.5891e-02,  1.4592e-02],\n",
       "           [-3.0556e-02,  9.3818e-03,  4.0286e-02]],\n",
       " \n",
       "          [[-2.7050e-02, -1.7057e-02,  2.0408e-02],\n",
       "           [ 2.3277e-02,  3.4720e-02, -2.5510e-02],\n",
       "           [ 3.2969e-02, -3.7801e-02, -2.1019e-02]],\n",
       " \n",
       "          [[ 3.2294e-02, -5.6080e-03,  3.2732e-02],\n",
       "           [ 8.6077e-03,  2.3426e-02, -2.1425e-02],\n",
       "           [-2.0202e-03,  2.7345e-02,  3.4072e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.1652e-02,  3.0896e-02, -2.1383e-02],\n",
       "           [ 3.9330e-02, -3.5811e-02,  1.0374e-02],\n",
       "           [ 1.3004e-02,  1.1354e-02, -3.0207e-02]],\n",
       " \n",
       "          [[-1.4926e-02, -2.4606e-02, -2.8183e-02],\n",
       "           [ 2.0890e-03,  3.1961e-02, -2.7149e-02],\n",
       "           [ 3.3376e-02, -4.2753e-03,  1.3732e-02]],\n",
       " \n",
       "          [[-3.1865e-02,  2.2441e-02, -2.7422e-02],\n",
       "           [ 3.6593e-02, -1.8769e-02, -1.5301e-02],\n",
       "           [ 1.6204e-02,  1.2891e-02, -2.4532e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.5621e-02, -7.0786e-03, -1.0761e-02],\n",
       "           [-5.0155e-03, -3.1408e-02,  3.0269e-02],\n",
       "           [ 3.1515e-02, -2.1544e-02,  2.6874e-04]],\n",
       " \n",
       "          [[-4.1376e-02, -6.8147e-04, -2.9623e-02],\n",
       "           [-2.8259e-02,  3.9103e-03, -2.5514e-02],\n",
       "           [ 4.5687e-03, -1.8919e-02, -7.3692e-03]],\n",
       " \n",
       "          [[-2.4708e-02, -2.0455e-02, -1.3034e-02],\n",
       "           [ 2.2828e-02,  4.3709e-04,  3.2861e-02],\n",
       "           [ 1.2342e-02,  3.5667e-02, -2.3626e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.7968e-02,  2.0814e-02, -1.4589e-02],\n",
       "           [ 3.5259e-02,  3.3609e-02,  1.2301e-02],\n",
       "           [ 7.3563e-03, -1.7608e-02,  2.7465e-02]],\n",
       " \n",
       "          [[ 6.1034e-03, -1.7165e-02, -6.9117e-03],\n",
       "           [ 2.9456e-02,  3.4874e-02,  1.7228e-02],\n",
       "           [ 3.4576e-02,  1.7019e-02, -2.9988e-02]],\n",
       " \n",
       "          [[-3.0170e-02, -2.8234e-02,  3.7775e-02],\n",
       "           [ 3.1383e-02,  3.2691e-02,  3.0495e-02],\n",
       "           [ 3.4685e-03, -1.7589e-02,  2.4322e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7058e-04, -2.9093e-02, -4.0108e-02],\n",
       "           [ 3.2973e-02,  1.0041e-02,  2.1036e-02],\n",
       "           [-2.9385e-02, -6.8148e-03,  3.9885e-02]],\n",
       " \n",
       "          [[ 2.3979e-03, -3.9971e-02, -1.7004e-02],\n",
       "           [-3.3634e-02,  1.1719e-02, -2.8307e-02],\n",
       "           [-3.2483e-02, -1.1167e-02,  1.5263e-02]],\n",
       " \n",
       "          [[ 6.7131e-03,  3.7708e-02,  3.4297e-02],\n",
       "           [ 2.8094e-02,  3.8445e-02,  3.6810e-05],\n",
       "           [ 8.6228e-03,  1.4023e-02,  2.4469e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.1655e-02,  3.6069e-02, -1.6624e-02],\n",
       "           [ 2.2531e-02,  3.7367e-02, -2.7337e-02],\n",
       "           [-5.4987e-03, -3.2980e-03, -2.1241e-02]],\n",
       " \n",
       "          [[-2.5833e-03,  4.0529e-02, -2.8201e-02],\n",
       "           [ 7.9990e-03, -1.9770e-02, -9.0786e-03],\n",
       "           [ 1.9084e-02,  1.7834e-02,  1.1820e-02]],\n",
       " \n",
       "          [[-2.1395e-02, -1.0014e-02,  3.2037e-02],\n",
       "           [ 7.6473e-03,  2.1263e-02,  2.7528e-02],\n",
       "           [ 6.0682e-03,  1.0778e-02,  3.1273e-02]]]]),\n",
       " 'layer5.0.bias': tensor([-0.0339, -0.0223, -0.0282,  0.0156, -0.0012,  0.0039, -0.0381, -0.0168,\n",
       "         -0.0001,  0.0406, -0.0392, -0.0376,  0.0081, -0.0122,  0.0069, -0.0187,\n",
       "         -0.0228,  0.0056,  0.0213,  0.0152, -0.0076,  0.0395,  0.0266,  0.0247,\n",
       "         -0.0403, -0.0306,  0.0379, -0.0167, -0.0330, -0.0408, -0.0215,  0.0353,\n",
       "         -0.0026, -0.0162, -0.0388, -0.0065, -0.0216,  0.0155, -0.0153, -0.0108,\n",
       "         -0.0357, -0.0315, -0.0267, -0.0286, -0.0062, -0.0391,  0.0140, -0.0273,\n",
       "          0.0051,  0.0157, -0.0315, -0.0401,  0.0289, -0.0257, -0.0148,  0.0070,\n",
       "          0.0054,  0.0089, -0.0287, -0.0140, -0.0269,  0.0319, -0.0325, -0.0247]),\n",
       " 'linear.weight': tensor([[ 0.0916,  0.0274,  0.0482, -0.0863,  0.0164, -0.0115, -0.0446,  0.1144,\n",
       "           0.0822,  0.0646,  0.0713,  0.0645,  0.0248,  0.0175,  0.0264,  0.0958,\n",
       "           0.1223,  0.0784, -0.0136,  0.1037,  0.0573,  0.0770, -0.0095, -0.0206,\n",
       "           0.1065,  0.0468,  0.0714,  0.1024,  0.0972, -0.0708, -0.0112,  0.0418,\n",
       "          -0.0384, -0.1069, -0.0260, -0.1035,  0.0921, -0.0480, -0.0806, -0.0412,\n",
       "           0.1044, -0.1037,  0.0918,  0.1132, -0.0616,  0.0739,  0.0543,  0.0419,\n",
       "          -0.1071,  0.0925, -0.0029, -0.0504, -0.1013, -0.1048, -0.1139,  0.0840,\n",
       "           0.0123, -0.0573,  0.0097,  0.1013,  0.0002,  0.0757, -0.1092, -0.0977],\n",
       "         [-0.0976,  0.0218,  0.1236, -0.0156, -0.0988, -0.0866,  0.0803,  0.0230,\n",
       "          -0.0454, -0.0290,  0.1180, -0.1043, -0.1040, -0.0692, -0.0319,  0.0671,\n",
       "           0.0057,  0.0187,  0.1229,  0.0675,  0.0289, -0.0853,  0.0172,  0.0875,\n",
       "          -0.0131, -0.0520, -0.0023, -0.0837, -0.1215,  0.0070,  0.0172, -0.0861,\n",
       "           0.0510,  0.1128,  0.0698, -0.0959,  0.1047, -0.0330,  0.0005,  0.0720,\n",
       "           0.0032, -0.0150, -0.0510, -0.0521,  0.1005,  0.0592, -0.1216, -0.1142,\n",
       "           0.1248,  0.0489, -0.0867, -0.0501, -0.0520,  0.0541, -0.0905,  0.0906,\n",
       "           0.0294,  0.0876,  0.0433, -0.0863,  0.0969, -0.0604, -0.1024, -0.0643],\n",
       "         [ 0.1154, -0.1186, -0.0199, -0.0405, -0.1105,  0.0765,  0.0550, -0.0141,\n",
       "          -0.1237, -0.0296, -0.0304,  0.1103, -0.1205,  0.0008,  0.0556,  0.0408,\n",
       "           0.0326, -0.0969,  0.1037, -0.0864, -0.0293, -0.0055, -0.0150,  0.0192,\n",
       "          -0.0729, -0.0636,  0.0296, -0.0577, -0.0287, -0.0820, -0.0236,  0.0443,\n",
       "          -0.0256, -0.0003, -0.0927, -0.0993,  0.0258,  0.1160, -0.0434, -0.0916,\n",
       "          -0.0447, -0.0580,  0.0066,  0.0233,  0.1130, -0.1072, -0.0153, -0.0757,\n",
       "          -0.1002, -0.0961,  0.0014, -0.0149, -0.1029,  0.1006, -0.0517,  0.0877,\n",
       "          -0.0534, -0.0673,  0.0225,  0.0355,  0.1090, -0.1145,  0.1039, -0.0724],\n",
       "         [ 0.0105, -0.0863,  0.0174,  0.0673, -0.0783, -0.0996,  0.0594, -0.0393,\n",
       "          -0.0898,  0.0047, -0.1083,  0.0007,  0.0800, -0.1056,  0.0060, -0.1029,\n",
       "           0.0600,  0.0489, -0.0016,  0.0796,  0.0368, -0.0254, -0.1187, -0.0487,\n",
       "          -0.0344, -0.0922, -0.0645, -0.0865,  0.0391,  0.0313,  0.0412,  0.1113,\n",
       "          -0.1223, -0.0501, -0.0932, -0.1201,  0.0443,  0.0366, -0.0723,  0.0465,\n",
       "           0.0954,  0.0633,  0.0466, -0.0724, -0.0795, -0.0819, -0.1092,  0.0291,\n",
       "           0.0500,  0.0143, -0.0998, -0.0054,  0.1042,  0.0176, -0.0459,  0.0192,\n",
       "           0.0816, -0.0294, -0.0316, -0.0542,  0.0943, -0.0240,  0.0372, -0.0403],\n",
       "         [-0.0505, -0.0937, -0.1059,  0.0898,  0.1196, -0.1027,  0.0251,  0.0276,\n",
       "          -0.0344,  0.0518, -0.0177,  0.1089,  0.0241,  0.0386, -0.0508,  0.0748,\n",
       "          -0.0992,  0.0156,  0.0612, -0.1040, -0.0413,  0.0628,  0.0228, -0.1098,\n",
       "           0.1152,  0.0852, -0.0362,  0.0163,  0.0398,  0.0036,  0.1198,  0.0906,\n",
       "           0.0250,  0.1074,  0.0016, -0.1161, -0.1173, -0.0486, -0.0867,  0.1088,\n",
       "          -0.0890,  0.0498,  0.0614, -0.1214, -0.0523,  0.0367, -0.0253,  0.0643,\n",
       "          -0.0216,  0.0619,  0.0555,  0.1103, -0.0463, -0.1167, -0.0215, -0.0346,\n",
       "          -0.0006, -0.0089, -0.0418,  0.0205,  0.0769, -0.0936,  0.0747, -0.0514],\n",
       "         [-0.1041, -0.0652, -0.1099, -0.1172, -0.1183,  0.0261, -0.0061, -0.0181,\n",
       "           0.0306,  0.1034,  0.1023, -0.0415, -0.0590,  0.1013, -0.0350, -0.1135,\n",
       "           0.1211,  0.0423,  0.0534, -0.0943,  0.0192, -0.1066,  0.0101, -0.0907,\n",
       "          -0.0641, -0.0925,  0.0244,  0.0263,  0.0090, -0.0468,  0.0813,  0.0006,\n",
       "           0.0742,  0.0435,  0.0780, -0.0913,  0.0158, -0.1105, -0.0535, -0.0402,\n",
       "          -0.0937,  0.0195,  0.0634, -0.0159,  0.1228, -0.0416,  0.0739,  0.0429,\n",
       "           0.0029, -0.0882, -0.0594, -0.0101,  0.0290, -0.0891,  0.0971, -0.0061,\n",
       "          -0.0823, -0.0365,  0.0256, -0.1039,  0.1206,  0.1249,  0.1118, -0.0718]]),\n",
       " 'linear.bias': tensor([ 0.0939, -0.0072, -0.0662,  0.0920, -0.0422,  0.0693])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cda6ca-2600-4980-8650-70de73a18e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c19ccbf5-7109-4406-9df0-37e580288c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of CNN(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f16f7b09-78c9-4093-8968-54580535e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd ={\n",
    "            name: torch.zeros_like(param, requires_grad=False)\n",
    "            for name, param in network.named_parameters()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dcb3064-f071-4872-998e-81745c82fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer0.0.weight', 'layer0.0.bias', 'layer0.1.weight', 'layer0.1.bias', 'layer1.0.weight', 'layer1.0.bias', 'layer1.1.weight', 'layer1.1.bias', 'layer2.0.weight', 'layer2.0.bias', 'layer2.1.weight', 'layer2.1.bias', 'layer3.0.weight', 'layer3.0.bias', 'layer3.1.weight', 'layer3.1.bias', 'layer4.0.weight', 'layer4.0.bias', 'layer4.1.weight', 'layer4.1.bias', 'layer5.0.weight', 'layer5.0.bias', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b55f5187-ea9e-42dc-9dad-f6158273d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of CNN(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bcd1231-7d65-4664-8598-98d973b58146",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1: 23, 3:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b7abd70-7835-4476-b472-475420760140",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {1:32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab87d681-d311-418b-8457-617e194c5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8f4a1de-1b43-4c4a-bb49-7ef2441d554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 32, 3: 4}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "764db03d-a1cb-4989-92ba-732f0b0ba625",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.state_dict().update(current_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1b690-356c-4de3-ad27-dc5133145037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b5e1a-337f-4e7b-9b22-75cd64b3b505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcbaec38-b73f-48fb-bf46-e0ca16b6ac9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m----> 2\u001b[0m net\u001b[38;5;241m.\u001b[39mupdate(\u001b[43msampled_param\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mload_state_dict(net)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_batchnorm()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampled_param' is not defined"
     ]
    }
   ],
   "source": [
    "net = network.state_dict()\n",
    "net.update(sampled_param)\n",
    "self.network.load_state_dict(net)\n",
    "self._update_batchnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e228735-b6a0-4c4e-80a5-fa50da0d2160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41eaa50-161f-4e0e-bdd7-236b2e7681b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf8f4c-a757-4703-83f7-f2f7b06512a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31625156-5d19-4f81-82ac-a51978657b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5206ff-c719-4b9a-af5e-bda07c04ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718be63c-25eb-4ba2-9e6c-615c9c456180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6644e0b9-6a79-4a70-825d-383b00b9d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceMode(enum.Enum):\n",
    "    \"\"\"\n",
    "    Inference mode switch for your implementation.\n",
    "    `MAP` simply predicts the most likely class using pretrained MAP weights.\n",
    "    `SWAG_DIAGONAL` and `SWAG_FULL` correspond to SWAG-diagonal and the full SWAG method, respectively.\n",
    "    \"\"\"\n",
    "    MAP = 0\n",
    "    SWAG_DIAGONAL = 1\n",
    "    SWAG_FULL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6892e47-141e-4f6e-909a-789dc2600856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf53d3-9bc8-44a3-acbd-fa1565df6e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7629ca91-fe5d-4879-8e45-3d9eff304a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAGInference(object):\n",
    "    \"\"\"\n",
    "    Your implementation of SWA-Gaussian.\n",
    "    This class is used to run and evaluate your solution.\n",
    "    You must preserve all methods and signatures of this class.\n",
    "    However, you can add new methods if you want.\n",
    "\n",
    "    We provide basic functionality and some helper methods.\n",
    "    You can pass all baselines by only modifying methods marked with TODO.\n",
    "    However, we encourage you to skim other methods in order to gain a better understanding of SWAG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_xs: torch.Tensor,\n",
    "        model_dir: pathlib.Path,\n",
    "        # TODO(1): change inference_mode to InferenceMode.SWAG_DIAGONAL\n",
    "        # TODO(2): change inference_mode to InferenceMode.SWAG_FULL\n",
    "        inference_mode: InferenceMode = InferenceMode.SWAG_DIAGONAL, # InferenceMode.MAP,\n",
    "        # TODO(2): optionally add/tweak hyperparameters\n",
    "        swag_epochs: int = 30,\n",
    "        swag_learning_rate: float = 0.045,\n",
    "        swag_update_freq: int = 1,\n",
    "        deviation_matrix_max_rank: int = 15,\n",
    "        bma_samples: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param train_xs: Training images (for storage only)\n",
    "        :param model_dir: Path to directory containing pretrained MAP weights\n",
    "        :param inference_mode: Control which inference mode (MAP, SWAG-diagonal, full SWAG) to use\n",
    "        :param swag_epochs: Total number of gradient descent epochs for SWAG\n",
    "        :param swag_learning_rate: Learning rate for SWAG gradient descent\n",
    "        :param swag_update_freq: Frequency (in epochs) for updating SWAG statistics during gradient descent\n",
    "        :param deviation_matrix_max_rank: Rank of deviation matrix for full SWAG\n",
    "        :param bma_samples: Number of networks to sample for Bayesian model averaging during prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.inference_mode = inference_mode\n",
    "        self.swag_epochs = swag_epochs\n",
    "        self.swag_learning_rate = swag_learning_rate\n",
    "        self.swag_update_freq = swag_update_freq\n",
    "        self.deviation_matrix_max_rank = deviation_matrix_max_rank\n",
    "        self.bma_samples = bma_samples\n",
    "\n",
    "        # Network used to perform SWAG.\n",
    "        # Note that all operations in this class modify this network IN-PLACE!\n",
    "        self.network = CNN(in_channels=3, out_classes=6)\n",
    "\n",
    "        # Store training dataset to recalculate batch normalization statistics during SWAG inference\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(train_xs)\n",
    "\n",
    "        # SWAG-diagonal\n",
    "        # TODO(1): create attributes for SWAG-diagonal\n",
    "        #  Hint: self._create_weight_copy() creates an all-zero copy of the weights\n",
    "        #  as a dictionary that maps from weight name to values.\n",
    "        #  Hint: you never need to consider the full vector of weights,\n",
    "        #  but can always act on per-layer weights (in the format that _create_weight_copy() returns)\n",
    "        self.w_swa = self._create_weight_copy()\n",
    "        self.w2_swa = self._create_weight_copy()\n",
    "        self.n = 0\n",
    "\n",
    "        # Full SWAG\n",
    "        # TODO(2): create attributes for SWAG-diagonal\n",
    "        #  Hint: check collections.deque\n",
    "\n",
    "        # Calibration, prediction, and other attributes\n",
    "        # TODO(2): create additional attributes, e.g., for calibration\n",
    "        self._prediction_threshold = None  # this is an example, feel free to be creative\n",
    "\n",
    "    def update_swag(self) -> None:\n",
    "        \"\"\"\n",
    "        Update SWAG statistics with the current weights of self.network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a copy of the current network weights\n",
    "        current_params = {name: param.detach() for name, param in self.network.named_parameters()}\n",
    "\n",
    "        # SWAG-diagonal\n",
    "        for name, param in current_params.items():\n",
    "            # TODO(1): update SWAG-diagonal attributes for weight `name` using `current_params` and `param`\n",
    "            self.w_swa[name] = (self.n * self.w_swa[name] + param)/(self.n + 1)\n",
    "            self.w2_swa[name] = (self.n * self.w2_swa[name] + param*param)/(self.n + 1)\n",
    "            # raise NotImplementedError(\"Update SWAG-diagonal statistics\")\n",
    "\n",
    "        # Full SWAG\n",
    "        if self.inference_mode == InferenceMode.SWAG_FULL:\n",
    "            # TODO(2): update full SWAG attributes for weight `name` using `current_params` and `param`\n",
    "            raise NotImplementedError(\"Update full SWAG statistics\")\n",
    "\n",
    "    def fit_swag(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Fit SWAG on top of the pretrained network self.network.\n",
    "        This method should perform gradient descent with occasional SWAG updates\n",
    "        by calling self.update_swag().\n",
    "        \"\"\"\n",
    "\n",
    "        # We use SGD with momentum and weight decay to perform SWA.\n",
    "        # See the paper on how weight decay corresponds to a type of prior.\n",
    "        # Feel free to play around with optimization hyperparameters.\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=self.swag_learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        # TODO(2): Update SWAGScheduler instantiation if you decided to implement a custom schedule.\n",
    "        #  By default, this scheduler just keeps the initial learning rate given to `optimizer`.\n",
    "        lr_scheduler = SWAGScheduler(\n",
    "            optimizer,\n",
    "            epochs=self.swag_epochs,\n",
    "            steps_per_epoch=len(loader),\n",
    "        )\n",
    "\n",
    "        # TODO(1): Perform initialization for SWAG fitting\n",
    "        self.update_swag()\n",
    "        # raise NotImplementedError(\"Initialize SWAG fitting\")\n",
    "\n",
    "        self.network.train()\n",
    "        with tqdm.trange(self.swag_epochs, desc=\"Running gradient descent for SWA\") as pbar:\n",
    "            pbar_dict = {}\n",
    "            for epoch in pbar:\n",
    "                average_loss = 0.0\n",
    "                average_accuracy = 0.0\n",
    "                num_samples_processed = 0\n",
    "                for batch_xs, batch_is_snow, batch_is_cloud, batch_ys in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred_ys = self.network(batch_xs)\n",
    "                    batch_loss = loss(input=pred_ys, target=batch_ys)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    pbar_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    average_loss = (batch_xs.size(0) * batch_loss.item() + num_samples_processed * average_loss) / (\n",
    "                        num_samples_processed + batch_xs.size(0)\n",
    "                    )\n",
    "                    average_accuracy = (\n",
    "                        torch.sum(pred_ys.argmax(dim=-1) == batch_ys).item()\n",
    "                        + num_samples_processed * average_accuracy\n",
    "                    ) / (num_samples_processed + batch_xs.size(0))\n",
    "                    num_samples_processed += batch_xs.size(0)\n",
    "                    pbar_dict[\"avg. epoch loss\"] = average_loss\n",
    "                    pbar_dict[\"avg. epoch accuracy\"] = average_accuracy\n",
    "                    pbar.set_postfix(pbar_dict)\n",
    "\n",
    "                # TODO(1): Implement periodic SWAG updates using the attributes defined in __init__\n",
    "                if epoch % self.swag_update_freq == 0:\n",
    "                    self.n = epoch/self.swag_update_freq\n",
    "                    self.update_swag()\n",
    "                # raise NotImplementedError(\"Periodically update SWAG statistics\")\n",
    "\n",
    "    def calibrate(self, validation_data: torch.utils.data.Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Calibrate your predictions using a small validation set.\n",
    "        validation_data contains well-defined and ambiguous samples,\n",
    "        where you can identify the latter by having label -1.\n",
    "        \"\"\"\n",
    "        if self.inference_mode == InferenceMode.MAP:\n",
    "            # In MAP mode, simply predict argmax and do nothing else\n",
    "            self._prediction_threshold = 0.0\n",
    "            return\n",
    "\n",
    "        # TODO(1): pick a prediction threshold, either constant or adaptive.\n",
    "        #  The provided value should suffice to pass the easy baseline.\n",
    "        self._prediction_threshold = 2.0 / 3.0\n",
    "\n",
    "        # TODO(2): perform additional calibration if desired.\n",
    "        #  Feel free to remove or change the prediction threshold.\n",
    "        val_xs, val_is_snow, val_is_cloud, val_ys = validation_data.tensors\n",
    "        assert val_xs.size() == (140, 3, 60, 60)  # N x C x H x W\n",
    "        assert val_ys.size() == (140,)\n",
    "        assert val_is_snow.size() == (140,)\n",
    "        assert val_is_cloud.size() == (140,)\n",
    "\n",
    "    def predict_probabilities_swag(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform Bayesian model averaging using your SWAG statistics and predict\n",
    "        probabilities for all samples in the loader.\n",
    "        Outputs should be a Nx6 tensor, where N is the number of samples in loader,\n",
    "        and all rows of the output should sum to 1.\n",
    "        That is, output row i column j should be your predicted p(y=j | x_i).\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.eval()\n",
    "\n",
    "        # Perform Bayesian model averaging:\n",
    "        # Instead of sampling self.bma_samples networks (using self.sample_parameters())\n",
    "        # for each datapoint, you can save time by sampling self.bma_samples networks,\n",
    "        # and perform inference with each network on all samples in loader.\n",
    "        per_model_sample_predictions = []\n",
    "        for _ in tqdm.trange(self.bma_samples, desc=\"Performing Bayesian model averaging\"):\n",
    "            # TODO(1): Sample new parameters for self.network from the SWAG approximate posterior\n",
    "            self.sample_parameters()\n",
    "            # raise NotImplementedError(\"Sample network parameters\")\n",
    "\n",
    "            # TODO(1): Perform inference for all samples in `loader` using current model sample,\n",
    "            #  and add the predictions to per_model_sample_predictions\n",
    "            predictions = []\n",
    "            for (batch_xs,) in loader:\n",
    "                predictions.append(self.network(batch_xs))\n",
    "            \n",
    "            predictions = torch.cat(predictions)\n",
    "            \n",
    "            per_model_sample_predictions.append(predictions.unsqueeze(0))\n",
    "            # raise NotImplementedError(\"Perform inference using current model\")\n",
    "\n",
    "        assert len(per_model_sample_predictions) == self.bma_samples\n",
    "        assert all(\n",
    "            isinstance(model_sample_predictions, torch.Tensor)\n",
    "            and model_sample_predictions.dim() == 2  # N x C\n",
    "            and model_sample_predictions.size(1) == 6\n",
    "            for model_sample_predictions in per_model_sample_predictions\n",
    "        )\n",
    "\n",
    "        # TODO(1): Average predictions from different model samples into bma_probabilities\n",
    "        # raise NotImplementedError(\"Aggregate predictions from model samples\")\n",
    "        bma_logits = torch.cat(per_model_sample_predictions, 0)\n",
    "        bma_logits = torch.mean(bma_logits, 0)\n",
    "        bma_probabilities = torch.softmax(bma_logits, dim=-1)\n",
    "\n",
    "        assert bma_probabilities.dim() == 2 and bma_probabilities.size(1) == 6  # N x C\n",
    "        return bma_probabilities\n",
    "\n",
    "    def sample_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Sample a new network from the approximate SWAG posterior.\n",
    "        For simplicity, this method directly modifies self.network in-place.\n",
    "        Hence, after calling this method, self.network corresponds to a new posterior sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # Instead of acting on a full vector of parameters, all operations can be done on per-layer parameters.\n",
    "        net = self.network.state_dict()\n",
    "\n",
    "        for name, param in self.network.named_parameters():\n",
    "            # SWAG-diagonal part\n",
    "            z_1 = torch.randn(param.size())\n",
    "            # TODO(1): Sample parameter values for SWAG-diagonal\n",
    "            # raise NotImplementedError(\"Sample parameter for SWAG-diagonal\")\n",
    "            \n",
    "            current_mean = self.w_swa[name]\n",
    "            current_std = torch.sqrt(self.w2_swa[name] - self.w_swa[name]**2)\n",
    "            \n",
    "            assert current_mean.size() == param.size() and current_std.size() == param.size()\n",
    "\n",
    "            # Diagonal part\n",
    "            sampled_param = current_mean + current_std * z_1\n",
    "\n",
    "            # Full SWAG part\n",
    "            if self.inference_mode == InferenceMode.SWAG_FULL:\n",
    "                # TODO(2): Sample parameter values for full SWAG\n",
    "                raise NotImplementedError(\"Sample parameter for full SWAG\")\n",
    "                sampled_param += ...\n",
    "\n",
    "            # Modify weight value in-place; directly changing self.network\n",
    "            param.data = sampled_param\n",
    "\n",
    "            net[name] = sampled_param\n",
    "\n",
    "        # TODO(1): Don't forget to update batch normalization statistics using self._update_batchnorm()\n",
    "        #  in the appropriate place!\n",
    "        self.network.load_state_dict(net)\n",
    "        self._update_batchnorm()\n",
    "\n",
    "        # raise NotImplementedError(\"Update batch normalization statistics for newly sampled network\")\n",
    "\n",
    "    def predict_labels(self, predicted_probabilities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict labels in {0, 1, 2, 3, 4, 5} or \"don't know\" as -1\n",
    "        based on your model's predicted probabilities.\n",
    "        The parameter predicted_probabilities is an Nx6 tensor containing predicted probabilities\n",
    "        as returned by predict_probabilities(...).\n",
    "        The output should be a N-dimensional long tensor, containing values in {-1, 0, 1, 2, 3, 4, 5}.\n",
    "        \"\"\"\n",
    "\n",
    "        # label_probabilities contains the per-row maximum values in predicted_probabilities,\n",
    "        # max_likelihood_labels the corresponding column index (equivalent to class).\n",
    "        label_probabilities, max_likelihood_labels = torch.max(predicted_probabilities, dim=-1)\n",
    "        num_samples, num_classes = predicted_probabilities.size()\n",
    "        assert label_probabilities.size() == (num_samples,) and max_likelihood_labels.size() == (num_samples,)\n",
    "\n",
    "        # A model without uncertainty awareness might simply predict the most likely label per sample:\n",
    "        # return max_likelihood_labels\n",
    "\n",
    "        # A bit better: use a threshold to decide whether to return a label or \"don't know\" (label -1)\n",
    "        # TODO(2): implement a different decision rule if desired\n",
    "        return torch.where(\n",
    "            label_probabilities >= self._prediction_threshold,\n",
    "            max_likelihood_labels,\n",
    "            torch.ones_like(max_likelihood_labels) * -1,\n",
    "        )\n",
    "\n",
    "    def _create_weight_copy(self) -> typing.Dict[str, torch.Tensor]:\n",
    "        \"\"\"Create an all-zero copy of the network weights as a dictionary that maps name -> weight\"\"\"\n",
    "        return {\n",
    "            name: torch.zeros_like(param, requires_grad=False)\n",
    "            for name, param in self.network.named_parameters()\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        loader: torch.utils.data.DataLoader,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform full SWAG fitting procedure.\n",
    "        If `PRETRAINED_WEIGHTS_FILE` is `True`, this method skips the MAP inference part,\n",
    "        and uses pretrained weights instead.\n",
    "\n",
    "        Note that MAP inference can take a very long time.\n",
    "        You should hence only perform MAP inference yourself after passing the hard baseline\n",
    "        using the given CNN architecture and pretrained weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # MAP inference to obtain initial weights\n",
    "        PRETRAINED_WEIGHTS_FILE = self.model_dir / \"map_weights.pt\"\n",
    "        if USE_PRETRAINED_INIT:\n",
    "            self.network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE))\n",
    "            print(\"Loaded pretrained MAP weights from\", PRETRAINED_WEIGHTS_FILE)\n",
    "        else:\n",
    "            self.fit_map(loader)\n",
    "\n",
    "        # SWAG\n",
    "        if self.inference_mode in (InferenceMode.SWAG_DIAGONAL, InferenceMode.SWAG_FULL):\n",
    "            self.fit_swag(loader)\n",
    "\n",
    "    def fit_map(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        MAP inference procedure to obtain initial weights of self.network.\n",
    "        This is the exact procedure that was used to obtain the pretrained weights we provide.\n",
    "        \"\"\"\n",
    "        map_epochs = 140\n",
    "        initial_lr = 0.01\n",
    "        decayed_lr = 0.0001\n",
    "        decay_start_epoch = 50\n",
    "        decay_factor = decayed_lr / initial_lr\n",
    "\n",
    "        # Create optimizer, loss, and a learning rate scheduler that aids convergence\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=initial_lr,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            [\n",
    "                torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0),\n",
    "                torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer,\n",
    "                    start_factor=1.0,\n",
    "                    end_factor=decay_factor,\n",
    "                    total_iters=(map_epochs - decay_start_epoch) * len(loader),\n",
    "                ),\n",
    "            ],\n",
    "            milestones=[decay_start_epoch * len(loader)],\n",
    "        )\n",
    "\n",
    "        # Put network into training mode\n",
    "        # Batch normalization layers are only updated if the network is in training mode,\n",
    "        # and are replaced by a moving average if the network is in evaluation mode.\n",
    "        self.network.train()\n",
    "        with tqdm.trange(map_epochs, desc=\"Fitting initial MAP weights\") as pbar:\n",
    "            pbar_dict = {}\n",
    "            # Perform the specified number of MAP epochs\n",
    "            for epoch in pbar:\n",
    "                average_loss = 0.0\n",
    "                average_accuracy = 0.0\n",
    "                num_samples_processed = 0\n",
    "                # Iterate over batches of randomly shuffled training data\n",
    "                for batch_xs, _, _, batch_ys in loader:\n",
    "                    # Training step\n",
    "                    optimizer.zero_grad()\n",
    "                    pred_ys = self.network(batch_xs)\n",
    "                    batch_loss = loss(input=pred_ys, target=batch_ys)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Save learning rate that was used for step, and calculate new one\n",
    "                    pbar_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    with warnings.catch_warnings():\n",
    "                        # Suppress annoying warning (that we cannot control) inside PyTorch\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    average_loss = (batch_xs.size(0) * batch_loss.item() + num_samples_processed * average_loss) / (\n",
    "                        num_samples_processed + batch_xs.size(0)\n",
    "                    )\n",
    "                    average_accuracy = (\n",
    "                        torch.sum(pred_ys.argmax(dim=-1) == batch_ys).item()\n",
    "                        + num_samples_processed * average_accuracy\n",
    "                    ) / (num_samples_processed + batch_xs.size(0))\n",
    "                    num_samples_processed += batch_xs.size(0)\n",
    "\n",
    "                    pbar_dict[\"avg. epoch loss\"] = average_loss\n",
    "                    pbar_dict[\"avg. epoch accuracy\"] = average_accuracy\n",
    "                    pbar.set_postfix(pbar_dict)\n",
    "\n",
    "    def predict_probabilities(self, xs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given images xs.\n",
    "        This method returns an NxC float tensor,\n",
    "        where row i column j corresponds to the probability that y_i is class j.\n",
    "\n",
    "        This method uses different strategies depending on self.inference_mode.\n",
    "        \"\"\"\n",
    "        self.network = self.network.eval()\n",
    "\n",
    "        # Create a loader that we can deterministically iterate many times if necessary\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(xs),\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():  # save memory by not tracking gradients\n",
    "            if self.inference_mode == InferenceMode.MAP:\n",
    "                return self.predict_probabilities_map(loader)\n",
    "            else:\n",
    "                return self.predict_probabilities_swag(loader)\n",
    "\n",
    "    def predict_probabilities_map(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict probabilities assuming that self.network is a MAP estimate.\n",
    "        This simply performs a forward pass for every batch in `loader`,\n",
    "        concatenates all results, and applies a row-wise softmax.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for (batch_xs,) in loader:\n",
    "            predictions.append(self.network(batch_xs))\n",
    "\n",
    "        predictions = torch.cat(predictions)\n",
    "        return torch.softmax(predictions, dim=-1)\n",
    "\n",
    "    def _update_batchnorm(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset and fit batch normalization statistics using the training dataset self.train_dataset.\n",
    "        We provide this method for you for convenience.\n",
    "        See the SWAG paper for why this is required.\n",
    "\n",
    "        Batch normalization usually uses an exponential moving average, controlled by the `momentum` parameter.\n",
    "        However, we are not training but want the statistics for the full training dataset.\n",
    "        Hence, setting `momentum` to `None` tracks a cumulative average instead.\n",
    "        The following code stores original `momentum` values, sets all to `None`,\n",
    "        and restores the previous hyperparameters after updating batchnorm statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        old_momentum_parameters = dict()\n",
    "        for module in self.network.modules():\n",
    "            # Only need to handle batchnorm modules\n",
    "            if not isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "                continue\n",
    "\n",
    "            # Store old momentum value before removing it\n",
    "            old_momentum_parameters[module] = module.momentum\n",
    "            module.momentum = None\n",
    "\n",
    "            # Reset batch normalization statistics\n",
    "            module.reset_running_stats()\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        self.network.train()\n",
    "        for (batch_xs,) in loader:\n",
    "            self.network(batch_xs)\n",
    "        self.network.eval()\n",
    "\n",
    "        # Restore old `momentum` hyperparameter values\n",
    "        for module, momentum in old_momentum_parameters.items():\n",
    "            module.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccebbaf-47b1-4bf7-b640-8f32b998a491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77b6aa-6f31-4ebc-a756-6ffe1f7752c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0b1d2-6408-4674-9baa-8202dfcb7ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bcfcb-a940-4394-961d-eea5958c81f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d24952e0-a851-4c93-aa19-226404112d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAGScheduler(torch.optim.lr_scheduler.LRScheduler):\n",
    "    \"\"\"\n",
    "    Custom learning rate scheduler that calculates a different learning rate each gradient descent step.\n",
    "    The default implementation keeps the original learning rate constant, i.e., does nothing.\n",
    "    You can implement a custom schedule inside calculate_lr,\n",
    "    and add+store additional attributes in __init__.\n",
    "    You should not change any other parts of this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_lr(self, current_epoch: float, old_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate for the epoch given by current_epoch.\n",
    "        current_epoch is the fractional epoch of SWA fitting, starting at 0.\n",
    "        That is, an integer value x indicates the start of epoch (x+1),\n",
    "        and non-integer values x.y correspond to steps in between epochs (x+1) and (x+2).\n",
    "        old_lr is the previous learning rate.\n",
    "\n",
    "        This method should return a single float: the new learning rate.\n",
    "        \"\"\"\n",
    "        # TODO(2): Implement a custom schedule if desired\n",
    "        return old_lr\n",
    "\n",
    "    # TODO(2): Add and store additional arguments if you decide to implement a custom scheduler\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int,\n",
    "        steps_per_epoch: int,\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        super().__init__(optimizer, last_epoch=-1, verbose=False)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n",
    "            )\n",
    "        return [\n",
    "            self.calculate_lr(self.last_epoch / self.steps_per_epoch, group[\"lr\"])\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    swag: SWAGInference,\n",
    "    eval_dataset: torch.utils.data.Dataset,\n",
    "    extended_evaluation: bool,\n",
    "    output_dir: pathlib.Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate your model.\n",
    "    Feel free to change or extend this code.\n",
    "    :param swag: Trained model to evaluate\n",
    "    :param eval_dataset: Validation dataset\n",
    "    :param: extended_evaluation: If True, generates additional plots\n",
    "    :param output_dir: Directory into which extended evaluation plots are saved\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluating model on validation data\")\n",
    "\n",
    "    # We ignore is_snow and is_cloud here, but feel free to use them as well\n",
    "    xs, is_snow, is_cloud, ys = eval_dataset.tensors\n",
    "\n",
    "    # Predict class probabilities on test data,\n",
    "    # most likely classes (according to the max predicted probability),\n",
    "    # and classes as predicted by your SWAG implementation.\n",
    "    pred_prob_all = swag.predict_probabilities(xs)\n",
    "    pred_prob_max, pred_ys_argmax = torch.max(pred_prob_all, dim=-1)\n",
    "    pred_ys = swag.predict_labels(pred_prob_all)\n",
    "\n",
    "    # Create a mask that ignores ambiguous samples (those with class -1)\n",
    "    nonambiguous_mask = ys != -1\n",
    "\n",
    "    # Calculate three kinds of accuracy:\n",
    "    # 1. Overall accuracy, counting \"don't know\" (-1) as its own class\n",
    "    # 2. Accuracy on all samples that have a known label. Predicting -1 on those counts as wrong here.\n",
    "    # 3. Accuracy on all samples that have a known label w.r.t. the class with the highest predicted probability.\n",
    "    accuracy = torch.mean((pred_ys == ys).float()).item()\n",
    "    accuracy_nonambiguous = torch.mean((pred_ys[nonambiguous_mask] == ys[nonambiguous_mask]).float()).item()\n",
    "    accuracy_nonambiguous_argmax = torch.mean(\n",
    "        (pred_ys_argmax[nonambiguous_mask] == ys[nonambiguous_mask]).float()\n",
    "    ).item()\n",
    "    print(f\"Accuracy (raw): {accuracy:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, your predictions): {accuracy_nonambiguous:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, predicting most-likely class): {accuracy_nonambiguous_argmax:.4f}\")\n",
    "\n",
    "    # Determine which threshold would yield the smallest cost on the validation data\n",
    "    # Note that this threshold does not necessarily generalize to the test set!\n",
    "    # However, it can help you judge your method's calibration.\n",
    "    thresholds = [0.0] + list(torch.unique(pred_prob_max, sorted=True))\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        thresholded_ys = torch.where(pred_prob_max <= threshold, -1 * torch.ones_like(pred_ys), pred_ys)\n",
    "        costs.append(cost_function(thresholded_ys, ys).item())\n",
    "    best_idx = np.argmin(costs)\n",
    "    print(f\"Best cost {costs[best_idx]} at threshold {thresholds[best_idx]}\")\n",
    "    print(\"Note that this threshold does not necessarily generalize to the test set!\")\n",
    "\n",
    "    # Calculate ECE and plot the calibration curve\n",
    "    calibration_data = calc_calibration_curve(pred_prob_all.numpy(), ys.numpy(), num_bins=20)\n",
    "    print(\"Validation ECE:\", calibration_data[\"ece\"])\n",
    "\n",
    "    if extended_evaluation:\n",
    "        print(\"Plotting reliability diagram\")\n",
    "        fig = draw_reliability_diagram(calibration_data)\n",
    "        fig.savefig(output_dir / \"reliability_diagram.pdf\")\n",
    "\n",
    "        sorted_confidence_indices = torch.argsort(pred_prob_max)\n",
    "\n",
    "        # Plot samples your model is most confident about\n",
    "        print(\"Plotting most confident validation set predictions\")\n",
    "        most_confident_indices = sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(xs[sample_idx].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {pred_ys[sample_idx]}, true {ys[sample_idx]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if ys[sample_idx] >= 0:\n",
    "                    bar_colors[ys[sample_idx]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), pred_prob_all[sample_idx].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Most confident predictions\", size=20)\n",
    "        fig.savefig(output_dir / \"examples_most_confident.pdf\")\n",
    "\n",
    "        # Plot samples your model is least confident about\n",
    "        print(\"Plotting least confident validation set predictions\")\n",
    "        least_confident_indices = sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(xs[sample_idx].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {pred_ys[sample_idx]}, true {ys[sample_idx]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if ys[sample_idx] >= 0:\n",
    "                    bar_colors[ys[sample_idx]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), pred_prob_all[sample_idx].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Least confident predictions\", size=20)\n",
    "        fig.savefig(output_dir / \"examples_least_confident.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f4cba-589b-4c0b-a57c-95f2f9d1eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87029b-688f-4484-8c0f-6fc76b975dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4008c64c-b00f-4ae8-8cf0-ebbd7c6752d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all randomness\n",
    "setup_seeds()\n",
    "\n",
    "# Build and run the actual solution\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "swag = SWAGInference(\n",
    "    train_xs=dataset_train.tensors[0],\n",
    "    model_dir=model_dir,\n",
    ")\n",
    "# swag.fit(train_loader)\n",
    "swag.calibrate(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900200b6-67c9-437d-90f5-4a56d2ef8e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b77476a7-4ede-4b33-934a-c3c02526befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Bayesian model averaging:   0%|              | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mfork_rng():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXTENDED_EVALUATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 68\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(swag, eval_dataset, extended_evaluation, output_dir)\u001b[0m\n\u001b[1;32m     63\u001b[0m xs, is_snow, is_cloud, ys \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mtensors\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Predict class probabilities on test data,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# most likely classes (according to the max predicted probability),\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# and classes as predicted by your SWAG implementation.\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m pred_prob_all \u001b[38;5;241m=\u001b[39m \u001b[43mswag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m pred_prob_max, pred_ys_argmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(pred_prob_all, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m pred_ys \u001b[38;5;241m=\u001b[39m swag\u001b[38;5;241m.\u001b[39mpredict_labels(pred_prob_all)\n",
      "Cell \u001b[0;32mIn[58], line 429\u001b[0m, in \u001b[0;36mSWAGInference.predict_probabilities\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_probabilities_map(loader)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_probabilities_swag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 199\u001b[0m, in \u001b[0;36mSWAGInference.predict_probabilities_swag\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    196\u001b[0m per_model_sample_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbma_samples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming Bayesian model averaging\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# TODO(1): Sample new parameters for self.network from the SWAG approximate posterior\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# problem\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# raise NotImplementedError(\"Sample network parameters\")\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# TODO(1): Perform inference for all samples in `loader` using current model sample,\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m#  and add the predictions to per_model_sample_predictions\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[58], line 264\u001b[0m, in \u001b[0;36mSWAGInference.sample_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# TODO(1): Don't forget to update batch normalization statistics using self._update_batchnorm()\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#  in the appropriate place!\u001b[39;00m\n\u001b[1;32m    263\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m--> 264\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mload_state_dict(net)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_batchnorm()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pai/lib/python3.8/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "with torch.random.fork_rng():\n",
    "    evaluate(swag, dataset_val, EXTENDED_EVALUATION, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef281437-ab95-4727-9a83-dd3cc16f665e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c80f4-59ef-4fed-96e9-3bc99838a2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff8f3a93-161c-4766-b3fc-9118da54a648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdc757-ec2a-4475-8737-e16aef33c965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9178544f-cc44-4851-a5fb-1af6351b1740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cab3f79-db2a-42e6-ab0a-fa36ed8fbd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cc4375e-80dd-4a45-9c5e-76a398f4a34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89e7a579-9435-4c4a-8c20-bc470bde2531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29937286-8d31-4605-b33a-7c606065eecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d003005-1850-406f-99e7-89aafd556186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(1): Don't forget to update batch normalization statistics using self._update_batchnorm()\n",
    "#  in the appropriate place!\n",
    "net = swag.network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "000f4e48-1b46-4da5-af00-78ff3326596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layer0.0.weight', 'layer0.0.bias', 'layer0.1.weight', 'layer0.1.bias', 'layer0.1.running_mean', 'layer0.1.running_var', 'layer0.1.num_batches_tracked', 'layer1.0.weight', 'layer1.0.bias', 'layer1.1.weight', 'layer1.1.bias', 'layer1.1.running_mean', 'layer1.1.running_var', 'layer1.1.num_batches_tracked', 'layer2.0.weight', 'layer2.0.bias', 'layer2.1.weight', 'layer2.1.bias', 'layer2.1.running_mean', 'layer2.1.running_var', 'layer2.1.num_batches_tracked', 'layer3.0.weight', 'layer3.0.bias', 'layer3.1.weight', 'layer3.1.bias', 'layer3.1.running_mean', 'layer3.1.running_var', 'layer3.1.num_batches_tracked', 'layer4.0.weight', 'layer4.0.bias', 'layer4.1.weight', 'layer4.1.bias', 'layer4.1.running_mean', 'layer4.1.running_var', 'layer4.1.num_batches_tracked', 'layer5.0.weight', 'layer5.0.bias', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e0268aa-f6dc-4bb4-872c-e75e78a8f80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0c4f187-4bed-4b02-9c94-62586119d144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "085441c5-9b04-4909-8a58-80434f5e6a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer0.0.weight',\n",
       "              tensor([[[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.],\n",
       "                        [0., 0., 0., 0., 0.]]]])),\n",
       "             ('layer0.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer0.1.weight',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer0.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer0.1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer0.1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('layer0.1.num_batches_tracked', tensor(0)),\n",
       "             ('layer1.0.weight',\n",
       "              tensor([[[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]]])),\n",
       "             ('layer1.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer1.1.weight',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer1.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer1.1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer1.1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('layer1.1.num_batches_tracked', tensor(0)),\n",
       "             ('layer2.0.weight',\n",
       "              tensor([[[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]]])),\n",
       "             ('layer2.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer2.1.weight',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer2.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer2.1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer2.1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('layer2.1.num_batches_tracked', tensor(0)),\n",
       "             ('layer3.0.weight',\n",
       "              tensor([[[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]]])),\n",
       "             ('layer3.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer3.1.weight',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer3.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer3.1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer3.1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('layer3.1.num_batches_tracked', tensor(0)),\n",
       "             ('layer4.0.weight',\n",
       "              tensor([[[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]]])),\n",
       "             ('layer4.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer4.1.weight',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer4.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer4.1.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('layer4.1.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('layer4.1.num_batches_tracked', tensor(0)),\n",
       "             ('layer5.0.weight',\n",
       "              tensor([[[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]],\n",
       "              \n",
       "              \n",
       "                      [[[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]],\n",
       "              \n",
       "                       [[0., 0., 0.],\n",
       "                        [0., 0., 0.],\n",
       "                        [0., 0., 0.]]]])),\n",
       "             ('layer5.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('linear.weight',\n",
       "              tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])),\n",
       "             ('linear.bias', tensor([0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e553e9b0-202d-4c57-ab86-883db3ffdae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_param\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pai/lib/python3.8/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "net.update(sampled_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0d372-6dd1-452f-98ec-1a9de270f04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5ce9a-52fd-4e07-a0bf-778741a5112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "swag.network.load_state_dict(net)\n",
    "swag._update_batchnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3256c8-6925-4564-8c74-396849721555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5b8e4-dd37-478f-b4e0-ec4ee3142089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d547419-3871-4722-b179-de4a32dc199f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecb99b-f71f-4958-b0b5-f21b7e9e0845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c704ef4-8636-46ef-ae42-f7669d88f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c702ef5-4483-4b31-91dd-4860699a4a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aeee07-06da-4f56-bb68-4967578bdbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f4adc-a10c-4412-8cef-9922fb8b402b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59d78a-4153-4aff-8015-21b827969f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
